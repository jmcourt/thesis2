\chapter{PANTHEON suite}

\label{app:PAN}

\par Below is the code for the entire suite of \textit{PANTHEON} (Python ANalytical Tools for High-energy Event data manipulatiON) that I used during the work presented in this thesis.  This code is also available at \url{https://github.com/jmcourt/PANTHEON}.  PANTHEON makes use of the Astropy \citep{Astropy}, Matplotlib \citep{Hunter_MatPlotLib}, Numpy, Scipy \citep{NumPy} and Numba \citep{Numba}.

\section{FITS Genie}

\par \textit{FITS Genie} is a script that allows the user to extract data from raw \texttt{FITS} files.  The script was designed to interface with \textit{RXTE} data, but there is also limited implementation with \textit{Suzaku}.  The script produces \texttt{.plotd} and \texttt{.speca} files, which can be further processed with \textit{Plot Demon} and \textit{Spec Angel}.

\begin{minted}[fontsize=\scriptsize]{python}
#! /usr/bin/env python

# |----------------------------------------------------------------------|
# |-----------------------------FITS GENIE-------------------------------|
# |----------------------------------------------------------------------|

# Call as ./fitsgenie.py FILE1 PROD_REQ [LCHAN] [HCHAN] [BINNING] [FOURIER RES] 
#   [FOURIER SEP] [BGEST] [FLAVOUR]
#
# Takes 1 FITS Event file and produces .speca and .plotd formatted products to be
#   analysed by plotdemon
# and specangel.
#
# Arguments:
#
#  FILE1
#   The absolute path to the file to be used.
#
#  PROD_REQ
#   The products requested by the user.  The following inputs are valid:
#      'spec','speca','s' will cause FITSGenie to produce only a .speca file as output
#      'plot','plotd','p' will cause FITSGenie to produce only a .plotd file as output
#      'both','all','b','a','sp','ps' will cause both files to be output
#
#  [LCHAN]
#   Optional: The lowest channel on the PCA instrument on RXTE which will be used to
#     populate the data.  Default of 0 (minimum).
#
#  [HCHAN]
#   Optional: The highest channel on the PCA instrument on RXTE which will be used to
#     populate the data.  Default of 255 (maximum).
#
#  [BINNING]
#   Optional: The size, in seconds, of bins into which data will be sorted.  Takes the
#     value of the time resolution of the data if not specified by the user.  Default
#     of 2^-15s
#
#  [FOURIER RES]
#   Optional: The size of the individual time windows in which the data is to be split.
#   Fourier spectra will be made of each of these windows.  Default of 128s.
#
#  [FOURIER SEP]
#   Optional: The separation of the startpoints of individual time windows in which the
#     data is to be split.  Fourier spectra will be made of each of these windows.
#     Default of 128s.
#
#  [BGEST]
#   Optional: The approximate average background count rate during the observation in
#     cts/s.  Default of 30cts/s.
#
#  [FLAVOUR]
#   Optional: A useful bit of text to put on plots to help identify them later on.

#-----User-set Parameters--------------------------------------------------------------

ptdbinfac=1                 # To save space and time, the time bins for saved 
                            #   plotdemon data will be greater than the time bins for
                            #   the not-saved specangel data by this factor.  Must be
                            #   power of 2.
spcbinfac=4096              # The binning factor for SpecAngel data to use when
                            #   searching for data peaks and troughs
usrmin=-13                  # The smallest time resolution to consider is 2^usrmin
                            #   seconds
version=6.1

#-----Welcoming Header-----------------------------------------------------------------

print ''
print '-------Running FITSGenie: J.M.Court, 2015-------'
print ''

#-----Importing Modules----------------------------------------------------------------

try:
   import sys
   import pan_lib as pan
   from astropy.io import fits
except ImportError:
   print 'Modules missing!  Aborting!'
   print ''
   print '------------------------------------------------'
   print ''
   exit()

#-----Checking Validity of Filename----------------------------------------------------

args=sys.argv
pan.argcheck(args,1)                    # Must give at least 1 args (the function call)
if len(args)<2:
   filename=raw_input('Filename: ')
   print ''
else:
   filename=args[1]                     # Fetch file name from arguments
if len(args)<3:
   print ''
   print 'FitsGenie can produce [P]lotDemon files, [S]pecangel files or [B]oth.'
   print ''
   prod_req=raw_input('Select product(s): ')
   print ''
else:
   prod_req=args[2]                     # Fetch products request from arguments

#-----Identifying Products-------------------------------------------------------------

if prod_req.lower() in ['spec','speca','s','both','all','a','b','ps','sp']:
   spec_on=True
   print '.speca File will be created!'
else:
   spec_on=False
   print '.speca File will NOT be created!'
if prod_req.lower() in ['plot','plotd','p','both','all','a','b','ps','sp']:
   plot_on=True
   print '.plotd File will be created!'
else:
   plot_on=False
   print '.plotd File will NOT be created!'
del prod_req
print ''

#-----Opening FITS file, identifying mission-------------------------------------------

try:
   assert filename[-6:] not in ('.speca','.plotd') # Don't try to open plotd/speca
                                                   #   files please...
   event=fits.open(filename)                       # Unleash the beast! [open the file]
except:
   print 'Could not open file "'+filename+'"!'
   print 'Aborting!'
   pan.signoff()
   exit()

from math import log                               # Import remaining modules.  This is
                                                   #   a slight speedup when running a
                                                   #   a script to attempt to FITSgenie
                                                   #   some valid files and some 
                                                   #   invalid files
from numpy import arange, array, histogram, zeros
from numpy import sum as npsum
from scipy.fftpack import fft
import pylab as pl

try:
   mission=event[1].header['TELESCOP']             # Fetch the name of the telescope
except:
   print 'Could not identify mission!'
   print 'Aborting!'
   pan.signoff()
   exit()
if mission in ['XTE','SUZAKU','SWIFT']:
   print mission,'data detected!'
else:
   print mission,'data not yet supported!'
   pan.signoff()
   exit()
if mission == 'XTE' :
   etype='channel'                                 # XTE requires an input of channel
                                                   #   IDs
   escale=''
   escaleb=''
   #try:
   import xtepan_lib as inst                       # Import XTE extraction functions
   #except:
   #   print 'XTE PANTHEON Library not found!  Aborting!'
   #   pan.signoff()
   #   exit()
elif mission == 'SUZAKU':
   etype='energy'                                  # SUZAKU requires an input of raw
                                                   #   energies
   escale='eV'
   escaleb=' (eV)'
   try:
      import szkpan_lib as inst                    # Import SUZAKU extraction functions
   except:
      print 'Suzaku PANTHEON Library not found!  Aborting!'
      pan.signoff()
      exit()
elif mission == 'SWIFT':
   etype='energy'                                  # SUZAKU requires an input of raw
                                                   #   energies
   escale='eV'
   escaleb=' (eV)'
   try:
      import swfpan_lib as inst                    # Import SUZAKU extraction functions
   except:
      print 'Swift PANTHEON Library not found!  Aborting!'
      pan.signoff()
      exit()
else:
   print "This error shouldn't happen...  sorry 'bout that!"
   pan.signoff()
   exit()
try:
   obsdata=inst.getobs(event,event[1].header['DATAMODE'],filename)       # Fetch object
   print event[1].header['DATAMODE'],'format detected.'
except:
   print 'Could not identify DATAMODE!'
   print 'Aborting!'
   pan.signoff()
   exit()
if event[1].header['DATAMODE'][:2] in ['B_','SB']:
   spec_on=False
   bin_dat=True
   print 'No .speca file can be produced!'
   if not plot_on:
      print 'Aborting!'
      pan.signoff()
      exit()
else:
   bin_dat=False

#-----Checking validity of remaining inputs--------------------------------------------

print 'Object =',obsdata[0]
print 'Obs_ID =',obsdata[1]
print ''
maxen=inst.maxen(event[1].header['DATAMODE'])            # Get the value of the
                                                         #   highest energy or channel
                                                         #   for the instrument
print 'Inputs:'
print ''
if len(args)>3:
   lowc=int(args[3])                                     # Collect minimum channel
                                                         #   label from user
   print 'Min Channel =',lowc
else:
   try:
      lowc=int(raw_input("Minimum "+etype+escaleb+": "))
   except:
      lowc=0
      print "Using min "+etype+" of 0"+escale+"!"
if len(args)>4:
   highc=int(args[4])                                    # Collect maximum channel
                                                         #   label from user
   print 'Max Channel =',highc
else:
   try:
      highc=int(raw_input("Maximum "+etype+escaleb+": "))
   except:
      highc=maxen
      print "Using max "+etype+" of "+str(maxen)+escale+"!"
if lowc<0:    lowc=0                                     # Force channels to be in
                                                         #   range 0,255
if highc>maxen: highc=maxen
if lowc>highc:
   print 'Invalid '+etype+'!  Aborting!'                 # Abort if user gives
                                                         #   lowc>highc
   pan.signoff()
   exit()
cs=str(int(lowc))+'-'+str(int(highc))
if len(args)>5:
   bszt=float(args[5])                                   # Collect binsize from inputs
                                                         #   if given, else ask user,
                                                         #   else use resolution
                                                         #   encoded in .fits file
   print 'Bin size (s)=',bszt
else:
   try:
      bszt=float(raw_input("Photon count bin-size (s): "))
   except:
      bszt=0
      print "Using max time resolution..."
if len(args)>6:
   foures=float(args[6])                                 # Collect Fourier resolution
                                                         #   from inputs if given, else
                                                         #   ask user, else use 128s
   print 'Fourier Res.=',foures
elif not spec_on:
   foures=16
else:
   try:
      foures=float(raw_input("Length of time per Fourier Window (s): "))
   except:
      foures=128
      print "Using 128s per spectrum..."
if len(args)>7:
   slide=float(args[7])                                  # Collect Fourier resolution
                                                         #   from inputs if given, else
                                                         #   ask user, else use 128s
   print 'Fourier Sep.=',slide
elif not spec_on:
   slide=16
else:
   try:
      slide=float(raw_input("Separation of Fourier Windows (s): "))
   except:
      slide=foures
      print "Using "+str(slide)+"s per spectrum..."
if len(args)>8:
   bgest=float(args[8])                                  # Collect background estimate
                                                         #   from inputs if given, else
                                                         #   ask user, else use 30c/s
   print 'Background  =',bgest
elif not spec_on:
   bgest=0
else:   
   try:
      bgest=float(raw_input("Estimate of background (c/s): "))
   except:
      bgest=30
      print "Using 30c/s background..."
   print ''
if len(args)>9:
   flavour=args[9]                                       # Collect flavour if given,
                                                         #   else flavourless
   print 'Flavour     =',flavour
else:
   flavour=''
print ''
wtype='Boxcar'                                           # Setting all windows to
                                                         #   BoxCar; will make 
                                                         #   transition to SpecAngel
                                                         #   4.0 smoother if this
                                                         #   takes a value

#-----Masking data---------------------------------------------------------------------

gti=inst.getgti(event)                                   # Extract GTI indices
datas=inst.getdat(event)                                 # Extract event data
print 'Discarding photons outside of '+etype+' range '\
   +str(lowc)+escale+'-'+str(highc)+escale+'...'
datas=inst.discnev(datas,event[1].header['DATAMODE'])    # Discarding non-events /
                                                         #   reformatting XTE Binned
                                                         #   data into a less awful
                                                         #   structure
if event[1].header['DATAMODE'][:2] == 'B_':
   olen=str(npsum(array(datas)))
else:
   olen=str(len(datas))
datas=inst.chrange(datas,lowc,highc,event[1].header)
tstart=inst.getini(event)
if event[1].header['DATAMODE'][:2] == 'B_':
   phcts=npsum(datas)
else:
   phcts=len(datas)
if float(olen)==0:
   print 'No photons!  Aborting!'
   pan.signoff()
   exit()
pcg=str(int(100*phcts/float(olen)))+'\%'
print str(phcts)+'/'+olen+' photons fall within '+etype+' range ('+pcg+')!'
if phcts==0:
   print 'Aborting!'
   pan.signoff()
   exit()
print ''

#-----Fetching Bin Size----------------------------------------------------------------

bsz=inst.getbin(event,event[1].header['DATAMODE']) # Fetch 'Binning' as the time
                                                   #   resolution of the data
ores=bsz
if bszt>bsz:                                       # If user enters a lower binning
                                                   #   resolution than maximum, use
                                                   #   that instead
   bsz=bszt
n=usrmin                                           # Rounding bsz to the nearest
                                                   #   (greater) power of 2
while (2**n)<bsz:
   n+=1
bsz=2**n
bsz=float(bsz)

#-----Fetching Time Axis---------------------------------------------------------------

print 'SpecAngel binsize rounded to 2^'+str(n)+'s ('+str(bsz)+'s)!'
print 'PlotDemon binsize rounded to 2^'+str(n+int(log(ptdbinfac,2)))\
   +'s ('+str(bsz*ptdbinfac)+'s)!'
times,datas=inst.gettim(datas,event[1].data,tstart,ores,event[1].header['DATAMODE'])
                     # ^ Extracting list of photon incident times as a separate object
pcwrds=inst.getwrd(datas,event[1].header['DATAMODE'])
sttim=times[0]
times=times-sttim

#-----Fetching Fourier Range Size------------------------------------------------------

if foures>max(times):
   foures=128
if slide>max(times):
   slide=foures
slidelock=slide==foures          # If foures=slide, lock them together
n=0                              # Rounding foures to the nearest (greater) power of 2
while (2**n)<foures:
   n+=1
foures=2**n
if slidelock:
   slide=foures
else:
   plot_on=False
   print 'No .plotd file can be produced!'
   if not spec_on:
      print 'Aborting!'
      pan.signoff()
      exit()
print 'Fourier window length rounded to 2^'+str(n)+'s ('+str(foures)+'s)!'
print 'Fourier window separation rounded to',str(slide)+'s!'
print ''

#-----Rescaling GTI--------------------------------------------------------------------

for j in pan.eqrange(gti):
   gti[j]=gti[j][0]-sttim,gti[j][1]-sttim

#-----Setting up power spectra---------------------------------------------------------

ndat=int(max(times)/bsz)
datres=int(foures/bsz)                      # Work out how many data points corresponds
                                            #   to the user given time interval
                                            #   'foures'
stpres=int(slide/bsz)                       # Work out how many data points corresponds
                                            #   to the user given time separation
                                            #   'slide'
numstep=(ndat/stpres)                       # Calculate how many intervals of 'datres'
                                            #   can be divided into the data length
while ((numstep-1)*slide)+foures>max(times):
   numstep-=1                               # Way to make sure the final bin doesn't
                                            #   exceed the data time limit
print 'Analysing data...'
print ''
fourgrlin=[]                                # Set up matrix
bad=0                                       # Counter to count ranges which fall out of
                                            #   the GTIs
good=[]                                     # Array to keep track of which ranges were
                                            #   good
prates=[]                                   # 'Peak Rates'
trates=[]                                   # 'Trough Rates'
rates=[]                                    # Array of count rates to be populated
npcus=[]                                                                 
t=arange(0,foures+bsz,bsz)                  # Setting up SpecAngel resolution time
                                            #   series per Fourier bin
tp=arange(0,foures+bsz*spcbinfac,bsz*spcbinfac) # Setting up SpecAngel coarse
                                                #   resolution time series
tc=arange(0,foures+bsz*ptdbinfac,bsz*ptdbinfac) # Setting up PlotDemon coarse
                                                #   resolution time series per
                                                #   Fourier bin
ta=arange(0,(foures*numstep),bsz*ptdbinfac)     # Setting up PlotDemon resolution
                                                #   full time series

#-----Populating power spectra---------------------------------------------------------

fullhist=[]                                     # Create empty flux array to pass to
                                                #   plotdemon
fullerrs=[]
tcounts=0                                       # Initiate photon counter
pcus=None
if not bin_dat:
   for step in range(numstep):                              ## For every [foures]s
                                                            ##   interval in the data:
      stpoint=step*slide                                       # Calculate the start
                                                               #   point of the
                                                               #   interval
      edpoint=stpoint+foures                                   # Calculate the endpoint
                                                               #   of the interval
      in_gti=False                                             # Assume the subrange is
                                                               #   not in the GTI
      for j in pan.eqrange(gti):
         if gti[j][0]<=stpoint<edpoint<=gti[j][1]: in_gti=True # Change in_gti flag if
                                                               #   this range is wholly
                                                               #   within one GTI
      mask=times>=stpoint
      datrow=times[mask]                                       # Take all photons in
                                                               #   the event data which
                                                               #   occurred after the
                                                               #   start point
      wrdrow=inst.getwrdrow(pcwrds,mask,event[1].header['DATAMODE'])
      mask=datrow<edpoint
      datrow=datrow[mask]                                      # Remove all photons
                                                               #   which occurred after
                                                               #   the end point
      wrdrow=inst.getwrdrow(wrdrow,mask,event[1].header['DATAMODE'])
      fc,null=histogram(datrow,tc+stpoint)                     # Coarsely bin this sub
                                                               #   range of event data
      fp,null=histogram(datrow,tp+stpoint)                     #   Very Coarsely bin
                                                               #   this subrange of
                                                               #   event data
      del null
      fullhist=fullhist+list(fc)
      fullerrs=fullerrs+list((array(fc)**0.5))
      if in_gti:
         f,txis=histogram(datrow,t+stpoint)                     # Bin well this sub
                                                                #   range of event data
         pcus=inst.getpcu(wrdrow,event[1].header,t_pcus=pcus)   # Count active PCUs by
                                                                #   assuming any that
                                                                #   recorded 0 events
                                                                #   in the time period
                                                                #   were inactive
         npcus.append(pcus)
         counts=sum(f)
         peak=max(fp)
         trough=min(fp)
         rates.append(float(counts)/foures)
         prates.append(float(peak)*datres/(foures*spcbinfac))
         trates.append(float(trough)*datres/(foures*spcbinfac))
         tcounts+=counts
         tsfdata=fft(f)                                         # Fourier transform the
                                                                #   interval
         tsfdata=pan.leahyn(tsfdata,counts,datres)              # Normalise to Leahy
                                                                #   Power
         good.append(True)                                      # Flag this column as
                                                                #   good
      else:
         tsfdata=zeros(datres/2)
         npcus.append(0)
         rates.append(0)
         prates.append(0)
         trates.append(0)
         good.append(False)                                     #  Flag this column as
                                                                #    bad
      fourgrlin.append(tsfdata)                                 #  Append the FT'd data
                                                                #    to the matrix
      prog=step+1
      if (prog \% 5)==0 or prog==numstep:
         print str(prog)+'/'+str(numstep)+' series analysed...' # Display progress 
                                                                #   every 5 series
   pcg=str(int(100*tcounts/float(phcts)))+'\%'
   print ''
   print str(tcounts)+'/'+str(phcts)+' ('+pcg+') photons in GTI '\
      +str((gti[0][0],gti[-1][1]))+'!'
   if tcounts==0:
      print 'Aborting!'
      pan.signoff()
      exit()
else:                                                           # Not doing Spectra for
                                                                #   Binned data just
                                                                #   yet...
   print 'Number of PCUs unknown!'
   npcus=[int(raw_input('Number of Active PCUS: '))]            # Ask the user how many
                                                                #   there are
   ta,fullhist,fullerrs=pan.binify(times,datas/ores*bsz*ptdbinfac,
      (datas**0.5)/ores*bsz*ptdbinfac,bsz)

#-----Save .speca and .plotd files-----------------------------------------------------

print ''
print 'Saving...'
print ''
filext=(filename.split('.')[-1])           # Identify file extension from the original
                                           #   filename
if filext!=filename:
   print filename
   tfilename=filename[:-len(filext)-1]     # Remove file extension, if present
   if tfilename[-1]!='.':                  # Saving extensionless files with .. in the
                                           #   path name *breaks without this*
      filename=tfilename
filename=filename+'_'+cs+'_'+str(bsz)+'s'
if plot_on:
   pfilename=pan.plotdsv(filename,ta,array(fullhist)/bsz,array(fullerrs)/bsz,tstart,
      bsz*ptdbinfac,gti,max(npcus),bgest,'False',None,flavour,cs,mission,obsdata,
      version)
   print "PlotDemon file saved to "+pfilename
else:
   print "PlotDemon file not saved."
if spec_on:
   sfilename=pan.specasv(filename,fourgrlin,good,rates,prates,trates,tcounts,
      max(npcus),bsz,bgest,foures,flavour,cs,mission,obsdata,wtype,slide,spcbinfac,
      version)
   print "SpecAngel file saved to "+sfilename
else:
   print "SpecAngel file not saved."

#-----Footer---------------------------------------------------------------------------

pan.signoff()
\end{minted}

\section{Plot Demon}

\begin{minted}[fontsize=\scriptsize]{python}
#! /usr/bin/env python

# |----------------------------------------------------------------------|
# |------------------------------PLOT DEMON------------------------------|
# |----------------------------------------------------------------------|

# Call as ./plotdemon.py FILE1 [FILE2] [FILE3] BINNING
#
# Takes 1-3 .plotd files and plots relevant astrometric plots
#
# Arguments:
#
#  FILE1
#   The absolute path to the first file to be used (generally the lowest energy band)
#
#  [FILE2]
#   The absolute path to the second file to be used
#
#  [FILE3]
#   The absolute path to the third file to be used (generally the highest energy band)
#
#  [BINNING]
#   Optional: the size, in seconds, of bins into which data will be sorted.

#-----User-set Parameters--------------------------------------------------------------

minbin=0.0078125                  # The minimum bin size the code is allowed to attempt
                                  #   to use.  This can prevent long hang-ups
version=4.3                       # The version of PlotDemon
cbin=32.0                         # The number of bins to use when calculating
                                  #   inhomonogeneity in circfold

#-----Welcoming Header-----------------------------------------------------------------

print ''
print '-------Running Plot Demon: J.M.Court, 2014------'
print ''

#-----Importing Modules----------------------------------------------------------------

try:
   import sys,os,imp
   import pylab as pl
   import pan_lib as pan
   import numpy as np
   import scipy.signal as sig
   import scipy.optimize as optm
   from math import pi
except ImportError:
   print 'Modules missing!  Aborting!'
   print ''
   print '------------------------------------------------'
   print ''
   exit()
try:
   imp.find_module('PyAstronomy')                         # Check if PyAstronomy exists
   module_pyastro=True
except ImportError:
   module_pyastro=False

#-----Opening Files--------------------------------------------------------------------

args=sys.argv                                          # Fetching arguments; softest
                                                       #    energy band first please
pan.argcheck(args,1)
try:
   float(args[-1])                                     # If the final argument can be
                                                       #   converted to integer, assume
                                                       #   user intends it as a binning 
   isbininp=True                                       # "IS BINsize given as an
                                                       #   INPut?"
except:
   isbininp=False
nfiles=max(len(args)-isbininp-1,1)                     # Fetch number of infiles (total
                                                       #   args minus one or two iff
                                                       #   binsize given)
if nfiles>3: nfiles=3
if len(args)<2:
   file1=raw_input('Filename: ')
else:
   file1=args[1]
isplotd1=pan.filenamecheck(file1,'plotd',continu=True) # Work out whether input file is
                                                       #   a plotdemon file or a csv
ch={}                                                  # Save channel info in a library
bg1=0
bg2=0
bg3=0
print 'Opening',file1                                  # Opening file 1
x1r,y1r,ye1r,tst1,bsz1,gti,pcus1,bg1,bsub1,bdata1,flv1,
   ch[1],mis1,obsd1,v1=pan.pdload(file1,isplotd1)
y1r=y1r/float(pcus1)                                   # Normalising flux by dividing
                                                       # by the number of active PCUs
                                                       # and the binsize
ye1r=ye1r/float(pcus1)
flavour=flv1
if flavour=='':
   qflav=''
else:
   qflav=' "'+flavour+'"'
if nfiles>1:
   file2=args[2]
   isplotd2=pan.filenamecheck(file2,'plotd',continu=True)
   print 'Opening',file2                               # Opening file 2
   x2r,y2r,ye2r,tst2,bsz2,gti2,pcus2,bg2,bsub2,bdata2,flv2,ch[2],mis2,obsd2,v2\
     =pan.pdload(file2,isplotd2)
   y2r=y2r/float(pcus2)                               # Normalising flux by dividing
                                                      # by the number of active PCUs
                                                      # and the binsize
   ye2r=ye2r/float(pcus2)
else: x2r=y2r=ye2r=tst2=bsz2=None
if nfiles>2:
   file3=args[3]
   isplotd3=pan.filenamecheck(file3,'plotd',continu=True)
   print 'Opening',file3                              # Opening file 3
   x3r,y3r,ye3r,tst3,bsz3,gti3,pcus3,bg3,bsub3,bdata3,flv3,ch[3],mis3,obsd3,v3\
     =pan.pdload(file3,isplotd3)
   y3r=y3r/float(pcus3)                               # Normalising flux by dividing
                                                      # by the number of active PCUs
                                                      # and the binsize
   ye3r=ye3r/float(pcus3)                             
else: x3r=y3r=ye3r=tst3=bsz3=None
bg=(bg1+bg2+bg3)/float(pcus1)
xit1=x1r[-1]
if nfiles>1:
   xit2=x2r[-1]
else:
   xit2=None
if nfiles==3:
   xit3=x3r[-1]
else:
   xit3=None
oet=max(xit1,xit2,xit3)                               # Fetch the observation end time
mint=0                                                # Save original start and end-
                                                      # points for use in clipping
maxt=oet
if nfiles>1:                                          # Checking that start-times of
                                                      # files 1 & 2 match
   if tst1!=tst2:
      if tst1>tst2:
         while x1r[0]+tst1>x2r[0]+tst2:               # Hack data off of the start of
                                                      # file 2 until its startpoint
                                                      # matches file 1
            if len(x2r)==0:
               print 'Times domains for files 1 & 2 do not overlap!  Aborting!'
               pan.signoff()
               exit()
            x2r=np.delete(x2r,0)
            y2r=np.delete(y2r,0)
            ye2r=np.delete(ye2r,0)
         if tst1+x1r[0]!=tst2+x2r[0]:
            print 'Starting times for files 1 & 2 do not match!  Aborting!'
                                                      # If this overshoots, give up
            pan.signoff()
            exit()
         else:
            tst2+=x2r[0]                              # Amend new start time
            x2r=x2r-x2r[0]
      else:
         while x2r[0]+tst2>x1r[0]+tst1:               # Or Hack data off of the start
                                                      # of file 1 until its startpoint
                                                      # matches file 2
            if len(x1r)==0:
               print 'Times domains for files 1 & 2 do not overlap!  Aborting!'
               pan.signoff()
               exit()
            x1r=np.delete(x1r,0)
            y1r=np.delete(y1r,0)
            ye1r=np.delete(ye1r,0)
         if tst1+x1r[0]!=tst2+x2r[0]:
            print 'Starting times for files 1 & 2 do not match!  Aborting!'
            pan.signoff()
            exit()
         else:
            tst1+=x1r[0]
            x1r=x1r-x1r[0]
if nfiles>2:                                          # Checking that start-times of
                                                      # files 1 & 3 match (and thus 2 &
                                                      #  3 also match)
   if tst1!=tst3:
      if tst1>tst3:
         while x1r[0]+tst1>x3r[0]+tst3:               # Hack data off of the start of
                                                      # file 3 until its startpoint
                                                      # matches file 1
            if len(x3r)==0:
               print 'Times domains for files 1 & 3 do not overlap!  Aborting!'
               pan.signoff()
               exit()
            x3r=np.delete(x3r,0)
            y3r=np.delete(y3r,0)
            ye3r=np.delete(ye3r,0)
         if tst1+x1r[0]!=tst3+x3r[0]:
            print 'Starting times for files 1 & 3 do not match!  Aborting!'
            pan.signoff()
            exit()
         else:
            tst3+=x3r[0]                              # Amend new start time
            x3r=x3r-x3r[0]
      else:
         while x3r[0]+tst3>x1r[0]+tst1:               # Or Hack data off of the start
                                                      # of files 1 & 2 until their
                                                      # startpoint matches file 3
            if len(x1r)==0:
               print 'Times domains for files 1 & 3 do not overlap!  Aborting!'
               pan.signoff()
               exit()
            x1r=np.delete(x1r,0)
            y1r=np.delete(y1r,0)
            ye1r=np.delete(ye1r,0)
            x2r=np.delete(x2r,0)
            y2r=np.delete(y2r,0)
            ye2r=np.delete(ye2r,0)
         if tst1+x1r[0]!=tst3+x3r[0]:
            print 'Starting times for files 1 & 3 do not match!  Aborting!'
            pan.signoff()
            exit()
         else:
            tst1+=x1r[0]
            x1r=x1r-x1r[0]

#-----Binning--------------------------------------------------------------------------

if isbininp:
   binning=float(args[-1])                            # Collect binsize input if given
else:
   while True:                                        # Keep asking until good response
                                                      # is given
      try:
         binning=float(raw_input("Enter bin size (s): "))
                                                      # Ask for binsize in dialogue box
         break
      except:
         print 'Invalid bin size input!'
if minbin>max(binning,bsz1,bsz2,bsz3):
   print 'Warning!  User-entered bin is smaller than the minimum!'
   print 'Minimum can be changed in the user-input section of this code'
binning=max(binning,bsz1,bsz2,bsz3,minbin)            # Prevent overbinning by setting
                                                      # minimum binning to the maximum
                                                      # of the binnings of the files
print ''
print 'Bin size='+str(binning)+'s'  
print 'Binning File 1...'
x1,y1,ye1=pan.binify(x1r,y1r,ye1r,binning)            # Bin File 1 using 'binify' in
                                                      # pan_lib
if nfiles>1:
   print 'Binning File 2...'
   x2,y2,ye2=pan.binify(x2r,y2r,ye2r,binning)         # Bin File 2 using 'binify' in
                                                      # pan_lib
   if nfiles>2:
      print 'Binning File 3...'
      x3,y3,ye3=pan.binify(x3r,y3r,ye3r,binning)      # Bin File 3 using 'binify' in
                                                      # pan_lib
print 'Binning complete!'
print ''
wrongsize=False
x3l=len(x1)                                           # Fix to make this work for 2 or
                                                      # 3 mismatched files

#-----Force file lengths to match------------------------------------------------------

if nfiles>1:                                          # Checking file lengths match
   if len(x1)!=len(x2):
      print 'Warning!  Files 1&2 of different lengths!'
      wrongsize=True
if nfiles==3:
   if len(x1)!=len(x3):
      print 'Warning!  Files 1&3 of different lengths!'
      wrongsize=True
      x3l=len(x3)
if wrongsize:                                         # Forcing file lengths to match
                                                      # if possible
   print 'Attempting to crop files...'
   mindex=min(len(x1),len(x2),x3l)-1
   if x1[mindex]!=x2[mindex]:
      print 'Cannot crop, aborting!'
      pan.signoff()
      exit()
   if nfiles==3:
      if x1[mindex]!=x3[mindex]:
         print 'Cannot crop, aborting!'
         pan.signoff()
         exit()
   mindex+=1
   x1=x1[:mindex]
   y1=y1[:mindex]
   ye1=ye1[:mindex]
   x2=x2[:mindex]
   y2=y2[:mindex]
   ye2=ye2[:mindex]
   if nfiles==3:
      x3=x3[:mindex]
      y3=y3[:mindex]
      ye3=ye3[:mindex]
   print 'Cropped succesfully!'
   print ''
   
#-----Fetch GTI Mask-------------------------------------------------------------------

print 'Fetching GTI mask...'
def getmask(xarr,gtis):
   if gtis is None:
       return np.array([True]*len(xarr))              # Assume all is in gtis if
                                                      # loaded from csv
   else:
       return pan.gtimask(xarr,gtis)
gmask=getmask(x1,gti)                                 # A mask to blank values that
                                                      # fall outside of the GTIs
if nfiles>1:
   gmask2=getmask(x2,gti2)                            # 'And' masks for different files
   gmask=gmask&gmask2
if nfiles>2:
   gmask3=getmask(x3,gti3)    
   gmask=gmask&gmask3
print str(int(100*sum(gmask)/len(gmask)))+'% of data within GTI!'
print ''

#-----Fetch Colours--------------------------------------------------------------------

def colorget(verbose=True):                           # Define colorget to easily re-
                                                      # obtain colours if base data is
                                                      # modified
   if verbose:
      print 'Analysing Data...'
   times=x1[gmask]
   timese=np.zeros(len(times))
   ys={}
   yes={}
   col={}
   cole={}
   if nfiles==1:                                      # If only one file given, flux
                                                      # and flux_error are just the
                                                      # flux and error of this one file
      flux=y1[gmask]                                  # Use gmask to clip out the areas
                                                      # outside of GTI
      fluxe=ye1[gmask]
   elif nfiles==2:
      flux,fluxe,ys,yes,col,cole=pan.pdcolex2(y1,y2,ye1,ye2,gmask)
                                                      # Get 2/1 and 1/2 colour info
                                                      # using PDColEx in pan_lib
   elif nfiles==3:
      flux,fluxe,ys,yes,col,cole=pan.pdcolex3(y1,y2,y3,ye1,ye2,ye3,gmask)
                                                      # Get ALL colour values with 3D
                                                      # PDColEx
   else:
      print 'Error!  Too much data somehow.'          # This warning should never come
                                                      # up...
      pan.signoff()
      exit()
   return times,timese,flux,fluxe,ys,yes,col,cole
times,timese,flux,fluxe,ys,yes,col,cole=colorget()    # Use colorget
print 'Done!'
print ''

#-----Setting up plot environment------------------------------------------------------

show_block=False                                      # Do not force plots to stay open
                                                      # by default
plotopt=''
es=True                                               # Options to keep track of what
                                                      # form the data is in.  'es':
                                                      # with error bars.
cs=False                                              # 'cs' with colour key
ls=False                                              # 'ls' with delineation
folded=False                                          # 'folded' has been folded over
                                                      # some period
saveplots=False
def doplot(x,xe,y,ye,ovr=False,ft='-k',per2=False):   # Defining short function to
                                                      # determine whether errorbars are
                                                      # needed on the fly
                                                      # 'ovr' allows to override colour
                                                      # and line options so lightcurves
                                                      # can be made differently
   if ovr: formst=ft                                  # If override given, accept input
                                                      # format; if none given, just
                                                      # plot lines
   elif ls: formst='-ok'                              # If deLineate mode on, connect
                                                      # points with lines and mark
                                                      # points
   else: formst='ok'                                  # If neither deLineate nor
                                                      # override on, just plot points.
   if ls and not ovr:
      plotx=np.append(x,x[0])
      ploty=np.append(y,y[0])
      plotxe=np.append(xe,xe[0])
      plotye=np.append(ye,ye[0])
   elif per2 and ovr:
      plotx=np.append(x,x+1.0)
      ploty=np.append(y,y)
      plotxe=np.append(xe,xe)
      plotye=np.append(ye,ye)
      pl.axvline(1,color='0.7',linestyle=':')      
   else:
      plotx=x
      ploty=y
      plotxe=xe
      plotye=ye
   if cs and not ovr:                                 # If coloured mode on, colour
                                                      # first 5 data points unless
                                                      # override given
      if len(x)<5:                                    # Abort if less than 5 data
                                                      # points present
         print 'Not enough data to colour!'
      else:
         pl.plot(x[0],y[0],'or',zorder=1)             # Plot a round marker over each
                                                      # of the first five points with
                                                      # colour ascending red->blue
         pl.plot(x[1],y[1],'oy',zorder=2)
         pl.plot(x[2],y[2],'og',zorder=3)
         pl.plot(x[3],y[3],'oc',zorder=4)
         pl.plot(x[4],y[4],'ob',zorder=5)
   if es:
      pl.errorbar(plotx,ploty,xerr=plotxe,yerr=plotye,fmt=formst,zorder=0)
                                                      # Plot errorbar plot if errors
                                                      # turned on
   else:
      pl.plot(plotx,ploty,formst,zorder=0)            # Else plot regular graph
def plot_save(saveplots,show_block):                  # Add a function to redirect all
                                                      # show calls to savefigs if
                                                      # toggled
   if saveplots:
      pl.savefig(raw_input('Save plot as: '))
      print 'Plot saved!'
   else:
      pl.show(block=show_block)
def burstplot(key,text,units):
   if bursts is None:
      print 'No burst data to plot!  Run "burst get" first!'
      return
   print 'Plotting Histogram of Burst '+text+'...'
   pl.figure()
   pl.hist(bursts[key],bins=np.arange(min(bursts[key]),max(bursts[key]),
   (max(bursts[key])-min(bursts[key]))/21.0))
   pl.xlabel(text,'('+units+')')
   pl.ylabel('Frequency')
   pl.title('Histogram of Burst '+text)
   plot_save(saveplots,show_block)
fldtxt=''
bursts=None
burst_alg='cubic spline'
flux_axis=r'Flux (cts s$^{-1}$ PCU$^{-1}$)'
time_n=0                                              # Normalise dump time

#-----User Menu------------------------------------------------------------------------

def give_inst():                                      # Define printing this list of
                                                      # instructions as a function
   print 'COMMANDS: Enter a command to manipulate data.'
   print ''
   print 'DATA:'
   print '* "rebin" to reset the data and load it with a different binning.'
   print '* "clip" to clip the data.'
   print '* "norm time" to renormalise the times by the start time of the data'
   print '* "mask" to remove a range of data.'
   print '* "rms" to return the fractional rms of the data.'
   print '* "fold" to fold data over a period of your choosing'+(' (requires PyAstron'+
         'omy module!)' if not module_pyastro else '')+'.'
   print '* "autofold" to automatically seek a period over which to fold data'+(' (re'+
         'quires PyAstronomy module!)' if not module_pyastro else '')+'.'
   print '* "varifold" to fold over a non-constant period using an algorithm optimise'+
         'd for high-amplitude quasi-periodic flares.'
   print '* "plot bursts" to plot the results of the peak-finding algorithm used in v'+
         'arifold.'
   print ''
   print '1+ DATASET PLOTS:'
   print '* "lc" to plot a simple graph of flux over time.'
   print '* "bg" to plot background over time, if background has been estimated for t'+
         'hese files.'
   print '* "animate" to create an animation of the lightcurve as the binning is incr'+
         'eased.'
   print '* "circanim" to create an animation of the lightcurve circularly folded as '+
         'the period is increased.'
   print '* "lombscargle" to create a Lomb-Scargle periodogram of the lightcurve.'
   print '* "autocor" to plot the auto-correlation function.'
   print '* "rmsflux" to plot the rms-flux relationship of the data.'
   if nfiles>1:                                       # Only display 2-data-set inst-
                                                      # -ructions if 2+ datasets given
      print ''
      print '2+ DATASET PLOTS:'
      print '* "hardness21" to plot a hardness/time diagram of file2/file1 colour ove'+
            'r time.'
      print '* "hardness12" to plot a hardness/time diagram of file1/file2 colour ove'+
            'r time.'
      print '* "hid21" to plot a hardness-intensity diagram of file2/file1 colour aga'+
            'inst total flux.'
      print '* "hid12" to plot a hardness-intensity diagram of file1/file2 colour aga'+
            'inst total flux.'
      print '* "calcloop21" to return the probability of a null hysteresis in the 12 '+
            'HID.'
      print '* "col21" to plot file2/file1 colour against time.'
      print '* "col12" to plot file1/file2 colour against time.'
      print '* "band" to plot the lightcurve of a single energy band.'
      print '* "bands" to plot lightcurves of all bands on adjacent axes.'
      print '* "xbands" to plot lightcurves of all bands on the same axes.'
      print '* "compbands21" to plot lightcurves of bands 2 and 1 against each other.'
      print '* "crosscor21" to plot the cross-correlation function of band 1 with ban'+
            'd 2.'
      print '* "timeres crosscor21" to plot the time-resolved cross-correlation funct'+
            'ion of band 1 with band 2' 
      print '* "all" to plot all available data products.'
   if nfiles==3:                                      # Only display 3-data-set inst-
                                                      # -ructions if 3 datasets given
      print ''
      print '3 DATASET PLOTS:'
      print '* "hardness32" to plot a hardness/time diagram of file3/file2 colour ove'+
            'r time.'
      print '* "hardness23" to plot a hardness/time diagram of file2/file3 colour ove'+
            'r time.'
      print '* "hardness31" to plot a hardness/time diagram of file3/file1 colour ove'+
            'r time.'
      print '* "hardness13" to plot a hardness/time diagram of file1/file3 colour ove'+
            'r time.'
      print '* "hid32" to plot a hardness-intensity diagram of file3/file2 colour aga'+
            'inst total flux.'
      print '* "hid23" to plot a hardness-intensity diagram of file2/file3 colour aga'+
            'inst total flux.'
      print '* "calcloop32" to return the probability of a null hysteresis in the 32 '+
            'HID.'
      print '* "hid31" to plot a hardness-intensity diagram of file3/file1 colour aga'+
            'inst total flux.'
      print '* "hid13" to plot a hardness-intensity diagram of file1/file3 colour aga'+
            'inst total flux.'
      print '* "calcloop31" to return the probability of a null hysteresis in the 31 '+
            'HID.'
      print '* "col32" to plot file3/file2 colour against time.'
      print '* "col23" to plot file2/file3 colour against time.'
      print '* "col31" to plot file3/file1 colour against time.'
      print '* "col13" to plot file1/file3 colour against time.'
      print '* "compbands31" to plot lightcurves of bands 3 and 1 against each other.'
      print '* "compbands32" to plot lightcurves of bands 3 and 2 against each other.'
      print '* "ccd" to plot a colour-colour diagram (3/1 colour against 2/1 colour).'
      print '* "timeres crosscor31" to plot the time-resolved cross-correlation funct'+
            'ion of band 3 with band 1'
      print '* "timeres crosscor32" to plot the time-resolved cross-correlation funct'+
            'ion of band 3 with band 2'
      print '* "crosscor31" to plot the cross-correlation function of band 3 with ban'+
            'd 1.'
      print '* "crosscor32" to plot the cross-correlation function of band 3 with ban'+
            'd 2.'
   print ''
   print 'BURST ANALYSIS:'
   print '* "burst get" to interactively extract burst data for analysis.'
   print '* "burst peaks" for a histogram of peak heights of extracted bursts.'
   print '* "burst risetimes" for a histogram of rise times of extracted bursts.'
   print '* "burst falltimes" for a histogram of fall times of extracted bursts.'
   print '* "burst lengths" for a histogram of durations of extracted bursts.'
   print '* "burst help" for further information on burst analysis.'
   print ''
   print 'SAVING DATA TO ASCII:'
   print '* "export" to dump the lightcurve and colour data into an ASCII file.'
   print '* "bgdump" to export background lightcurve to an ASCII file.'
   print '* "timenorm" to toggle absolute or relative time values on x-axis.'
   print ''
   print 'TOGGLE OPTIONS:'
   print '* "errors" to toggle whether to display errors in plots.'
   print '* "lines" to toggle lines joining points in graphs.'
   print '* "ckey" to toggle colour key (red-blue) for the first five points in all p'+
         'lots.'
   print '* "save" to save to disk any plots which would otherwise be shown.'
   print ''
   print 'ADVANCED OPTIONS:'
   print '* "burstalg" to select algorithm for finding pulse peaks in lightcurve.'
   print ''
   print 'OTHER COMMANDS:'
   print '* "info" to display a list of facts and figures about the current PlotDemon'+
         ' session.'
   print '* "reflav" to rewrite the flavour text used for graph titles.'
   print '* "help" or "?" to display this list of instructions again.'
   print '* "quit" to quit.'

#give_inst()                                          # Print the list of instructions
print ''
print ' --------------------'

#-----Entering Interactive Mode--------------------------------------------------------

while plotopt not in ['quit','exit']:                 # If the previous command given
                                                      # was not quit, continue
   print ''
   plotopt=raw_input('Give command [? for help]: ').lower() # Fetch command from user
   print ''

   #-----Aliasing options--------------------------------------------------------------

   if plotopt=='shid':                                # 'shid' refers to the 2/1 HID
      plotopt='hid21'
   elif plotopt=='hhid':                              # 'hhid' refers to the 3/1 HID
      plotopt='hid31'
   elif plotopt=='hid' and nfiles==2:
      plotopt='hid21'                                 # 'hid' refers to the 2/1 HID if
                                                      # that is the only HID available

   #-----Hidden 'stick' option---------------------------------------------------------

   if plotopt=='stick':                               # For use when scripting with
                                                      # Plotdemon.  If turned on, this
                                                      # causes all plots to block when
                                                      # shown.
      show_block=not show_block
      if show_block:
         print 'Sticky Plots on!'
      else:
         print 'Sticky Plots off!'

   #-----'save' option-----------------------------------------------------------------

   elif plotopt=='save':                              # Causes a plot to be saved when
                                                      # it would otherwise have been
                                                      # shown
      saveplots=not saveplots
      if saveplots:
         print 'Plot saving on!'
      else:
         print 'Plot saving off!'

   #-----'rebin' option----------------------------------------------------------------

   elif plotopt=='rebin':                             # Rebin data
      bursts=None                                     # Remove burst data
      fldtxt=''
      while True:                                     # Keep asking until a good
                                                      # response is given
         try:
            binning=float(raw_input("Enter bin size (s): "))      # Ask for binsize in
                                                                  #dialogue box
            assert binning>=minbin
            break
         except:
            print 'Invalid bin size input!'
      print 'Binning File 1...'
      x1,y1,ye1=pan.binify(x1r,y1r,ye1r,binning)          # Bin File 1 using 'binify'
                                                          # in pan_lib
      if nfiles>1:
         print 'Binning File 2...'
         x2,y2,ye2=pan.binify(x2r,y2r,ye2r,binning)       # Bin File 2 using 'binify'
                                                          # in pan_lib
         if nfiles>2:
            print 'Binning File 3...'
            x3,y3,ye3=pan.binify(x3r,y3r,ye3r,binning)    # Bin File 3 using 'binify'
                                                          # in pan_lib
      if nfiles>1:                                    # Checking file lengths match
         if len(x1)!=len(x2):
            wrongsize=True
      if nfiles==3:
         if len(x1)!=len(x3):
            wrongsize=True
            x3l=len(x3)
      if wrongsize:                                   # Forcing file lengths to match
                                                      # if possible
         mindex=min(len(x1),len(x2),x3l)-1
         if x1[mindex]!=x2[mindex]:
            print 'Cannot crop, aborting!'
            pan.signoff()
            exit()
         if nfiles==3:
            if x1[mindex]!=x3[mindex]:
               print 'Cannot crop, aborting!'
               pan.signoff()
               exit()
         mindex+=1
         x1=x1[:mindex]
         y1=y1[:mindex]
         ye1=ye1[:mindex]
         x2=x2[:mindex]
         y2=y2[:mindex]
         ye2=ye2[:mindex]
         if nfiles==3:
            x3=x3[:mindex]
            y3=y3[:mindex]
            ye3=ye3[:mindex]
      gmask=getmask(x1,gti)                           # Re-establish gmask
      print 'Binning complete!'
      print ''
      times,timese,flux,fluxe,ys,yes,col,cole=colorget() # Re-get colours
      folded=False                                       # Re-allow clipping
      print 'Done!'
      print ''

   #-----'fold' Option-----------------------------------------------------------------

   elif plotopt=='fold':                              # Fold lightcurve
      bursts=None                                     # Remove burst data
      if folded:
         print 'Data already folded!  Rebin before re-folding.'
         continue
      if not module_pyastro:                          # Only attempt to fold if pyastro
                                                      # is present
         print 'PyAstronomy Module not found!  Cannot perform fold!'# Warn user they
                                                                    # cannot fold as
                                                                    # module is missing
         continue
      while True:                                     # Keep asking user until they
                                                      # give a sensible period
         try:
            period=float(raw_input('Input period to fold over (s): '))   # Fetch period
                                                                         # from user
            break
         except:
            print "Invalid period!"                   # Keep trying until they give a
                                                      # sensible input
      while True:                                     # Keep asking user until they
                                                      # put a sensible phase resolution
         try:
            phres=float(raw_input('Input phase resolution (0-1): ')) # Fetch phase
                                                                     # resolution from
                                                                     # user
            assert phres<1.0
            assert phres>0.0
            break
         except:
            print "Invalid phase resolution!"         # Keep trying until they give a
                                                      # sensible input
      x1=x1[gmask];y1=y1[gmask];ye1=ye1[gmask]        # Zeroing all data points outside
                                                      # of GTI
      x1,y1,ye1=pan.foldify(x1,y1,ye1,period,binning,phres=phres,name='ch. '+ch[1])
                                                      # Fold using foldify function
                                                      # from pan_lib
      fldtxt='Folded '
      if nfiles>1:
         x2=x2[gmask];y2=y2[gmask];ye2=ye2[gmask]     # Zeroing all data points outside
                                                      # of GTI
         x2,y2,ye2=pan.foldify(x2,y2,ye2,period,binning,phres=phres,name='ch. '+ch[2])
                                                      # Fold data of file 2 if present
      if nfiles==3:
         x3=x3[gmask];y3=y3[gmask];ye3=ye3[gmask]     # Zeroing all data points outside
                                                      # of GTI
         x3,y3,ye3=pan.foldify(x3,y3,ye3,period,binning,phres=phres,name='ch. '+ch[3])
                                                      # Fold data of file 3 if present
      gmask=np.ones(len(x1),dtype=bool)               # Re-establish gmask
      times,timese,flux,fluxe,ys,yes,col,cole=colorget()  # Re-get colours
      folded=True
      print 'Folding Complete!'
      print ''

   #-----'norm time' Option------------------------------------------------------------

   elif plotopt=='norm time':
      if folded:
         print 'Cannot renormalise time on folded data!'
         continue
      times=times-times[0]
      print 'Renormalised times!'

   #-----'autofold' Option-------------------------------------------------------------

   elif plotopt=='autofold':                          # Autofold data lightcurve
      bursts=None                                     # Remove burst data
      if folded:
         print 'Data already folded!  Rebin before re-folding.'
         continue
      if not module_pyastro:                          # Only attempt to fold if pyastro
                                                      # is present
         print 'PyAstronomy Module not found!  Cannot perform fold!'
                                                      # Warn user they cannot fold as
                                                      # module is missing
         continue
      ls_st=max(4.0/(times[-1]-times[0]),0.005)
      ls_end=0.5/binning
      lsx=np.arange(ls_st,ls_end,(ls_end-ls_st)/2500.0) # Perform Lomb-Scargle Analysis
                                                        # on the data to seek best
                                                        # period
      lsy=pan.lomb_scargle(times,flux,fluxe,lsx)
      period=1.0/(lsx[lsy.tolist().index(max(lsy))])
      while True:                                     # Keep asking user until they put
                                                      # a sensible phase resolution
         try:
            phres=float(raw_input('Input phase resolution (0-1): '))
                                                      # Get phase resolution from user
            assert phres<1.0
            assert phres>0.0
            break
         except:
            print "Invalid phase resolution!"         # Keep trying until they give a
                                                      # sensible input
      print ''
      print 'Using period of '+str(period)+'!'
      x1=x1[gmask];y1=y1[gmask];ye1=ye1[gmask]        # Zeroing all data points outside
                                                      # of GTI
      x1,y1,ye1=pan.foldify(x1,y1,ye1,period,binning,phres=phres,name='ch. '+ch[1])
                                                      # Fold using foldify function
                                                      # from pan_lib
      fldtxt='Folded '
      if nfiles>1:
         x2=x2[gmask];y2=y2[gmask];ye2=ye2[gmask]     # Zeroing all data points outside
                                                      # of GTI
         x2,y2,ye2=pan.foldify(x2,y2,ye2,period,binning,phres=phres,name='ch. '+ch[2])
                                                      # Fold data of file 2 if present
      if nfiles==3:
         x3=x3[gmask];y3=y3[gmask];ye3=ye3[gmask]     # Zeroing all data points outside
                                                      # of GTI
         x3,y3,ye3=pan.foldify(x3,y3,ye3,period,binning,phres=phres,name='ch. '+ch[3])
                                                      # Fold data of file 3 if present
      gmask=np.ones(len(x1),dtype=bool)               # Re-establish gmask
      times,timese,flux,fluxe,ys,yes,col,cole=colorget()       # Re-get colours
      folded=True
      print 'Folding Complete!'
      print ''

   #-----'varifold' Option-------------------------------------------------------------

   elif plotopt=='varifold':
      if folded:
         print 'Cannot perform burst analysis on folded data!'
         continue
      if burst_alg=='cubic spline':
         while True:
            try:
               iq_lo=float(raw_input('Low Threshold:  '))
               iq_hi=float(raw_input('High Threshold: '))
               assert iq_hi>iq_lo
               assert iq_hi<=100
               assert iq_lo>=0
               break
            except AssertionError:
               print 'Invalid Entry!  Valid entry is of the form High>Low.'
      else:
         iq_lo=0
         iq_hi=100
      while True:
         try:
            phase_res=float(raw_input('Input phase resolution (0-1): '))
            assert phase_res<1.0
            assert phase_res>0.0
            break
         except AssertionError:
            print 'Invalid Phase Resolution!'
      phases,numpeaks,flpeaks=pan.fold_bursts(times,flux,iq_hi,iq_lo,do_smooth=False,
                                              alg=burst_alg,savgol=5)
      peaksep=(times[-1]-times[0])/numpeaks
      print numpeaks,'flares identified: average separation of',str(peaksep)+'s'
      st_time=flpeaks[0]
      endtime=flpeaks[1]
      intran=np.array(range(len(times)))
      ymask1=intran>=st_time
      ymask2=intran<endtime
      ymask=ymask1&ymask2
      nbins=int(1.0/phase_res)
      print len(phases),len(x1),len(times)
      print len(gmask)
      phases=(nbins*phases[ymask]).astype(int)
      x1=x1[gmask][ymask];y1=y1[gmask][ymask];ye1=ye1[gmask][ymask]
                                                      # Removing all data points
                                                      # outside of GTI
      newx1=[]
      newy1=[]
      newye1=[]
      for i in range(nbins):
         newx1.append(float(i)/float(nbins))
         newy1.append(np.mean(y1[phases==i]))
         newye1.append((np.sum(ye1[phases==i]**2))**0.5/len(ye1[phases==i]))
      x1=np.array(newx1)
      #y1=sig.savgol(np.array(newy1),5,3)
      #ye1=sig.savgol(np.array(newye1),5,3)
      y1=np.array(newy1)
      ye1=np.array(newye1)
      if nfiles>1:
         x2=x2[gmask][ymask];y2=y2[gmask][ymask];ye2=ye2[gmask][ymask]
                                                      # Removing all data points
                                                      # outside of GTI
         newx2=[]
         newy2=[]
         newye2=[]
         for i in range(nbins):
            newx2.append(float(i)/float(nbins))
            newy2.append(np.mean(y2[phases==i]))
            newye2.append((np.sum(ye2[phases==i]**2))**0.5/len(ye2[phases==i]))
         x2=np.array(newx2)
         y2=np.array(newy2)
         ye2=np.array(newye2)
      if nfiles==3:
         x3=x3[gmask][ymask];y3=y3[gmask][ymask];ye3=ye3[gmask][ymask]
                                                      # Removing all data points
                                                      # outside of GTI
         newx3=[]
         newy3=[]
         newye3=[]
         for i in range(nbins):
            newx3.append(float(i)/float(nbins))
            newy3.append(np.mean(y3[phases==i]))
            newye3.append((np.sum(ye3[phases==i]**2))**0.5/len(ye3[phases==i]))
         x3=np.array(newx3)
         y3=np.array(newy3)
         ye3=np.array(newye3)
      gmask=np.ones(len(x1),dtype=bool)               # Re-establish gmask
      times,timese,flux,fluxe,ys,yes,col,cole=colorget()       # Re-get colours
      folded=True
      print 'Folding Complete!'
      print ''
      period='N/A'

   #-----Get GTIs----------------------------------------------------------------------

   elif plotopt=='get gtis':
      if folded:
         print 'Cannot perform burst analysis on folded data!'
         continue
      if burst_alg=='cubic spline':
         while True:
            try:
               iq_lo=float(raw_input('Low Threshold:  '))
               iq_hi=float(raw_input('High Threshold: '))
               assert iq_hi>iq_lo
               assert iq_hi<=100
               assert iq_lo>=0
               break
            except AssertionError:
               print 'Invalid Entry!  Valid entry is of the form High>Low.'
      else:
         iq_lo=0
         iq_hi=100
      while True:
         try:
            nphbins=int(raw_input('Number of phase bins: '))
            assert nphbins>1
            break
         except AssertionError:
            print 'Invalid Phase Resolution!'
      spline=pan.get_phases_intp(flux,windows=1,q_lo=iq_lo,q_hi=iq_hi,peaks=None,
                                 givespline=True)
      start_valid=times[spline.firstpeak]+tst1
      end_valid=times[spline.lastpeak]+tst1
      numpeaks=int(spline(spline.lastpeak))
      print 'Spline created, extracting phases...'
      print ''
      flnm_prefix=raw_input('Filename Prefix: ')
      gtif={}
      for i in range(nphbins):
          gtif[i]=open(flnm_prefix+'_'+str(i)+'.csv','w')
      guess=spline(0)          
      prevcut=optm.fsolve(spline,guess)[0]*binning+tst1
      for i in range(numpeaks):
          for j in range(nphbins):
              subval=i+((j+1)/float(nphbins))
              def newspline(x):
                  return spline(x)-subval
              newcut=optm.fsolve(newspline,guess)[0]*binning+tst1
              if newcut<end_valid and prevcut>start_valid:
                 gtif[j].write(str(prevcut)+','+str(newcut)+'\n')
              prevcut=newcut     
      for i in range(nphbins):
          gtif[i].close()
      print ''
      print 'GTI files written!'

   #-----'Plot Bursts' Option----------------------------------------------------------

   elif plotopt=='plot bursts':
      if folded:
         print 'Cannot perform burst analysis on folded data!'
         continue
      while True:
         try:
            q_lo=float(raw_input('Low Threshold : '))
            assert q_lo<100
            assert q_lo>0
            break
         except:
            pass
      while True:
         try:
            q_hi=float(raw_input('High Threshold: '))
            assert q_hi<100
            assert q_hi>0
            assert q_hi>=q_lo
            break
         except:
            pass
      peaks=pan.get_bursts_windowed(flux,1,q_lo=q_lo,q_hi=q_hi,smooth=False)
      pl.figure()
      doplot(times,timese,flux,fluxe,ovr=True,per2=False)
      for i in peaks:
         pl.plot([times[i]],[flux[i]],'g*',zorder=5)
      pl.axhline(np.percentile(flux,q_lo),color='b',zorder=2)
      pl.axhline(np.percentile(flux,q_hi),color='r',zorder=2)
      plot_save(saveplots,show_block)

   #-----'clip' Option-----------------------------------------------------------------

   elif plotopt=='clip':                              # Clipping data
      bursts=None                                     # Remove burst data
      if folded:
         print 'Cannot clip folded data!'
      else:
         print 'Clipping data'
         print ''
         print 'Time range is '+str(x1[0])+'s - '+str(x1[-1])+'s'
         print 'Please choose new range of data:'
         mint,maxt,srbool=pan.srinr(x1,binning,'time')# Fetch new time domain endpoints
                                                      # using srinr function from
                                                      # pan_lib
         if srbool:
            print 'Clipping...'
            x1=x1[mint:maxt]                          # Clip file 1
            y1=y1[mint:maxt]
            ye1=ye1[mint:maxt]
            if nfiles>1:
               x2=x2[mint:maxt]                       # Clip file 2
               y2=y2[mint:maxt]
               ye2=ye2[mint:maxt]
            if nfiles==3:
               x3=x3[mint:maxt]                       # Clip file 3
               y3=y3[mint:maxt]
               ye3=ye3[mint:maxt]
            gmask=getmask(x1,gti)                     # Re-establish gmask
            times,timese,flux,fluxe,ys,yes,col,cole=colorget() # Re-get colours
            print 'Data clipped!'

   #-----'mask' Option-----------------------------------------------------------------

   elif plotopt=='mask':
      bursts=None                                     # Remove burst data
      if folded:
         print 'Cannot mask folded data!'
      else:
         print 'Masking data'
         print ''
         print 'Select time range to mask: '
         mint,maxt,srbool=pan.srinr(x1,binning,'time')# Fetch time domain endpoints of
                                                      # bad window using srinr function
                                                      # from pan_lib
         if srbool:
            print 'Masking...'
            gmask[mint:maxt]=False                    # Force all values inside the bad
                                                      # window to appear as outside of
                                                      # GTIs
            times,timese,flux,fluxe,ys,yes,col,cole=colorget()    # Re-get colours
            print 'Data masked!'

   #-----'rms' Option------------------------------------------------------------------

   elif plotopt=='rms':
      if nfiles>1:                                    # If more than one file loaded,
                                                      # prompt user to select one
         if nfiles==3:
            is_band_3=', 3'
         else:
            is_band_3=''
         selected_band=raw_input('Select Energy Band [1, 2'+is_band_3+', All]: '
                                ).lower()
         if selected_band not in ['1','2','3','all']:
            print 'Invalid band!'
            continue
      else:
         selected_band='all'
      if selected_band=='1':                          # Fetch the rms
         rms=pan.rms(y1)
      elif selected_band=='2':
         rms=pan.rms(y2)
      elif selected_band=='3':
         rms=pan.rms(y3)
      else:
         rms=pan.rms(flux)
      if rms=='div0':                                 # If the mean of the data is 0,
                                                      # fail safely
         print 'Error!  Div 0 Encountered!  Aborting!'
      else:
         print 'rms =',str(rms*100)+'%'               # Otherwise, print RMS

   #-----'rmsflux' Option--------------------------------------------------------------

   elif plotopt=='rmsflux':
      try:
         trmsbin=float(raw_input('Time binning: '))
         frmsbin=float(raw_input('Rate binning: '))
      except:
         print 'Invalid value(s)!'
         continue
      ilen=int(trmsbin/binning)
      numbins=int(len(times)/ilen)
      frflux=[]
      frrms_=[]
      frrmse=[]
      for i in range(numbins):
         kst=i*ilen
         ked=(i+1)*ilen
         kflux=flux[kst:ked]
         kfluxe=fluxe[kst:ked]
         arms,armse=pan.rms(kflux,data_err=kfluxe,with_err=True)
         frrms_.append(arms)
         frrmse.append(armse)
         frflux.append(np.mean(kflux))
      frflux=np.array(frflux)
      frrms_=np.array(frrms_)*frflux
      frrmse=np.array(frrmse)*frflux
      cflux=[]
      crms_=[]
      crmse=[]
      top=int(max(frflux)/frmsbin) +1
      for i in range(0,top):
         flow=i*frmsbin
         f_hi=(i+1)*frmsbin
         mask=np.logical_and(frflux>=flow,frflux<f_hi)
         if np.sum(mask)==0:
            continue
         mask=np.logical_and(mask,np.logical_not(np.isnan(frrms_)))
         cflux.append((flow+f_hi)/2.0)
         crms_.append(np.mean(frrms_[mask]))
         crmse.append(np.sqrt(np.sum(frrmse[mask])**2 )/sum(mask))
      pl.figure()      
      pl.errorbar(cflux,crms_,yerr=crmse,fmt='x',color='0.7',zorder=20)
      pl.plot(cflux,crms_,'kx',zorder=25)
      pl.xlabel('Rate (cts s$^{-1}$')
      pl.ylabel('RMS (cts s$^{-1}$')
      plot_save(saveplots,show_block) 

   #-----'lc' Option-------------------------------------------------------------------

   elif plotopt=='lc':                                # Plot lightcurve
      taxis='Phase' if folded else 'Time (s)'
      pl.figure()
      doplot(times,timese,flux,fluxe,ovr=True,per2=folded)     # Plot flux/time using
                                                               # doplot from pan_lib
      pl.xlabel(taxis)
      pl.ylabel(flux_axis)
      pl.ylim(ymin=0)
      pl.title(fldtxt+'Lightcurve'+qflav)
      plot_save(saveplots,show_block)
      print 'Lightcurve plotted!'

   #-----'bg' Option-------------------------------------------------------------------

   elif plotopt=='bg':                                # Plot background lightcurve
      is_bdata=(bdata1!=None)
      if nfiles>=2:
         is_bdata=is_bdata and (bdata2!=None)
      if nfiles==3:
         is_bdata=is_bdata and (bdata3!=None)
      if is_bdata:                                    # Only proceed if all infiles
                                                      # have background data
         try:
            bdata_x=bdata1[0]
            bdata_y=bdata1[1]
            if nfiles>=2:
               bdata_y+=bdata2[1]
            if nfiles==3:
               bdata_y+=bdata3[1]                     # Sum counts of all <=3
                                                      # backgrounds 
            pl.figure()
            pl.plot(bdata_x,bdata_y)
            pl.xlabel('Time (s)')
            pl.ylabel(flux_axis)
            pl.ylim(ymin=0)
            pl.title(fldtxt+'Lightcurve'+qflav)
            plot_save(saveplots,show_block)
            print 'Background plotted!'
         except:
            print 'Backgrounds inconsistenly formatted!' # Abort if backgrounds are
                                                         # somehow of different lengths
      else:
         print 'Not all files have background data!'

   #-----'bgdump' Option---------------------------------------------------------------

   elif plotopt=='bgdump':                            # Export background lightcurve
      is_bdata=(bdata1!=None)
      if nfiles>=2:
         is_bdata=is_bdata and (bdata2!=None)
      if nfiles==3:
         is_bdata=is_bdata and (bdata3!=None)
      if is_bdata:                                    # Only proceed if all infiles
                                                      # have background data
         try:
            bdata_x=bdata1[0]
            bdata_y=bdata1[1]
            if nfiles>=2:
               bdata_y+=bdata2[1]
            if nfiles==3:
               bdata_y+=bdata3[1]                     # Sum counts of all <=3
                                                      # backgrounds
         except:
            print 'Backgrounds inconsistenly formatted!' # Abort if backgrounds are
                                                         # somehow of different lengths
            continue
         ofilename=raw_input('Save textfile as: ')    # Fetch filename from user
         ofil = open(ofilename, 'w')                  # Open file
         for i in range(len(bdata_x)):
            row=['0.0']*3                             # Create a row of strings reading
                                                      # 0.0, append data into it
            row[0]=str(bdata_x[i]+time_n)+' '         # Column 01: Time
            row[1]=str(bdata_y[i])+' '                # Column 02: Rate
            row[2]='\n'
            ofil.writelines(row) 
         print 'Background saved as '+ofilename+'!'
      else:
         print 'Not all files have background data!'

   #-----'export' Option---------------------------------------------------------------

   elif plotopt=='export':                            # Export lightcurve to ASCII file
      ofilename=raw_input('Save textfile as: ')       # Fetch filename from user
      ofil = open(ofilename, 'w')                     # Open file
      if folded:
         itr=[0,1]
      else:
         itr=[0]
      for ti in itr:
         for i in range(len(times)):
            row=['0.0 ']*15                           # Create a row of strings reading
                                                      #  0.0, append data into it
            row[0]=str(times[i]+time_n+ti)+' '        # Column 00: Time
            row[1]=str(timese[i])+' '                 # Column 01: Time Error
            row[2]=str(flux[i])+' '                   # Column 02: Total Flux
            row[3]=str(fluxe[i])+' '                  # Column 03: Total Flux Error
            row[4]=str(y1[gmask][i])+' '              # Column 04: Band 1 Flux
            row[5]=str(ye1[gmask][i])+' '             # Column 05: Band 1 Flux Error
            row[14]='\n'                              # Column 14: Return (so further
                                                      #  data will be appended to a new
                                                      #  line)
            if nfiles>1:                              # If 2+ bands are given:
               row[6]=str(y2[gmask][i])+' '           # Column 06: Band 2 Flux
               row[7]=str(ye2[gmask][i])+' '          # Column 07: Band 2 Flux Error
               row[10]=str(col[21][i])+' '            # Column 10: [2/1] Colour
               row[11]=str(cole[21][i])+' '           # Column 11: [2/1] Colour Error
            if nfiles==3:                             # If 3 bands are given:
               row[8]=str(y3[gmask][i])+' '           # Column 08: Band 3 Flux
               row[9]=str(ye3[gmask][i])+' '          # Column 09: Band 3 Flux Error
               row[12]=str(col[31][i])+' '            # Column 12: [3/1] Colour
               row[13]=str(cole[31][i])+' '           # Column 13: [3/1] Colour Error
            ofil.writelines(row)                      # Append row of data into open
                                                      #  file
      ofil.close()                                    # Close file
      print 'Data saved!'

   #-----'timenorm' Option-------------------------------------------------------------
   
   elif plotopt=='timenorm':
      print ''
      if time_n==0:
         time_n=tst1
         print 'Using absolute time!'
      else:
         time_n=0
         print 'Using relative time!'
      print ''

   #-----'animate' Option--------------------------------------------------------------

   elif plotopt=='animate':
      animsloc=raw_input('Folder to save images: ')
      print ''
      if os.path.exists(animsloc):                    # Create the folder
         print 'Folder "'+animsloc+'" already exists...'
      else:
         print 'Creating folder "'+animsloc+'"...'
         os.makedirs(animsloc)
      here=os.getcwd()                                # Get current working directory
                                                      # (to move back to later)
      os.chdir(animsloc)                              # Change working directory to
                                                      # animation location
      animbin=0.0025                                  # Start with an arbitrarily low
                                                      # binsize
      while animbin<max(bsz1,bsz2,bsz3,minbin):       # Find lowest allowable binsize
                                                      # of the form 0.01*2^N
         animbin=animbin*2
      anstep=1                                        # Track the number of steps taken
      dst=times[0]                                    # Fetch largest and smallest
                                                      # times to use to force same
                                                      # scale on all graphs
      det=times[-1]
      while animbin<(det-dst)/4.0:                    # Set the maximum binsize at one
                                                      # quarter of the observation
                                                      # length
         print "Creating",str(animbin)+"s binned lightcurve"
         x1,y1,ye1=pan.binify(x1r,y1r,ye1r,animbin)   # Bin File 1 using 'binify' in
                                                      # pan_lib
         if nfiles>1:
            x2,y2,ye2=pan.binify(x2r,y2r,ye2r,animbin)# Bin File 2 using 'binify' in
                                                      # pan_lib
            if nfiles>2:
               x3,y3,ye3=pan.binify(x3r,y3r,ye3r,animbin)
                                                      # Bin File 3 using 'binify' in
                                                      # pan_lib
         mina,maxa,srbool=pan.srinr(x1,binning,'time',minv=dst,maxv=det)
                                                      # Clip each individual lightcurve
         if srbool:
            x1=x1[mina:maxa]                          # Clip file 1
            y1=y1[mina:maxa]
            ye1=ye1[mina:maxa]
            if nfiles>1:
               x2=x2[mina:maxa]                       # Clip file 2
               y2=y2[mina:maxa]
               ye2=ye2[mina:maxa]
            if nfiles==3:
               x3=x3[mint:maxt]                       # Clip file 3
               y3=y3[mint:maxt]
               ye3=ye3[mint:maxt]
         gmask=getmask(x1,gti)                        # Re-establish gmask
         times,timese,flux,fluxe,ys,yes,col,cole=colorget(verbose=False)
                                                      # Re-get colours
         if anstep==1:
            if es:
              merr=max(fluxe)
            else:
              merr=0
            maxany=max(flux)+merr                     # Calculate the scale of all
                                                      # plots based on the range of the
                                                      # first plot
            minany=min(flux)-merr
            if minany<0: minany=0
         taxis='Phase' if folded else 'Time (s)'
         pl.figure()
         doplot(times,timese,flux,fluxe,ovr=True)     # Plot the graph using doplot
         pl.xlabel(taxis)
         pl.ylabel(flux_axis)
         pl.title('Lightcurve ('+str(animbin)+'s binning)')
         pl.xlim(dst,det)
         pl.ylim(max(minany,0),maxany)
         pl.savefig(str("%04d" % anstep)+'.png')      # Save the figure with leading
                                                      # zeroes to preserve order when
                                                      # int convereted to string
         pl.close()
         anstep+=1                                    # Increment the step tracker
         animbin=animbin*2                            # Double the binsize
      print 'Cleaning up...'
      os.system ("convert -delay 10 -loop 0 *.png animation.gif")
                                                      # Use the bash command 'convert'
                                                      # to create the animated gif
      x1,y1,ye1=pan.binify(x1r,y1r,ye1r,binning)      # Reset binning of File 1 using
                                                      # 'binify' in pan_lib
      if nfiles>1:
         x2,y2,ye2=pan.binify(x2r,y2r,ye2r,binning)   # Reset binning of File 2 using
                                                      # 'binify' in pan_lib
         if nfiles>2:
            x3,y3,ye3=pan.binify(x3r,y3r,ye3r,binning)# Reset binning File 3 using
                                                      # 'binify' in pan_lib
      gmask=getmask(x1,gti)                           # Re-establish gmask
      times,timese,flux,fluxe,ys,yes,col,cole=colorget(verbose=False) # Re-get colours
      print ''
      print "Animation saved to",animsloc+'/animation.gif!'
      os.chdir(here)        


   #-----'hidxy' Option----------------------------------------------------------------

   elif plotopt[:3]=='hid':                           # Plot x/y HID
      ht=plotopt[3:]                                  # Collect the xy token from the
                                                      # user
      if nfiles>1:
         if not (ht in ['12','13','21','23','31','32']):
                                                      # Check that the token is 2 long
                                                      # long and contains two different
                                                      # characters of the set [1,2,3]
            print 'Invalid command!'
            print ''
            print 'Did you mean...'
            print ''
            print 'HID options:'
            print '* "hid21" for 2/1 colour'
            print '* "hid12" for 1/2 colour'
            if nfiles==3:
               print '* "hid32" for 3/2 colour'
               print '* "hid23" for 2/3 colour'
               print '* "hid31" for 3/1 colour'
               print '* "hid13" for 1/3 colour'
         elif ('3' in ht) and (nfiles<3):
            print 'Not enough infiles for advanced HID!'
                                                      # If token contains a 3 but only
                                                      # 2 infiles are used, abort!
         else:
            h1=int(ht[0])                             # Extract numerator file number
            h2=int(ht[1])                             # Extract denominator file number
            ht=int(ht)
            pl.figure()
            doplot(col[ht],cole[ht],flux,fluxe)       # Collect colours from col
                                                      # library and plot
            pl.ylabel(flux_axis)
            pl.xlabel('('+ch[h1]+'/'+ch[h2]+') colour')
            pl.title(fldtxt+'Hardness Intensity Diagram'+qflav)
            plot_save(saveplots,show_block)
            print 'File'+str(h1)+'/File'+str(h2)+' HID plotted!'
      else:
         print 'Not enough infiles for HID!'

   #-----'calcloopxy' Option-----------------------------------------------------------

   elif plotopt[:8]=='calcloop':                      # Calculate likelihood of loop in
                                                      # HID
      ht=plotopt[8:]                                  # Collect the xy token from the
                                                      # user
      if nfiles>1:
         if not (ht in ['12','13','21','23','31','32']): 
                                                      # Check that the token is 2 long
                                                      # and contains two different
                                                      # characters of the set [1,2,3]
            print 'Invalid command!'
            print ''
            print 'Did you mean...'
            print ''
            print 'CalcLoop options:'
            print '* "calcloop21" for 2/1 HID loop calculation'
            print '* "calcloop12" for 1/2 HID loop calculation'
            if nfiles==3:
               print '* "calcloop32" for 3/2  HID loop calculation'
               print '* "calcloop23" for 2/3  HID loop calculation'
               print '* "calcloop31" for 3/1  HID loop calculation'
               print '* "calcloop13" for 1/3  HID loop calculation'
         elif ('3' in ht) and (nfiles<3):
            print 'Not enough infiles for advanced HID!'
                                                      # If token contains a 3 but only
                                                      # 2 infiles are used, abort!
         else:
            h1=int(ht[0])                             # Extract numerator file number
            h2=int(ht[1])                             # Extract denominator file number
            ht=int(ht)
            lkl=pan.calcloop(flux,col[ht],fluxe,cole[ht])
                                                      # Collect likelihood of loop from
                                                      # pan_lib
            print 'Null hypothesis (no hysteresis) probability = '+str(lkl)
      else:
         print 'Not enough infiles for HID!'

   #-----'hardnessxy' Option-----------------------------------------------------------

   elif plotopt[:8]=='hardness':                      # Plot x/y hardness/time plot
      ht=plotopt[8:]                                  # Collect the xy token from the
                                                      # user
      if nfiles>1:
         if not (ht in ['12','13','21','23','31','32']):
                                                      # Check that the token is 2 long
                                                      # and contains two different
                                                      # characters of the set [1,2,3]
            print 'Invalid command!'
            print ''
            print 'Did you mean...'
            print ''
            print 'HID options:'
            print '* "hardness21" for 2/1 hardness over time'
            print '* "hardness12" for 1/2 hardness over time'
            if nfiles==3:
               print '* "hardness32" for 3/2 hardness over time'
               print '* "hardness23" for 2/3 hardness over time'
               print '* "hardness31" for 3/1 hardness over time'
               print '* "hardness13" for 1/3 hardness over time'
         elif ('3' in ht) and (nfiles<3):
            print 'Not enough infiles for advanced HID!'
                                                      # If token contains a 3 but only
                                                      # 2 infiles are used, abort!
         else:
            h1=int(ht[0])                             # Extract numerator file number
            h2=int(ht[1])                             # Extract denominator file number
            ht=int(ht)
            pl.figure()
            doplot(times,timese,col[ht],cole[ht],ovr=True,per2=folded)
                                                      # Collect colours from col
                                                      # library and plot
            pl.ylabel('('+ch[h1]+'/'+ch[h2]+') colour')
            pl.xlabel(taxis)
            pl.title(fldtxt+'Hardness/Time plot'+qflav)
            plot_save(saveplots,show_block)
            print 'File'+str(h1)+'/File'+str(h2)+' Hardness/time diagram plotted!'
      else:
         print 'Not enough infiles for HID!'

   #-----'compbandsxy' Option----------------------------------------------------------

   elif plotopt[:9]=='compbands':                     # Plot two bands against each
                                                      # other
      ht=plotopt[9:]                                  # Collect the xy token from the
                                                      # user
      if nfiles>1:
         if not (ht in ['12','13','21','23','31','32']):
                                                      # Check that the token is 2 long
                                                      # and contains two different
                                                      # characters of the set [1,2,3]
            print 'Invalid command!'
            print ''
            print 'Did you mean...'
            print ''
            print 'HID options:'
            print '* "compbands21" to plot lightcurve 2 against 1'
            print '* "compbands12" to plot lightcurve 1 against 2'
            if nfiles==3:
               print '* "compbands32" to plot lightcurve 3 against 2'
               print '* "compbands23" to plot lightcurve 2 against 3'
               print '* "compbands31" to plot lightcurve 3 against 1'
               print '* "compbands13" to plot lightcurve 1 against 3'
         elif ('3' in ht) and (nfiles<3):
            print 'Not enough infiles for advanced lightcurve comparison!'
                                                      # If token contains a 3 but only
                                                      # 2 infiles are used, abort!
         else:
            h1=int(ht[0])                             # Extract numerator file number
            h2=int(ht[1])                             # Extract denominator file number
            pl.figure()
            doplot(ys[h1],yes[h1],ys[h2],yes[h2])     # Collect colours from col
                                                      # library and plot
            pl.ylabel(r'Band '+str(h1)+r' rate (cts s$^{-1}$ PCU$^{-1}$)')
            pl.xlabel(r'Band '+str(h2)+r' rate (cts s$^{-1}$ PCU$^{-1}$)')
            pl.title(fldtxt+'Lightcurve Comparison Diagram'+qflav)
            plot_save(saveplots,show_block)
            print 'File'+str(h1)+'/File'+str(h2)+' LC Comparison plotted!'
      else:
         print 'Not enough infiles for lightcurve comparison!'

   #-----'autocor' Option--------------------------------------------------------------

   elif plotopt=='autocor':                           # Plot autocorrelation function
      ccor=sig.correlate(flux-np.mean(flux),flux-np.mean(flux),mode='same')/
                         (len(flux)*(np.std(flux)**2))
      nlen=len(times)
      cct=range(nlen)                                 # Set up the lag axis
      cct=((np.array(cct)-nlen/2.0)*binning).tolist()
      pl.figure()
      pl.plot(cct,ccor)
      pl.xlabel('Lag (s)')
      pl.ylabel('Cross-Correlation')
      plot_save(saveplots,show_block)
      print 'Autocorrelation diagram plotted!'

   #-----'crosscor' Option-------------------------------------------------------------

   elif plotopt[:8]=='crosscor':                      # Plot cross-correlation function
      ht=plotopt[8:]                                  # Collect the xy token from the
                                                      # user
      if nfiles>1:
         if not (ht in ['12','13','21','23','31','32']):
                                                      # Check that the token is 2 long
                                                      # and contains two different
                                                      # characters of the set [1,2,3]
            print 'Invalid command!'
            print ''
            print 'Did you mean...'
            print ''
            print 'HID options:'
            print '* "crosscor21" for 2 against 1 cross-correlation'
            if nfiles==3:
               print '* "crosscor32" for 3 against 2 cross-correlation'
               print '* "crosscor31" for 3 against 1 cross-correlation'
         elif ('3' in ht) and (nfiles<3):
            print 'Not enough infiles for advanced cross-correlation!'
                                                      # If token contains a 3 but only
                                                      # 2 infiles are used, abort!
         else:
            nlen=len(times)
            h1=int(ht[0])                             # Extract file 1 number
            h2=int(ht[1])                             # Extract file 2 number
            h1a=ys[h1]                                # Extract dataset 1
            h2a=ys[h2]                                # Extract dataset 2
            if nlen%2==0:                             # Cross-correlation only gives a
                                                      # point at zero lag if an even
                                                      # number of points are input
               nlen-=1
               h1a=h1a[:-1]
               h2a=h2a[:-1]
            ccor=sig.correlate(h1a-np.mean(h1a),h2a-np.mean(h2a),mode='same')/
                               (nlen*np.std(h1a)*np.std(h2a))
            cct=range(nlen)                           # Set up the lag axis
            cct=(((np.array(cct)-nlen/2.0)*binning)+(binning/2.0)).tolist()
            pl.figure()
            pl.plot(cct,ccor)
            pl.axvline(0,linestyle=':',color='0.7')
            pl.xlabel('Band '+str(h1)+' lag (s) wrt Band '+str(h2))
            pl.ylabel('Cross-Correlation')
            plot_save(saveplots,show_block)
            print 'Band '+str(h1)+' lag against band '+str(h2)+
                  ' (cross-correlation) diagram plotted!'
      else:
         print 'Not enough infiles for cross-correlation!'

   #-----'timeres crosscor' Option-----------------------------------------------------

   elif plotopt[:16]=='timeres crosscor':             # Plot time-resolved cross-
                                                      # correlation function
      ht=plotopt[16:]                                 # Collect the xy token from the
                                                      # user
      if nfiles>1:
         if not (ht in ['12','13','21','23','31','32']):
                                                      # Check that the token is 2 long
                                                      # and contains two different
                                                      # characters of the set [1,2,3]
            print 'Invalid command!'
            print ''
            print 'Did you mean...'
            print ''
            print 'HID options:'
            print '* "timeres crosscor21" for 2 against 1 time-resolved cross-correla'+
                  'tion'
            if nfiles==3:
               print '* "timeres crosscor32" for 3 against 2 time-resolved cross-corr'+
                     'elation'
               print '* "timeres crosscor31" for 3 against 1 time-resolved cross-corr'+
                     'elation'
         elif ('3' in ht) and (nfiles<3)
            print 'Not enough infiles for advanced cross-correlation!'
                                                      # If token contains a 3 but only
                                                      # 2 infiles are used, abort!
         else:
            try:
               nlen=int(float(raw_input('Length of segments (s): '))/binning)
               if nlen%2!=0: nlen-=1
               frang=float(raw_input('Max lag (s) : '))
            except:
               print 'Not a number!  Aborting!'
               continue
            maxtims=[]
            maxlocs=[]
            n=len(times)/nlen
            matrix=[]                                 # Set up the 2d matrix
            mtimes=[]                                 # Set up the time axis
            xn=range(nlen)                            # Set up the lag axis
            xn=((np.array(xn)-nlen/2.0)*binning).tolist()
            h1=int(ht[0])                             # Extract file 1 number
            h2=int(ht[1])                             # Extract file 2 number
            for i in range(n):
               y1clip=np.array(ys[h1][nlen*i:nlen*(i+1)]) # Clip the first lightcurve
                                                          # into chunks
               y2clip=np.array(ys[h2][nlen*i:nlen*(i+1)]) # Clip the second lightcurve
                                                          # into chunks
               cclip=sig.correlate(y1clip-np.mean(y1clip),y2clip-np.mean(y2clip),
                               mode='same')/(len(y2clip)*np.std(y1clip)*np.std(y2clip))
                                                          # Do the cross-correlation
               cclip=np.array(cclip)[np.array(xn)<=frang] # Cut the matrix line to the
                                                          # user requested lag range
               txn=np.array(xn)[np.array(xn)<=frang]
               cclip=cclip[txn>-frang]
               txn=txn[txn>-frang]  
               maxc=max(cclip)
               maxloc=txn[cclip.tolist().index(maxc)]
               matrix.append(cclip)                   # Grow the matrix!
               mtimes.append(nlen*i*binning)
               maxtims.append(nlen*(i+0.5)*binning)
               maxlocs.append(maxloc)
            pl.figure()
            pl.pcolor(np.array(mtimes+[mtimes[-1]+binning*nlen])-binning/2.0,txn,
                      np.array(matrix).T)
            pl.xlabel('Time (s)')
            pl.ylabel('Band '+str(h1)+' lag (s) wrt Band '+str(h2))
            pl.plot(maxtims,maxlocs,':k',label='Peak lag')
            pl.legend()
            plot_save(saveplots,show_block)
            print 'Band '+str(h1)+' lag against band '+str(h2)+' (time-resolved cross'+
                  '-correlation) diagram plotted!'
      else:
         print 'Not enough infiles for cross-correlation!'

   #-----'colxy' Option----------------------------------------------------------------

   elif plotopt[:3]=='col':                           # Plot x/y colour/t
      ht=plotopt[3:]                                  # Collect the xy token from the
                                                      # user
      if nfiles>1:
         if not (ht in ['12','13','21','23','31','32']):
                                                      # Check that the token is 2 long
                                                      # and contains two different
                                                      # characters of the set [1,2,3]
            print 'Invalid command!'
            print ''
            print 'Did you mean...'
            print ''
            print 'Col/t plot options:'
            print '*"col21" for 2/1 colour'
            print '*"col12" for 1/2 colour'
            if nfiles==3:
               print '*"col32" for 3/2 colour'
               print '*"col23" for 2/3 colour'
               print '*"col31" for 3/1 colour'
               print '*"col13" for 1/3 colour'
         elif ('3' in ht) and (nfiles<3):
            print 'Not enough infiles for advanced Col/t plot!'
                                                      # If token contains a 3 but only
                                                      # 2 infiles are used, abort!
         else:
            taxis='Phase' if folded else 'Time (s)'
            h1=int(ht[0])                             # Extract numerator file number
            h2=int(ht[1])                             # Extract denominator file number
            ht=int(ht)
            pl.figure()
            doplot(times,timese,col[ht],cole[ht],ovr=True,per2=folded)
                                                      # Collect colours from col
                                                      # library and plot
            pl.xlabel(taxis)
            pl.ylabel('('+ch[h1]+'/'+ch[h2]+') colour')
            pl.ylim(ymin=0)
            pl.title(fldtxt+'Colour over Time Diagram'+qflav)
            plot_save(saveplots,show_block)
            print 'File'+str(h1)+'/File'+str(h2)+' Colour over Time Diagram plotted!'
      else:
         print 'Not enough infiles for Col/t plot!'

   #-----'ccd' Option------------------------------------------------------------------

   elif plotopt=='ccd':                               # Plot Colour-Colour diagram
      if nfiles==3:
         pl.figure()
         doplot(col[31],cole[31],col[21],cole[21])
         pl.xlabel('('+ch[2]+'/'+ch[1]+') colour')
         pl.ylabel('('+ch[3]+'/'+ch[1]+') colour')
         pl.xlim(0,2)
         pl.ylim(0,2)
         pl.title(fldtxt+'Colour-Colour Diagram'+qflav)
         plot_save(saveplots,show_block)
         print 'CCD plotted!'
      else:
         print 'Not enough infiles for CCD!'

   #-----'all' Option------------------------------------------------------------------

   elif plotopt=='all':
      pl.figure()
      if   nfiles==3: colexp=2; rowexp=2; gexp=4      # If 3 files given, 4 graphs will
                                                      # be plotted in a 2x2 grid
      elif nfiles==2: colexp=1; rowexp=2; gexp=2      # If 2 files given, 2 grapgs will
                                                      # be plotted in a 2x1 grid
      else:           colexp=1; rowexp=1; gexp=1      # If 1 file given, only one graph
                                                      # can be plotted
      print 'Plotting Lightcurve...'
      taxis='Phase' if folded else 'Time (s)'
      pl.subplot(rowexp,colexp,1)                     # Create subplot in the first slot
      doplot(times,timese,flux,fluxe,ovr=True)        # Always plot the lightcurve
      pl.xlabel(taxis)
      pl.ylabel(flux_axis)
      pl.ylim(ymin=0)
      pl.title(fldtxt+'Lightcurve'+qflav)
      if nfiles>1:                                    # If 2+ files given, plot 2+ file
                                                      # data products
         print 'Plotting Soft Hardness-Intensity Diagram...'
         pl.subplot(rowexp,colexp,2)                  # Create subplot in the second
                                                      # slot
         doplot(col[21],cole[21],flux,fluxe)          # Plot Soft HID
         pl.xlim(0,2)
         pl.ylim(0,300)
         pl.ylabel(flux_axis)
         pl.xlabel('('+ch[2]+'/'+ch[1]+') colour')
         pl.title(fldtxt+'Soft HID'+qflav)
      if nfiles==3:                                   # If 3 files given, plot 3 file
                                                      # data products
         print 'Plotting Hard Hardness-Intensity Diagram...'
         print 'Plotting Colour-Colour Diagram...'
         pl.subplot(rowexp,colexp,3)                  # Create subplot in the 3rd slot
         doplot(col[31],cole[31],flux,fluxe)          # Plot Hard HID
         pl.xlim(0,2)
         pl.ylim(0,300)
         pl.ylabel(flux_axis)
         pl.xlabel('('+ch[3]+'/'+ch[1]+') colour')
         pl.title(fldtxt+'Hard HID'+qflav)
         pl.subplot(rowexp,colexp,4)                  # Create subplot in the 4th slot
         doplot(col[31],cole[31],col[21],cole[21])    # Plot CCD
         pl.xlim(0,2)
         pl.ylim(0,2)
         pl.ylabel('('+ch[2]+'/'+ch[1]+') colour')
         pl.xlabel('('+ch[3]+'/'+ch[1]+') colour')
         pl.title(fldtxt+'CCD'+qflav)
      print ''
      plot_save(saveplots,show_block)
      print 'All products plotted!'

   #-----'band' Option-----------------------------------------------------------------

   elif plotopt=='band':                              # Plot lightcurve of individual
                                                      # band
      if nfiles==1:
         user_b_band='1'                              # Select energy band to plot
      else:
         if nfiles==3:
            is_band_3=', 3'
         else:
            is_band_3=''
         user_b_band=raw_input('Select Energy Band [1, 2'+is_band_3+']: ')
      avail_b_band=['1','2']                          # Define valid user inputs
      if nfiles==3:
         avail_b_band.append('3')                     # Add '3' as a valid input if 3
                                                      # bands present
      if user_b_band in avail_b_band:
         taxis='Phase' if folded else 'Time (s)'
         pl.figure()
         if user_b_band=='1':
            doplot(times,timese,y1[gmask],ye1[gmask],ovr=True,per2=folded)
                                                      # Plot flux/time using doplot
                                                      # from pan_lib
            b_band_name='Band 1'
         elif user_b_band=='2':
            doplot(times,timese,y2[gmask],ye2[gmask],ovr=True,per2=folded)
            b_band_name='Band 2'
         else:
            b_band_name='Band 3'
            doplot(times,timese,y3[gmask],ye3[gmask],ovr=True,per2=folded)
         pl.xlabel(taxis)                             # Format plot
         pl.ylabel(flux_axis)
         pl.ylim(ymin=0)
         pl.title(fldtxt+b_band_name+'Lightcurve'+qflav)
         plot_save(saveplots,show_block)
         print b_band_name+' lightcurve plotted!'

   #-----'bands' Option----------------------------------------------------------------

   elif plotopt=='bands':                             # Plot lightcurves of individual
                                                      # bands apart
      taxis='Phase' if folded else 'Time (s)'
      pl.figure()
      pl.subplot(nfiles,1,1) 
      doplot(times,timese,y1[gmask],ye1[gmask],ovr=True,per2=folded)
                                                      # Plot the lowest band
      pl.xlabel(taxis)
      pl.ylabel(flux_axis)
      pl.title(fldtxt+ch[1]+' Lightcurve'+qflav)
      if nfiles>1:
         pl.subplot(nfiles,1,2)
         doplot(times,timese,y2[gmask],ye2[gmask],ovr=True,per2=folded)
                                                      # Plot the second band
         pl.xlabel(taxis)
         pl.ylabel(flux_axis)
         pl.title(fldtxt+ch[2]+' Lightcurve'+flv2)
      if nfiles>2:
         pl.subplot(nfiles,1,3)
         doplot(times,timese,y3[gmask],ye3[gmask],ovr=True,per2=folded)
                                                      # Plot the third band
         pl.xlabel(taxis)
         pl.ylabel(flux_axis)
         pl.title(fldtxt+ch[3]+' Lightcurve'+flv3)
      plot_save(saveplots,show_block)
      print 'Banded lightcurves plotted!'

   #-----'xbands' Option---------------------------------------------------------------

   # 'Same axes bands'

   elif plotopt=='xbands':                            # Plot lightcurves of individual
                                                      # bands together
      donorm=raw_input('Normalise bands? : ')         # Fetch whether user wants to
                                                      # normalise
      donorm=donorm in ('y','yes') 
      taxis='Phase' if folded else 'Time (s)'
      pl.figure()
      leg=[ch[1]]                                     # Create a legend array to
                                                      # populate with channel names
      if donorm:
         n1=max(y1[gmask])
      else:
         n1=1.0
      doplot(times,timese,y1[gmask]/n1,ye1[gmask]/n1,ovr=True,ft='-b',per2=folded)
                                                      # Plot the lowest band
      if nfiles>1:
         if donorm:
            n2=max(y2[gmask])
         else:
            n2=1.0
         doplot(times,timese,y2[gmask]/n2,ye2[gmask]/n2,ovr=True,ft='-g',per2=folded)
                                                      # Plot the second band
         leg.append(ch[2])                            # Append name of second channel
                                                      # to key
      if nfiles>2:
         if donorm:
            n3=max(y3[gmask])
         else:
            n3=1.0
         doplot(times,timese,y3[gmask]/n3,ye3[gmask]/n3,ovr=True,ft='-r',per2=folded)
                                                      # Plot the third band
         leg.append(ch[3])                            # Append name of third channel to
                                                      # key
      if folded:
         pl.axvline(1,linestyle=':',color='0.7')
      pl.legend(leg)                                  # Create key on plot
      pl.xlabel(taxis)
      pl.ylabel(flux_axis)
      pl.title(fldtxt+'Lightcurve'+qflav)
      plot_save(saveplots,show_block)
      print 'Banded lightcurves plotted!'

   #-----'burst get'-------------------------------------------------------------------

   elif plotopt in ['burst get','burstget','burst_get','bursts get','burstsget',
                    'bursts_get']:
      if folded:
         print 'Cannot perform burst analsysis on folded data!'
         continue
      while True:
         try:
            iq_lo=float(raw_input('Low Threshold:  '))
            iq_hi=float(raw_input('High Threshold: '))
            assert iq_hi>iq_lo
            assert iq_hi<=100
            assert iq_lo>=0
            break
         except:
            print 'Invalid Entry!  Valid entry is of the form High>Low.'
      bursts={}
      bursts['endpoints']=pan.get_bursts(flux,q_lo=iq_lo,q_hi=iq_hi,just_peaks=False,
                                         alg=burst_alg)
      pl.figure()
      doplot(times,timese,flux,fluxe,ovr=True)        # Plot flux/time using doplot
                                                      # from pan_lib
      t_lo=np.percentile(flux,iq_lo)                  # Fetch thresholds used in
                                                      # get_bursts
      t_hi=np.percentile(flux,iq_hi)
      col_toggle=True
      for i in bursts['endpoints']:
         if col_toggle:
            col_toggle=False
            burst_colour='#c7c7c7'
         else:
            col_toggle=True
            burst_colour='#e7e7e7'
         pl.axvspan(times[i[0]],times[i[1]], facecolor=burst_colour, edgecolor='none')
      pl.plot([times[0],times[-1]],[t_lo,t_lo],'g')
      pl.plot([times[0],times[-1]],[t_hi,t_hi],'b')
      pl.legend(['Flux','Low Pass Threshold','High Pass Threshold'])
      pl.xlabel('Time (s)')
      pl.ylabel(flux_axis)
      pl.ylim(ymin=0)
      pl.title(fldtxt+'Lightcurve with Bursts Highlighted'+qflav)
      plot_save(saveplots,show_block)
      print ''
      print 'Bursts plotted!'
      print len(bursts['endpoints']),'bursts found!'
      print ''
      print 'Analysing Bursts...'
      bursts['peaks']=[]
      bursts['rises']=[]
      bursts['falls']=[]
      bursts['duras']=[]
      for endpoints in bursts['endpoints']:
         burst=flux[endpoints[0]:endpoints[1]]
         btime=times[endpoints[0]:endpoints[1]]
         peak,trough,pk_time,rise_time,fall_time=pan.eval_burst(btime,burst)
         bursts['peaks'].append(peak)
         #bursts['trghs'].append(troughs)
         bursts['rises'].append(rise_time)
         bursts['falls'].append(fall_time)
         bursts['duras'].append(btime[-1]-btime[0])
      print ''
      print 'Analysis Complete!'
      print 'Burst Products now available!'

   #-----'burst peaks'-----------------------------------------------------------------

   elif plotopt in ['burst peaks','burstpeaks','burst_peaks','bursts peaks',
                    'burstspeaks','bursts_peaks']:
      burstplot('peaks','Peak Heights','cts/s/PCU')

   #-----'burst risetimes'-------------------------------------------------------------

   elif plotopt in ['burst risetimes','burstrisetimes','burst_risetimes',
                    'bursts risetimes','burstsrisetimes','bursts_risetimes']:
      burstplot('rises','Rise Times','s')

   #-----'burst falltimes'-------------------------------------------------------------

   elif plotopt in ['burst falltimes','burstfalltimes','burst_falltimes',
                    'bursts falltimes','burstsfalltimes','bursts_falltimes']:
      burstplot('falls','Fall Times','s')

   #-----'burst lengths'---------------------------------------------------------------

   elif plotopt in ['burst lengths','burstlengths','burst_lengths','bursts lengths',
                    'burstslengths','bursts_lengths']:
      burstplot('duras','Durations','s')

   #-----'bursts help'-----------------------------------------------------------------

   elif plotopt in ['burst help','bursthelp','burst_help','bursts help','burstshelp',
                    'bursts_help']:
      print 'Help coming soon.'

   #-----'burst alg'-------------------------------------------------------------------

   elif plotopt in ['burst alg','burstalg','burst_alg','bursts alg','burstsalg',
                    'bursts_alg']:
      print 'Available Burst-Finding Algorithms:'
      print ' * CUBIC SPLINE'
      print ' * LOAD'
      print ''
      inp_burst_alg=raw_input('Select Burst Algorithm: ').lower()
      if inp_burst_alg in ('cubic spline','load'):
         burst_alg=inp_burst_alg
         print 'Burst-Finding Algorithm set to "'+burst_alg+'"!'
      else:
         print 'Invalid Burst-Finding Algorithm!'

   #-----'lombscargle' Option----------------------------------------------------------

   elif plotopt=='lombscargle':
      if folded:                                      # If data is folded, abort
         print 'Cannot perform Lomb-Scargle on folded data!'
         continue
      if nfiles==1:
         user_scargl_bands='1'                        # Select energy bands to
                                                      # LombScargle
      else:
         if nfiles==3:
            is_band_3=', 3'
         else:
            is_band_3=''
         user_scargl_bands=raw_input('Select Energy Band [1, 2'+is_band_3+', All]: '
                                     ).lower()
      ls_st=max(4.0/(times[-1]-times[0]),0.005)
      ls_end=0.5/binning
      lsx=np.arange(ls_st,ls_end,(ls_end-ls_st)/2500.0)
      avail_scargl_bands=['1','2','all']              # Define valid user inputs
      if nfiles==3:
         avail_scargl_bands.append('3')               # Add '3' as a valid input if 3
                                                      # bands present
      if user_scargl_bands in avail_scargl_bands:
         if user_scargl_bands=='1':
            lsy=pan.lomb_scargle(times,y1[gmask],ye1[gmask],lsx)
                                                      # Perform LombScargle of band 1
                                                      # using lombscargle function
                                                      # defined in header
            s_band_name='band 1'
         elif user_scargl_bands=='2':
            lsy=pan.lomb_scargle(times,y2[gmask],ye2[gmask],lsx)
                                                      # Perform LombScargle of band 2
                                                      # using lombscargle function
                                                      # defined in header
            s_band_name='band 2'
         elif user_scargl_bands=='3':
            lsy=pan.lomb_scargle(times,y3[gmask],ye3[gmask],lsx)
                                                      # Perform LombScargle of band 3
                                                      # using lombscargle function
                                                      # defined in header
            s_band_name='band 3'
         else:
            lsy=pan.lomb_scargle(times,flux,fluxe,lsx)# Perform LombScargle of all
                                                      # bands using lombscargle
                                                      # function defined in header
            s_band_name='all bands'
            if user_scargl_bands!='all':
               print 'Invalid band!  Using all.'
         pl.figure()
         pl.plot(lsx,lsy,'k')                         # Plot lombscargle
         pl.xlabel('Frequency (Hz)')
         pl.ylabel('Power')
         pl.xlim(0,max(lsx))
         pl.ylim(1,100000)
         pl.yscale('log')
         pl.title('Lomb-Scargle Periodogram of '+s_band_name+qflav)
         plot_save(saveplots,show_block)
         print ''
         print 'Lomb-Scargle Diagram of '+s_band_name+' plotted!'
      else:
         print 'Invalid energy band!'

   #-----'errors' Option---------------------------------------------------------------

   elif plotopt in ['errors','error']:                # Toggle Errors
      if es:
         es=False
         print 'Errors suppressed!'
      else:
         es=True
         print 'Errors displayed!'

   #-----'ckey' Option-----------------------------------------------------------------

   # 'Colour Key'

   elif plotopt=='ckey':                              # Toggle Colour-key
      if cs:
         cs=False
         print 'Colour key suppressed!'
      else:
         cs=True
         print 'Colour key displayed!'

   #-----'lines' Option----------------------------------------------------------------

   elif plotopt=='lines':                             # Toggle Delineation
      if ls:
         ls=False
         print 'Plot Lines suppressed!'
      else:
         ls=True
         print 'Plot Lines displayed!'

   #-----'info' Option-----------------------------------------------------------------

   elif plotopt=='info':
      dst=times[0]
      det=times[-1]
      print 'PlotDemon.py version',version
      print ''
      print nfiles,'files loaded:'
      print ''
      filn1,loca1=pan.xtrfilloc(file1)
      print 'File 1:'
      print ' Filename       = ',filn1
      print ' Location       = ',loca1
      print ' Mission        = ',mis1
      print ' Object         = ',obsd1[0]
      print ' Obs_ID         = ',obsd1[1]
      if mis1 in ['SUZAKU']:
         print ' Energy         = ',ch[1],'eV'
      else:
         print ' Channel        = ',ch[1]
      print ' Resolution     = ',str(bsz1)+'s'
      print ' BG Subtracted  = ',bsub1
      print ' No. of PCUs    = ',pcus1
      print ' Flavour        = ',flv1
      print ' FITSGenie Ver. = ',v1
      if nfiles>1:
         filn2,loca2=pan.xtrfilloc(file2)
         print ''
         print 'File 2:'
         print ' Filename       = ',filn2
         print ' Location       = ',loca2
         print ' Mission        = ',mis2
         print ' Object         = ',obsd2[0]
         print ' Obs_ID         = ',obsd2[1]
         if mis2 in ['SUZAKU']:
            print ' Energy         = ',ch[2],'eV'
         else:
            print ' Channel        = ',ch[2]
         print ' Resolution     = ',str(bsz2)+'s'
         print ' BG Subtracted  = ',bsub2
         print ' No. of PCUs    = ',pcus2
         print ' Flavour        = ',flv2
         print ' FITSGenie Ver. = ',v2
      if nfiles==3:
         filn3,loca3=pan.xtrfilloc(file3)
         print ''
         print 'File 3:'
         print ' Filename       = ',filn3
         print ' Location       = ',loca3
         print ' Mission        = ',mis3
         print ' Object         = ',obsd3[0]
         print ' Obs_ID         = ',obsd3[1]
         if mis3 in ['SUZAKU']:
            print ' Energy         = ',ch[3],'eV'
         else:
            print ' Channel        = ',ch[3]
         print ' BG Subtracted  = ',bsub3
         print ' Resolution     = ',str(bsz3)+'s'
         print ' No. of PCUs    = ',pcus3
         print ' Flavour        = ',flv3
         print ' FITSGenie Ver. = ',v3
      print ''
      print 'Other Info:'
      print ' Global Flavour = ',flavour
      print ' Obs. Starttime = ',str(tst1)+'s (0.0s)'
                                                      # The start of the observation
      print ' Obs. Endtime   = ',str(oet+tst1)+'s ('+str(oet)+'s)'
                                                      # The end of the observation
      print ' Data Starttime = ',str(dst+tst1)+'s ('+str(dst)+'s)'
                                                      # The start of the data set (i.e.
                                                      # after GTI considerations and
                                                      # clipping)
      print ' Data Endtime   = ',str(det+tst1)+'s ('+str(det)+'s)'
                                                      # The end of the data set
      print ' Bin-size       = ',str(binning)+'s'
      print ' Background     = ',str(bg)+'cts/s/PCU'
      print ' Folded         = ',folded
      if folded:
         print ' Fold Period    = ',period
      print ' Errorbars      = ',es
      print ' Delineated     = ',ls
      print ' Colour-coded   = ',cs

   #-----'reflav' Option---------------------------------------------------------------

   elif plotopt=='reflav':
      print 'Please give a new flavour.'
      try:
         nflavour=raw_input('Flavour: ')
         assert nflavour!=''
         flavour=nflavour
         if flavour=='':
            qflav=''
         else:
            qflav=' "'+flavour+'"'
         print 'Flavour set to "'+flavour+'"'
      except:
         print 'Invalid flavour!  Flavour remains "'+flavour+'"'

   #-----'help' Option-----------------------------------------------------------------

   elif plotopt in ['help','?']:                      # Display instructions
      print 'Instructions:'
      print ''
      give_inst()                                     # Re-call the instructions list,
                                                      # defined as the get_inst()
                                                      # function in initialisation

   #-----'quit' Option-----------------------------------------------------------------
   
   elif plotopt not in ['quit','exit']:               # Invalid command if none of the
                                                      # if statements triggered and no
                                                      # 'q' given
      print 'Invalid command!'
   if plotopt not in ['quit','exit']:
      print ''
      print ' --------------------'

#-----Exiting Interactive Mode---------------------------------------------------------

print ''
print 'Goodbye!'                                           

#-----Footer---------------------------------------------------------------------------

pan.signoff()

\end{minted}

\section{Spec Angel}

\begin{minted}[fontsize=\scriptsize]{python}

#! /usr/bin/env python

# |----------------------------------------------------------------------|
# |------------------------------SPEC ANGEL------------------------------|
# |----------------------------------------------------------------------|

# Call as ./specangel.py FILE1 [LBINNING]

# Takes 1 RXTE FITS Event file and produces an interactive spectrogram
#
# Arguments:
#
#  FILE1
#   The absolute path to the file to be used.
#
#  [LBINNING]
#   Optional- the logarithmic binning factor 'x'; frequency data will be binned into 
#   bins which have their lefthand edges defined by the formula 10**(ix) for integer i.
#

#-----User-set Parameters--------------------------------------------------------------

logfreqres_default=0.005                              # The best resolution, in log10
                                                      # space, in which the data will
                                                      # be analysed
version=4.2                                           # The version of SpecAngel

#-----Welcoming Header-----------------------------------------------------------------

print ''
print '-------Running Spec Angel: J.M.Court, 2015------'
print ''

#-----Importing Modules----------------------------------------------------------------

try:
   import sys,os
   import pylab as pl
   import pan_lib as pan
   from astropy.io import fits
   from numpy import array, arange, mean, meshgrid, transpose, zeros
   from numpy import exp, histogram, linspace, log10, nonzero, sqrt
   from numpy import append as npappend               # Importing numpy append as
                                                      # npappend to avoid confusion
                                                      # with in-built append function
   from numpy import min as npmin
   from numpy import max as npmax
   from numpy import sum as npsum
   from scipy import delete
   from scipy.fftpack import fft
   from scipy.stats import spearmanr
   from scipy.optimize import brentq
except ImportError:
   print 'Modules missing!  Aborting!'
   print ''
   print '------------------------------------------------'
   print ''
   exit()

#-----Checking Validity of Arguments, Fetching File------------------------------------

args=sys.argv
pan.argcheck(args,2)                                  # Must give at least 2 args
                                                      # (Filename and the function
                                                      # call)
filename=args[1]                                      # Fetch file name from arguments
pan.filenamecheck(filename,'speca')

#-----Extracting data from file--------------------------------------------------------

print 'Opening '+str(filename)                        # Use SpecaLd from pan_lib to
                                                      # load data from file
loadmatrix,good,rates,pk_rates,tr_rates,ph_cts,bg,binsize,four_res,bg_est,load_flavour,
           cs,mis,obsid,wtype,slide,binfac,v=pan.specald(filename)
flavour=load_flavour
if flavour=='':
   q_flav=''
else:
   q_flav=' "'+flavour+'"'

#-----Initially normalising data-------------------------------------------------------

print ''
print 'Normalizing and binning...'
if len(args)>2:                                       # Check for logfreqres input,
                                                      # else request one
   try:
      logfreqres=float(args[2])
      assert logfreqres>0
   except:
      logfreqres=logfreqres_default
else:
   try:
      logfreqres=float(raw_input('Logarithmic binning factor: '))
   except:
      logfreqres=logfreqres_default
if len(args)>3:                                        # Check for normalization input,
                                                       # else request one
   try:
      norm=str(args[3])
   except:
      norm='nupnu'
else:
   try:
      norm=str(raw_input('Input normalisation [leahy, rms, nupnu]: '))
   except:
      norm='nupnu'
numstep=len(loadmatrix)
nleahy=float(sum(good))
lspec=npsum(loadmatrix,axis=0)/nleahy                 # Create the average Leahy
                                                      # spectrum
const=pan.lhconst(lspec)                              # Calculate the normalisation of
                                                      # noise
def constmi(k):                                       # Define nuP(nu) noise average as
                                                      # a function of  Leahy constant.
   nlspec=pan.lh2rms(lspec,mean(rates),bg,k)
   nlrang=arange(len(nlspec))
   nlrang=nlrang[int(4*len(nlspec)/5.0):]
   nlspec=nlspec[int(4*len(nlspec)/5.0):]
   return (mean(nlrang*nlspec))
const=(brentq(constmi,const-0.1,const+0.1))           # Minimise the above function to
                                                      # improve the constant
datres=int(four_res/binsize)
tfl=linspace(0.0, (1.0/2.0)*datres/float(four_res), (datres/2)+1)
                                                      # Create linearly spaced freq.
                                                      # domain up to the Nyquist freq.
                                                      # 1/2 (N/T)
tfl=tfl[:-1]
nulldat=zeros((datres/2))                             # Create null data with the same
                                                      # number of points as tfl
tf,null,null=pan.lbinify(tfl[1:],nulldat[1:],nulldat[1:],logfreqres)
                                                      # Fetch new array of bins to be
                                                      # output after lbinning
del null
print ''
def lbin(logfreqres,pow_norm='nupnu',prt=False):      # Defining a log-binning function
                                                      # that just depends on bin
                                                      # resolution and normalisation
   if True:                                           # Instructions with a list of
                                                      # possible normalisations
      if pow_norm not in ['rms','nupnu','leahy']:
         print 'Unknown normalisation selected!'
         print ''
         print 'Available normalisations are:'
         print '* "leahy" for Leahy-normalised power'
         print '* "rms" for RMS-normalised power'
         print '* "nupnu" for RMS-normalised power multiplied by frequency'
         print ''
         print 'Using "nupnu" normalisation:'
         pow_norm='nupnu'
      else:
         print 'Using "'+pow_norm+'" normalisation:'
   errgr=[]                                           # Set up matrix of errors
   fourgr=[]
   for i in range(numstep):
      tsfdata=loadmatrix[i]                           # Load a row of data
      errs=pan.lh2rms(tsfdata,rates[i],bg,0)          # Errors of a Leahy spectrum =
                                                      # the Leahy spectrum
      if pow_norm in ['rms','nupnu']:
         if pow_norm=='nupnu':
            sconst=const                              # For nuP(nu) normalisation, the
                                                      # Leahy constant will need
                                                      # subtracting
         else:
            sconst=0                                  # In RMS norm, it can stay
         tsfdata=pan.lh2rms(tsfdata,rates[i],bg,sconst)
                                                      # Convert to RMS-normalised data
                                                      # using the LH2RMS function from
                                                      # pan_lib
         if pow_norm=='nupnu':
            tsfdata=tsfdata*tfl                       # Multiply by frequency if nupnu
                                                      # normalisation requested
            errs=errs*tfl
      tf,fours,errs=pan.lbinify(tfl[1:],tsfdata[1:],errs[1:],logfreqres)
                                                      # Logarithmically bin the data
                                                      # using lbinify from pan_lib
      fourgr.append(fours)                            # Populate the data matrix
      errgr.append(abs(errs))                         # Populate the error matrix
      prog=i+1
      if prt and ((prog % 5)==0 or prog==numstep):
         print str(prog)+'/'+str(numstep)+' series re-binned...'
                                                      # Display progress every 5 series
   fourgr=transpose(fourgr)                           # Flip the matrices (makes them
                                                      # easier to plot the correct way
                                                      # round in spectrogram)
   errgr=transpose(errgr)
   return fourgr,errgr,pow_norm
fourgr,errgr,norm=lbin(logfreqres,prt=True,pow_norm=norm)  
print ''
print 'Preparing spectrogram...'
deftitle='Spectrogram'+q_flav                         # Define default title for
                                                      # spectrogram
if norm=='leahy':                                     # Define default key label for
                                                      # spectrogram
   defzlabl='Leahy-Normalised Power (Hz^-1)'
elif norm=='rms':
   defzlabl='RMS Normalised Power (Hz^-1)'
else:
   defzlabl='Frequency x RMS Normalised Power'
fourgrm=fourgr                                        # Storing a copy of the matrix in
                                                      # memory so it can be reset
errgrm=errgr
td=arange(0,(numstep+1)*slide,slide)                  # Creating the time domain as an
                                                      # array
tdg, tfg = meshgrid(td, tf)                           # Making a grid from the time and
                                                      # frequency domains
specopt=''                                            # Force spectrogram manipulation
                                                      # mode to trigger
speclog=False                                         # Indicate that the spectrogram
                                                      # is not initially logarithmic
stitle=deftitle                                       # Give an initial title
rtlabl=defzlabl                                       # Give an initial key label
tmdbin=four_res                                       # Initial time binning
frqbin=(tf[-1]-tf[0])/(len(tf)-1)                     # Initial freq binning
tdgd=tdg                                              # Saving default grid [Time
                                                      # Domain Grid- Default]
tfgd=tfg
tdlm=td                                               # Saving 1D arrays to re-form
                                                      # grids [time-domain linear,
                                                      # modifiable]
tflm=tf
ogood=good                                            # Save copy of the 'good' list
fudge=npmin(abs(fourgr[nonzero(fourgr)]))             # Obtain smallest nonzero value
                                                      # in array to add on when using
                                                      # logarithm to prevent log(0)
print 'Done!'
print ''

#-----Setting up Spectrogram Environment-----------------------------------------------

es=True                                               # Start with errors on by default
saveplots=False
show_block=False
def spectrogram(td,tfc,fourgr,zlabel=defzlabl,title=deftitle):
                                                      # Defining the creation of the
                                                      # spectrogram plot 's' as a
                                                      # function for clarity later
   pl.close('Spectrogram')                            # Close any previous spectrograms
                                                      # that may be open
   fg=pl.figure('Spectrogram')
   ax=fg.add_subplot(1,1,1)
   if speclog:
      pl.pcolor(td,tfc,(fourgr))
   else:
      pl.pcolor(td,tfc,fourgr,vmin=sgfloor,vmax=sgceil) # Plot spectrogram
   cbar=pl.colorbar()                                 # Create colourbar key
   cbar.set_label(zlabel)
   pl.xlabel('Time(s)')
   pl.ylabel('Frequency(Hz)')
   pl.title(title)
   ax.set_yscale('log')                               # Make the freq-axis logarithmic
                                                      # too
   pl.ylim(tfc[0,0],tfc[-1,-1])                       # Resize axes to fit the
                                                      # spectrogram
   pl.xlim(td[0,0],td[-1,-1])
   plot_save(saveplots,show_block)
sxlab='Frequency (Hz)'
sylab=defzlabl
szlab=defzlabl                                        # Give an initial key label,
                                                      # storing second copy
def give_inst():                                      # Define printing this list of
                                                      # instructions as a function
   print 'COMMANDS: Enter a command to manipulate data.'
   print ''
   print 'DATA:'
   print '* "rebin" to reset the data and load it with a different normalisation and '+
         ' binning.'
   print '* "clip" to clip the range of data.'
   print '* "reset" to reset data.'
   print ''
   print 'SPECTROGRAM:'
   print '* "sg plot" to plot the spectrogram currently being worked on.'
   print '* "sg floor" to set a minimum value for the spectrogram'+"'"+'s z-axis colo'+
         'ur key.'
   print '* "sg ceil" to set a maximum value for the spectrogram'+"'"+'s z-axis colou'+
         'r key.'
   print '* "sg auto" to automatically set colour floor and ceiling.'
   print '* "sg log" to toggle logarithmic spectrogram plotting.'
   print ''
   print 'POWER SPECTRA:'
   print '* "aspec" to plot the average spectrum and return the frequency of its high'+
         'est peak.'
   print '* "gspec" to get an individual spectrum at any time and plot it.'
   print '* "peaks" to plot a graph of the frequency of the strongest oscillation aga'+
         'inst time.'
   print '* "rates" to get a simple lightcurve of the data.'
   print '* "fqflux" to plot "peaks" against "rates".'
   print ''
   print 'TOGGLE OPTIONS:'
   print '* "errors" to toggle errorbars on power spectra plots.'
   print '* "save" to save to disk any plots which would otherwise be shown.'
   print ''
   print 'OTHER COMMANDS:'
   print '* "info" to display a list of facts and figures about the current SpecAngel'+
         ' session.'
   print '* "reflav" to rewrite the flavour text used for graph titles.'
   print '* "export" to create an ASCII file of the average power density spectrum.'
   print '* "help" or "?" to display this list of instructions again.'
   print '* "quit" to Quit'

#give_inst()                                          # Print the list of instructions
print ''
print ' --------------------'
sgfloor=max(npmin(fourgrm),0)
sgceil=npmax(fourgrm)
def plot_save(saveplots,show_block):                  # Add a function to redirect all
                                                      # show calls to savefigs if
                                                      # toggled
   if saveplots:
      pl.savefig(raw_input('Save plot as: '))
      print 'Plot saved!'
   else:
      pl.show(block=show_block)

#-----Entering Interactive Mode--------------------------------------------------------

while specopt not in ['quit','exit']:                 # If the previous command give
                                                      # was not quit, continue
   print ''
   specopt=raw_input('Give command [? for help]: ').lower()   # Fetch command from user
   print ''

   #-----Hidden 'stick' option---------------------------------------------------------

   if specopt=='stick':                               # For use when scripting with
                                                      # Specangel.  If turned on, this
                                                      # causes all plots to block when
                                                      # shown.
      show_block=not show_block
      if show_block:
         print 'Sticky Plots on!'
      else:
         print 'Sticky Plots off!'

   #-----'save' option-----------------------------------------------------------------

   elif specopt=='save':                              # Causes a plot to be saved when
                                                      # it would otherwise have been
                                                      # shown
      saveplots=not saveplots
      if saveplots:
         print 'Plot saving on!'
      else:
         print 'Plot saving off!'

   #-----'rebin' Option----------------------------------------------------------------

   elif specopt=='rebin':                             # Rebinning data    
      print 'Data currently binned in '+str(tmdbin)+'s and 10^'+str(logfreqres)+'Hz b'+
            'ins.'
      speclog=False                                  # Indicate that the spectrogram is
                                                     # not logarithmic
      stitle=deftitle                                # Restore initial title
      tmdbin=four_res                                # Initial time binning
      frqbin=(tf[-1]-tf[1])/(len(tf)-2)              # Initial freq binning
      fourgrm=fourgr                                 # Reload original, unmodified data
      errgrm=errgr
      tdgd=tdg                                       # Reloading default grid
      tfgd=tfg
      tdlm=td                                        # Loading 1D arrays to re-form
                                                     # grids [time-domain linear,
                                                     # modifiable]
      tflm=tf
      good=ogood
      norm=raw_input('Select normalisation [leahy, rms, nupnu]: ')
      try:
         tbinmult=int(raw_input('Input time-domain binning factor: '))
         if tbinmult<1: tbinmult=1                    # Prevent bin-sizes smaller than
                                                      # current bin-size
         if tbinmult>len(fourgrm)/2.0:tbinmult=1      # Forces there to be at least 2
                                                      # bins
      except:
         tbinmult=1                                   # Cancel binning if a non-number
                                                      # is entered
         print 'Invalid time binning!'
      tmdbin=four_res*tbinmult                        # Recover current binning
      try:
         newfbin=float(raw_input('Input new freq-domain exponent: '))
         if newfbin>0: logfreqres=newfbin             # Prevent bin-sizes smaller than
                                                      # zero             
      except:
         newfbin=0                                    # Cancel binning if a non-number
                                                      # is entered
         print 'Invalid frequency binning!'
      print ''
      print 'Re-binning...'
      tflm,null,null=pan.lbinify(tfl[1:],nulldat,nulldat,logfreqres)
                                                      # Fetch new array of bins to be
                                                      # output after lbinning
      del null
      fourgrm,errgrm,norm=lbin(logfreqres,prt=True,pow_norm=norm)
                                                      # Re log-bin data
      if tbinmult!=1:                                 # Cancel binning if new bin is
                                                      # not greater than old bin
         fourgrm,errgrm,tdlm,good=pan.mxrebin(fourgrm,errgrm,tdlm,good,tbinmult)
      tdgd,tfgd=meshgrid(tdlm,tflm)                   # Recreate grid from rescaled
                                                      # axes
      print ''
      print 'Data rebinned by '+str(tmdbin)+'s, [10^'+str(logfreqres)+'n]Hz.'
      print str(int(sum(good)))+'/'+str(len(good))+' power spectra are good'
      if norm=='leahy':
         sylab='Leahy-Normalised Power (Hz^-1)'
      elif norm=='rms':
         sylab='RMS Normalised Power (Hz^-1)'
      else:
         sylab='Frequency x RMS Normalised Power'
      defzlabl=sylab                                  # Restore root z label for
                                                      # spectrogram              
      rtlabl=defzlabl                                 # Restore initial key label
      szlab=defzlabl                                  # Restore initial key label,
                                                      # storing second copy
      sgfloor=max(npmin(fourgrm),0)                   # Reset spectrogram colour floor
                                                      # & ceil
      sgceil=npmax(fourgrm)

   #-----'sg plot' Option--------------------------------------------------------------

   # 'Spectrogram'

   elif specopt=='sg plot':                           # Plotting data
      proce=True                                      # Assume plot will be made
      npl=len(fourgrm[0,:])*len(fourgrm[:,0])         # Check how large this plot'll be
      if npl>1000000:                                 # Check for very large plots, ask
                                                      # user whether to proceed
          try:
             print "LARGE DATA WARNING! Plot will contain "+"{:,}".format(npl)+" elem"+
                   "ents."
             proc=raw_input("Proceed? [y/n]: ")
             if proc!='y': proce=False                # Cancel plot is user doesn't
                                                      # explicity say 'y'
          except:
             proce=False
      if proce==True:                                 # If all is ok...
         print 'Plotting...'
         spectrogram(tdgd,tfgd,fourgrm,szlab,stitle)  # Plot spectrogram 

   #-----'sg floor' Option-------------------------------------------------------------

   # 'Colour Floor'

   elif specopt=='sg floor':                          # Setting floor of spectrogram
                                                      # colour scale:
      sgfloor=raw_input('Input spectrogram colour floor: ')
                                                      # Ask user to input floor
      try:
         sgfloor=float(sgfloor)                       # Check floor is a number
         print 'Colour floor set!'
      except:
         sgfloor=max(npmin(fourgrm),0)
         print 'Invalid floor!'

   #-----'sg ceil' Option--------------------------------------------------------------

   # 'Colour Ceiling'

   elif specopt=='sg ceil':                           # Setting ceiling of spectrogram
                                                      # colour scale:
      sgceil=raw_input('Input spectrogram colour ceiling: ')
                                                      # Ask user to input floor
      try:
         sgceil=float(sgceil)                         # Check ceil is a number
         print 'Colour ceiling set!'
      except:
         sgceil=npmax(fourgrm)
         print 'Invalid ceiling!'

   #-----'sg auto' Option--------------------------------------------------------------

   # 'Auto Recolour'

   elif specopt=='sg auto':
      sgfloor=max(npmin(fourgrm),0)                   # Reset spectrogram colour floor
                                                      # & ceil
      sgceil=npmax(fourgrm)
      print 'Colour floor and ceiling automatically set!'

   #-----'sg log' Option---------------------------------------------------------------

   # 'Logarithm'

   elif specopt=='sg log':                            # Taking or undoing log of data
      if speclog:                                     # If the spectrogram was already
                                                      # logged, undo this with exp
         print 'Exponentiating spectrogram...'
         stitle=deftitle                              # Reset title to default
         rtlabl=rtlabl[4:]                            # Remove 'log ' from the start of
                                                      # both saved z-labels
         szlab=szlab[4:]
         fourgrm=10**(fourgrm)-fudge                  # Exponentiate every element of
                                                      # every power spectrum
         speclog=False                                # Indicate that spectrogram is no
                                                      # longer logged
         print 'Done!'
      else:
         print 'Taking logarithm of spectrogram...'
         stitle='Log '+deftitle                       # Add 'log ' to start of titles
                                                      # and labels
         rtlabl='Log '+rtlabl
         szlab='Log '+szlab
         fourgrm=log10(abs(fourgrm)+fudge)            # Take the log of every element
                                                      # of every power spectrum
         speclog=True                                 # Indicate that spectrum is
                                                      # logged
         print 'Done!'

   #-----'sg' Catch-All Help Message---------------------------------------------------

   elif specopt[:2]=='sg':
      print 'SPECTROGRAM COMMANDS:'
      print '* "sg plot" to plot the spectrogram currently being worked on.'
      print '* "sg floor" to set a minimum value for the spectrogram'+"'"+'s z-axis c'+
            'olour key.'
      print '* "sg ceil" to set a maximum value for the spectrogram'+"'"+'s z-axis co'+
            'lour key.'
      print '* "sg auto" to automatically set colour floor and ceiling.'
      print '* "sg log" to toggle logarithmic spectrogram plotting.'

   #-----'clip' Option-----------------------------------------------------------------

   elif specopt=='clip':                              # Clipping data
      print 'Clipping data'
      print ''
      print 'Time range is '+str(tdlm[0])+'s - '+str(tdlm[-2]+four_res)+'s'
      print 'Freq range is '+str(tflm[0])+'Hz- '+str(tflm[-1]+four_res)+'Hz'
      print ''
      print 'Please choose new range of data:'
      mint,maxt,null=pan.srinr(tdlm,tmdbin,'time')    # Fetch new time domain endpoints
                                                      # using srinr function from
                                                      # pan_lib
      minf,maxf,null=pan.srinr(tflm,frqbin,'freq')    # Fetch new freq domain endpoints
                                                      # using srinr function from
                                                      # pan_lib
      print 'Clipping...'
      tdlm=tdlm[mint:maxt]                            # Clip the time-domain array
      tflm=tflm[minf:maxf]                            # Clip the freq-domain array
      fourgrm=fourgrm[minf:maxf,mint:maxt]            # Clip the spectrogram data
      errgrm=errgrm[minf:maxf,mint:maxt]
      tdgd,tfgd=meshgrid(tdlm,tflm)                   # Recreate grid from rescaled
                                                      # axes
      print 'Data clipped!'

   #-----'reset' Option----------------------------------------------------------------

   elif specopt=='reset':                             # Resetting data
      print 'Resetting spectrogram...'               
      speclog=False                                   # Indicate that the spectrogram
                                                      # is not logarithmic
      stitle=deftitle                                 # Restore initial title
      rtlabl=defzlabl                                 # Restore initial key label
      szlab=defzlabl                                  # Restore initial key label,
                                                      # storing second copy
      tmdbin=four_res                                 # Initial time binning
      frqbin=(tf[-1]-tf[1])/(len(tf)-2)               # Initial freq binning
      fourgrm=fourgr                                  # Reload original, unmodified
                                                      # data
      errgrm=errgr
      tdgd=tdg                                        # Reloading default grid
      tfgd=tfg
      tdlm=td                                         # Loading 1D arrays to re-form
                                                      # grids [time-domain linear,
                                                      # modifiable]
      tflm=tf
      good=ogood                                      # Reload original 'good' list
      print 'Spectrogram reset!'

   #-----'aspec' Option----------------------------------------------------------------

   # 'Average Spec'

   elif specopt=='aspec':                             # Find the time-averaged spectrum
      if slide!=four_res:
         print "Warning!  Data taken with sliding window: data points not independent!"
         print ''
      print "Fetching time-averaged power spectrum..."
      spec=npsum(fourgrm, axis=1)/sum(good)           # Sum all spectra in the matrix
                                                      # and divide by the number of
                                                      # good columns
      err=sqrt(npsum( array(errgrm)**2, axis=1))/sum(good)
      ttl='Average power density spectrum'+q_flav
      pan.slplot(tflm,spec,err,sxlab,sylab,ttl,'spc',errors=es,typ='log')
                                                      # SLPlot from the pan_lib plots
                                                      # data on standard and log-log
                                                      # axes
      plot_save(saveplots,show_block)      
      scerr=spec-(err**0.5)
      print 'Maximum power found at '+str(tflm[scerr.argmax()])+'Hz!'
                                                      # Suggest a peak location
      print '  (Period of '+str(1.0/tflm[scerr.argmax()])+'s)'

   #-----'gspec' Option----------------------------------------------------------------
   
   # 'Get Spec'
   
   elif specopt=='gspec':                             # Find the power spectrum at a
                                                      # specific point in time
      if slide!=four_res:
         print "Warning!  Data taken with sliding window: data points not independent!"
         print ''
      print 'Getting a spectrum at a time (since start of observation) of your choice.'
      print ''
      while True:
         try:
            specid=float(raw_input('Enter time: '))   # Fetch raw time suggestion from
                                                      # user
            break
         except:
            print 'Invalid time!'
      specid=int((specid-tdlm[0])/tmdbin)             # Work out which time bin this
                                                      # would correlate to
      if 0<=specid<len(fourgrm[0,:])-1:               # Check that this time bin
                                                      # actually exists in the matrix
         if good[specid]:
            print "Fetching power spectrum at "+str(specid*tmdbin)+"s: "

            gsp=fourgrm[:,specid]                     # Extract the lightcurve from
                                                      # this bin
            ger=errgrm[:,specid]
            ttl='Power density spectrum "'+flavour+'" at +'+str(specid*tmdbin)+'s'
            pan.slplot(tflm,gsp,ger,sxlab,sylab,ttl,'spc',typ='log',errors=es)
            plot_save(saveplots,show_block)
            scerr=spec-(err**0.5)
            print 'Maximum power found at '+str(tflm[scerr.argmax()])+'Hz!'
                                                      # Suggest a peak location
         else:
            print 'Time not in GTI!'
      else:
         print 'Time not in range!'

   #-----'peaks' Option----------------------------------------------------------------

   elif specopt=='peaks':
      peaks=[]
      for i in range(len(fourgrm[1])):
         row=fourgrm[:,i]
         peaks.append(tflm[list(row).index(max(row))])
      pl.close('pk')
      pl.figure('pk')
      pl.semilogy(array(tdlm[:-1])[good],array(peaks)[good],'-ok')
      pl.xlabel('Time (s)')
      pl.ylabel('Frequency (Hz)')
      pl.title('Peak Frequency/Time Plot'+q_flav)
      plot_save(saveplots,show_block)

   #-----'rates' Option----------------------------------------------------------------

   elif specopt=='rates':
      print 'Average rate of',str(mean(rates[ogood]))+'c/s.'
      print str(ph_cts),'total counts.'
      print ''
      datasel=raw_input('Select Rates to plot [ave/peak/trough]: ')
      brates={}
      titles={}
      titles['ave']='Flux (photons/s/PCU)'
      titles['peak']='Peak '+str(binfac*binsize)+'s Flux (photons/s/PCU)'
      titles['trough']='Trough '+str(binfac*binsize)+'s Flux (photons/s/PCU)'
      if datasel not in ('ave','peak','trough'):
         print 'Invalid selection!  Using Average flux.'
         datasel='ave'
      brates['ave']=rates
      brates['peak']=pk_rates
      brates['trough']=tr_rates
      pl.close('lc')
      pl.figure('lc')
      pl.plot(td[:-1][ogood],brates[datasel][ogood],'-ok')
      pl.xlabel('Time (s)')
      pl.ylabel('Flux (photons/s/PCU)')
      pl.title('Lightcurve'+q_flav)
      plot_save(saveplots,show_block)

   #-----'fqflux' Option---------------------------------------------------------------

   elif specopt=='fqflux':
      peaks=[]
      for i in range(len(fourgrm[1])):
         row=fourgrm[:,i]
         peaks.append(tflm[list(row).index(max(row))])
      brates={}
      titles={}
      titles['ave']='Flux (photons/s/PCU)'
      titles['peak']='Peak '+str(binfac*binsize)+'s Flux (photons/s/PCU)'
      titles['trough']='Trough '+str(binfac*binsize)+'s Flux (photons/s/PCU)'
      brates['ave']=pan.vcrebin(rates,len(rates)/len(peaks))
      brates['peak']=pan.vcrebin(pk_rates,len(pk_rates)/len(peaks))
      brates['trough']=pan.vcrebin(tr_rates,len(tr_rates)/len(peaks))
      datasel=raw_input('Select Rates to plot against [ave/peak/trough]: ')
      if datasel not in ('ave','peak','trough'):
         print 'Invalid selection!  Using Average flux.'
         datasel='ave'
      print 'Spearman Rank Coefficient: ',spearmanr(array(peaks)[good],
                                                    brates[datasel][good])[1]
      pl.close('pr')
      pl.figure('pr')
      pl.semilogx(array(peaks)[good],brates[datasel][good],'ok')
      pl.ylabel(titles[datasel])
      pl.xlabel('Frequency (Hz)')
      pl.title('Flux/Peak Frequency Plot'+q_flav)
      plot_save(saveplots,show_block)

   #-----'errors' Option---------------------------------------------------------------

   elif specopt in ['error', 'errors']:               # Toggle Errors
      if es:
         es=False
         print 'Errors suppressed!'
      else:
         es=True
         print 'Errors displayed!'

   #-----'info' Option-----------------------------------------------------------------

   elif specopt=='info':
      print 'SpecAngel.py version',version
      print ''
      print '1 file loaded:'
      print ''
      local_name,file_path=pan.xtrfilloc(filename)
      print 'File 1:'
      print ' Filename       = ',local_name
      print ' Location       = ',file_path
      print ' Mission        = ',mis
      print ' Object         = ',obsid[0]
      print ' Obs_ID         = ',obsid[1]
      if mis in ['SUZAKU']:
         print ' Energy         = ',cs,'eV'
      else:
         print ' Channel        = ',cs
      print ' Resolution     = ',str(binsize)+'s'
      print ' Flavour        = ',load_flavour
      print ' FITSGenie Ver. = ',v
      print ''
      print 'Windowing:'
      print ' Shape          = ',wtype
      print ' Sliding        = ',slide!=four_res
      print ' Length         = ',str(four_res)+'s'
      if slide!=four_res:
         print ' Separation     = ',str(slide)+'s'
      print ''
      print 'Normalisation:'
      print ' Normalisation  = ',norm
      print ' Leahy constant = ',const
      print ''
      print 'Other Info:'
      print ' Global Flavour = ',flavour
      print ' Obs length     = ',str(four_res*numstep)+'s'
      print ' Time. Bin-size = ',str(tmdbin)+'s'
      print ' Freq. Bin-size = ',logfreqres
      print ' Num. Time Bins = ',str(int(len(good)))
      print ' Good Time Bins = ',str(int(sum(good)))
      print ' Avg. Rates     = ',str(mean(rates[ogood]))
      print ' Total photons  = ',ph_cts
      print ' Background     = ',str(bg_est)+'cts/s/PCU'
      print ' Errorbars      = ',es

   #-----'reflav' Option---------------------------------------------------------------

   elif specopt=='reflav':
      print 'Please give a new flavour.'
      try:
         nflavour=raw_input('Flavour: ')
         assert nflavour!=''
         flavour=nflavour
         if flavour=='':
            q_flav=''
         else:
            q_flav=' "'+flavour+'"'
         print 'Flavour set to "'+flavour+'"'
      except:
         print 'Invalid flavour!  Flavour remains "'+flavour+'"'

   #-----'export' Option---------------------------------------------------------------

   elif specopt=='export':
      aflname=raw_input('Filename: ')
      try:
         assert len(aflname)>0
         fle=open(aflname,'w')
         spec=npsum(fourgrm, axis=1)/sum(good)        # Sum all spectra in the matrix
                                                      # and divide by the number of
                                                      # good columns
         err=sqrt(npsum( array(errgrm)**2, axis=1))/sum(good)
         for i in range(len(tflm)):
            a=[str(tflm[i]),' ',str(spec[i]),' ',str(err[i]),'\n']
                                                      # Frequency, Power, Error
            fle.writelines(a)
         fle.close()
         print 'ASCII Leahy-normalised spectrum saved to',aflname+'!'
      except:
         print 'Invalid filename!'

   #-----'help' Option-----------------------------------------------------------------

   elif specopt in ['help','?']:                      # Display instructions
      print 'Instructions:'
      print ''
      give_inst()                                     # Re-call the instructions list,
                                                      # defined as the get_inst()
                                                      # function in initialisation

   #-----'quit' Option-----------------------------------------------------------------

   elif specopt not in ['quit','exit']:               # Invalid command if none of the
                                                      # if statements triggered and no
                                                      # 'q' given
      print 'Invalid command!'

   if specopt not in ['quit','exit']:
      print ''
      print ' --------------------'

#-----Exiting Interactive Mode---------------------------------------------------------

print ''
print 'Goodbye!'                                           

#-----Footer---------------------------------------------------------------------------

pan.signoff()

\end{minted}

\section{Back Hydra}

\begin{minted}[fontsize=\scriptsize]{python}
#! /usr/bin/env python

# |----------------------------------------------------------------------|
# |-----------------------------BACK HYDRA-------------------------------|
# |----------------------------------------------------------------------|

# Call as ./bckghydra.py DATA_FILE BACK_FILE SAVE_FILE

# Takes a .plotd file and a background file created with PCABACKEST and returns
#
# Arguments:
#
#  DATA_FILE
#   The absolute path to the file to be used as data.
#
#  BACK_FILE
#   The file to be used as background; does not need to be the same binning as File 1.
#   suggest using pcabackest from FTOOLS to produce this file.
#   FTOOLS can be found at http://heasarc.gsfc.nasa.gov/ftools/
#
#  SAVE_FILE
#   The location to save the resultant background-subtracted file
#

#-----Welcoming Header-----------------------------------------------------------------

print ''
print '-------Running BackHydra: J.M.Court, 2015-------'
print ''

#-----Importing Modules----------------------------------------------------------------

try:
   import sys
   import pan_lib as pan
   from astropy.io import fits
   import numpy as np
   import pylab as pl
except ImportError:
   print 'Modules missing!  Aborting!'
   print ''
   print '------------------------------------------------'
   print ''
   exit()

#-----Checking Validity of Filenames---------------------------------------------------

args=sys.argv
pan.argcheck(args,4)                                  # Must give at least 3 args (Both
                                                      # filenames function call)
data_filename=args[1]                                 # Fetch datafile name from
                                                      # arguments
back_filename=args[2]                                 # Fetch background file name from
                                                      # arguments
save_filename=args[3]
print 'Loading Data...'
datafile_packed=pan.plotdld(data_filename)            # Load datafile
print 'Loading Background...'
backfile_packed=fits.open(back_filename)

#-----Unpack Data----------------------------------------------------------------------

# Collect all data from the data file
b_sub='True'
data_x        = datafile_packed[0]
data_f        = datafile_packed[1]
data_fe       = datafile_packed[2]
data_t_start  = datafile_packed[3]
data_bin_size = datafile_packed[4]
data_gti      = datafile_packed[5]
data_maxpcus  = datafile_packed[6]
data_flavour  = datafile_packed[10]
data_channels = datafile_packed[11]
data_mission  = datafile_packed[12]
data_obs_data = datafile_packed[13]
data_fitsg_v  = datafile_packed[14]
data_obsid=data_obs_data[1]
# Collect only relevant data from the background file
backfile_data=backfile_packed[1].data
back_mission=backfile_packed[1].header['TELESCOP']
if back_mission == 'XTE' :
   try:
      import xtepan_lib as inst                       # Import XTE extraction functions
   except:
      print 'XTE PANTHEON Library not found!  Aborting!'
      pan.signoff()
      exit()
elif back_mission == 'SUZAKU':
   try:
      import szkpan_lib as inst                       # Import SUZAKU extraction
                                                      # functions
   except:
      print 'Suzaku PANTHEON Library not found!  Aborting!'
      pan.signoff()
      exit()
low_chan,high_chan=data_channels.split('-')
back_x,back_f,back_fe = inst.getbg(backfile_data,int(low_chan),int(high_chan))
back_x=back_x[back_f>0]
back_fe=back_fe[back_f>0]
back_f=back_f[back_f>0]
back_t_start  = back_x[0]
back_bin_size = inst.getbin(backfile_packed,None)

#-----Check Background and Data files are compatible-----------------------------------

same_mission  = ( data_mission == back_mission )      # Check the mission names match

if not same_mission:                                  # Abort if missions differ
   print 'Files are from different missions!'
   print 'Aborting!'
   pan.signoff()
   exit()

#-----Shift Arrays---------------------------------------------------------------------

back_x=pan.tnorm(back_x,back_bin_size)                # Force background x array to
                                                      # start at 0
shifted_data_x=data_x+data_t_start-back_t_start       # Create shifted data axis to
                                                      # align with a background
                                                      # starting at 0s
shifted_back_x=back_x+back_t_start-data_t_start       # Create shifted background axis
                                                      # to align with data starting at
                                                      # 0s
if back_x[0]>shifted_data_x[-1] or shifted_data_x[0]>back_x[-1]:
                                                      # Abort if the timescales don't
                                                      # overlap
   print 'WARNING! Files times do not overlap!'
   print ''
   print 'Estimating constant background.'
   print ''
   b_sub='Estimate'
   dump_file=open('backhydra_log.txt','w')
   dump_file.write('File and background times did not overlap!')
   dump_file.close()

#-----Define Background Subtraction----------------------------------------------------

def backgr(i):                                        # Function that returns the
                                                      # appropriate background counts
                                                      # at each point in the datafile
   timestamp=shifted_data_x[i]                        # Collect the timestamp of the
                                                      # ith data element
   st_i=int(timestamp/back_bin_size)                  # Collect the start and endpoints
                                                      # of the bg bin in which the
                                                      # timestamp falls
   ed_i=st_i+1
   if st_i<0:
      return back_f[0],back_fe[0]                     # Return startpoint background if
                                                      # sampling before bg range
   elif ed_i>=len(back_x):
      return back_f[-1],back_fe[-1]                   # Return endpoint background if
                                                      # sampling after bg range
   else:
      posit_in_bin=(timestamp % back_bin_size)/back_bin_size
                                                      # Work out where in the bin the
                                                      # timestamp falls
      f_est  = back_f[st_i]+posit_in_bin*(back_f[ed_i]-back_f[st_i])
                                                      # Linearly interpolate between
                                                      # two background points to return
                                                      # background estimate
      fe_est = back_fe[st_i]+posit_in_bin*(back_fe[ed_i]-back_fe[st_i])
                                                      # Collect error too
      return f_est,fe_est

#-----Perform Background Subtraction---------------------------------------------------

print 'Subtracting Background...'
for i in pan.eqrange(data_x):
   back,back_e=backgr(i)
   data_f[i]-=back
   data_fe[i]=(data_fe[i]**2+back_e**2)**0.5

#-----Re-save Data---------------------------------------------------------------------

print 'Saving...'
new_bg_est=np.mean(back_f)/data_maxpcus
bg_data=(shifted_back_x[(shifted_back_x>=data_x[0]) | (shifted_back_x<=data_x[-1])],
         back_f[(shifted_back_x>=data_x[0]) | (shifted_back_x<=data_x[-1])])
pan.plotdsv(save_filename,data_x,data_f,data_fe,data_t_start,data_bin_size,
            data_gti,data_maxpcus,new_bg_est,b_sub,bg_data,data_flavour,
            data_channels,data_mission,data_obs_data,data_fitsg_v)
print ''
print 'Background Subtracted file saved as "'+save_filename+'.plotd"!'

\end{minted}

\section{PAN Lib}

\begin{minted}[fontsize=\scriptsize]{python}
#! /usr/bin/env python

# |----------------------------------------------------------------------|
# |-------------------------------PAN_LIB--------------------------------|
# |----------------------------------------------------------------------|

# A selection of useful functions which are placed here to reduce clutter in the other
#  files of

# PANTHEON.
#
# Contents:
#
# ARGCHECK   - compares the list of arguments against a value given as the minimum
#              allowed number of arguments.  If the list of arguments is too short,
#              throw a warning and kill the script.
#
#  BINIFY    - takes a x-series with its associated y-axis data and y-axis errors.
#              Rebins the data into larger linear bins with a width of the user's
#              choosing, and returns the tuple x,y,y_error.
#
#  BOOLVAL   - takes a list of Boolean values and, interpreting it as binary, returns 
#              its integer value.
#
#  EQRANGE   -
#
#  EVAL_BURST-
#
#  FILENAMECHECK  - checks to see whether a proposed input file has the correct file
#                   extension.
#
#  FOLDIFY   - takes a time series with its associated y-axis data and y-axis errors. 
#              Folds this data over a time period of the user's choosing, and returns
#              them as the tuple x,y,y_error.
#
#  FOLD_BURSTS - uses GET_BURSTS to obtain burst locations then interpolates to
#                populate phase information for all other points
#
#  GET_BURSTS- takes an array of data, looks for bursts and returns an array of tuples
#              containing the start and end points of these bursts.
#
#  GET_DIP   - returns the index of the lowest point between two user-defined flags in
#              a dataset.
#
#  GTIMASK   - returns a data mask when given a time series and a GTI object
#
#  LBINIFY   - takes a linearly binned x-series with associated y-axis data and y-axis
#              errors and rebins them into bins of a constant width in logx space.
#              In places where the logarithmic bins would be finer than the linear
#              bins, the linear bins are retained.
#
#  LEAHYN    - takes the raw power spectrum output from the scipy FFT algorithm and
#              normalises it using Leahy normalisation.
#
#  LH2RMS    - takes a Leahy-normalised power spectrum and converts it to an
#              (RMS/Mean)^2-normalised power spectrum.
#
#  LHCONST   - returns the normalisation of the white noise component in a Leahy-
#              normalised power spectrum with no features in the range 1.5kHz - 4kHz.
# 
#
#  MXREBIN   - takes a 2-dimensional set of data and corresponding errors linearly
#              binned on the x-axis and rebins them by an integer binning factor of
#              the user's choice.
#
#  NONES     - like np.zeros, but with None.
#
#  PDCOLEX   - extracts colours from a set of 2 or 3 lightcurves
#
#  PLOTDLD   - load and unpickle a .plotd file and extract its data.
#
#  PLOTDSV   - collect a selection of data products as a library, pickle it and save
#              as a .plotd file.
#
#  RMS_N     - takes the raw power spectrum output from the scipy FFT algorithm and
#              normalises it using (RMS/Mean)^2 normalisation.
#
#  SAFE_DIV  - Divides two arrays by each other, replacing NaNs that would be caused
#              by div 0 errors with zeroes.
#
#  SIGNOFF   - prints an dividing line with some space.  That's all it does.
#
#  SINFROMCOS- calculates the sines of an array of values when also passed their co-
#              sines.  If both sines and cosines of the array are required, this method
#              is faster than calling both trig functions.
#              Also contains function COSFROMSIN.
#
#  SLPLOT    - plots an x-y line plot of two sets of data, and then below plots the
#              same data on another set of axes in log-log space.
#
#  SPECALD   - load and unpickle a .speca file and extract its data.
# 
#  SPECASV   - collect a selection of data products as a library, pickle it and save
#              as a .speca file.
#
#  SRINR     - calculates whether a value given by a user is within an existant evenly
#              spaced array and, if it is, returns the index value of the closest
#              match to this value within the array.
#              Intended for validating subranges specified by user.
#
#  TNORM     - takes a list of times, and subtracts the lowest value from each entry
#              such that a new list starting with 0 is produced.  Large number
#              subtraction errors are avoided by checking that every entry is an
#              integer number of time-resolution steps from zero.
#
#  UNIQFNAME - checks if a proposed filename is currently in use and, if so, proposes
#              an alternative filename to prevent overwrite.
#
#  XTRFILLOC - takes a filepath and outputs the file name and its absolute(ish)
#              location
#

#-----Importing Modules----------------------------------------------------------------

import os
try:
   import cPickle as pickle
except ImportError:
   import pickle
from matplotlib import pyplot as pl
import warnings
import scipy.optimize as optm
import scipy.interpolate as intp
import scipy.stats as stt
import scipy.signal as sgnl
import numpy as np
import operator as ope
try:
   import numba as nb
   gotnumba=True
except ImportError:
   print 'Warning: numba module not found!  May run slow.'
   gotnumba=False
from math import pi
from numpy import random as rn

# =========== CLASSES =================================================================

#-----ObsData--------------------------------------------------------------------------

class obsdata(object):
   def __init__(self,filename,tcol,ycol,ecol,name=''):
      self.name=name
      xs=[]
      ys=[]
      es=[]
      f=open(filename)
      for line in f:
         l=line.split()
         try:
            t=float(l[tcol])
            y=float(l[ycol])
            e=float(l[ecol])
            xs.append(t)
            ys.append(y)
            es.append(e)
         except (IndexError, ValueError):
            pass
      f.close()
      xs=np.array(xs)
      ys=np.array(ys)
      es=np.array(es)
      assert len(xs)==len(ys)
      assert len(xs)==len(es)
      self.t=xs
      self.y=ys
      self.e=es
   def get_len(self):
      return self.t[-1]-self.t[0]
   def get_start(self):
      return self.t[0]
   def get_end(self):
      return self.t[-1]
   def plot(self):
      pl.figure()
      pl.errorbar(self.t,self.y,yerr=self.e,fmt='0.7',zorder=-1)
      pl.plot(self.t,self.y,'k')
      pl.title(self.name)
      pl.show(block=True)
   def plot_on_ax(self,ax,zorder=0,errcolor='0.7',color='k'):
      ax.errorbar(self.t,self.y,yerr=self.e,fmt=errcolor,zorder=zorder-0.5)
      ax.plot(self.t,self.y,color,zorder=zorder,label=self.name)

# =========== FUNCTIONS ===============================================================

#-----Setup Conditional Wrapper to use jit when available------------------------------

class mjit(object):
    def __call__(self, f):
        if not gotnumba:
            return f
        else:
            return nb.jit(f)

#-----ArgCheck-------------------------------------------------------------------------

def argcheck(x,y):
   '''Argument Checker

   Description:
    Takes a list (of arguments passed into a script), and a value representing the
    smallest allowable number of arguments.  If the length of the list is smaller than
    the criterion, an error is returned and the script is killed.
   Inputs:
    x - LIST   : The list of arguments passed into the current working script, i.e.
                 x=sys.argv.
    y - INTEGER: The minimum allowable number of arguments.  Caution; if called from
                 bash, sys.argv returns the function call as the x[0] element, so y is
                 1 greater than the number of user inputs.
   Outputs:
    [none]

   -J.M.Court, 2014'''
   if len(x)<y:
      print 'Not enough arguments!'
      signoff()
      exit()

#-----Binify---------------------------------------------------------------------------

@mjit()
def binify(x,y,ye,binsize):                          # Defining 'binify' subscript
   '''Binify

   Description:
    Takes a 2-dimenstional set of data which has already been evenly binned on the
    x-axis, and re-bins it into larger, evenly spaced bins on the x-axis.
   Inputs:
    x       -  LIST: The x-values of the two-dimensional data.
    y       -  LIST: The y-values of the two-dimensional data, must be the same length
                     as x.
    ye      -  LIST: The errors associated with the y-values of the two-dimensional
                     data, must be the same length as x and y.
    binsize - FLOAT: the size of the new x-axis bins in which to re-bin the data.
   Outputs:

    xb      -  LIST: The re-binned x-values of the two-dimensional data, i.e. an array
                     of the left-hand edges of the new bins.
    yb      -  LIST: The re-binned y-values of the two-dimensional data.
    yeb     -  LIST: The errors associated with the rebinned y-values of the 2D data.

   -J.M.Court, 2014'''
   binlx=binsize*np.floor(x[0]/binsize)               # Initialising 'bin lowest x',
                                                      # or the lowest x value of the
                                                      # current bin
   binct=0.0                                          # Initialising 'bin count', or
                                                      # number of values sorted into 
                                                      # the current bin
   xb=[x[0]]                                          # Setting up arrays to append
                                                      # binned values into
   yb=[0]   
   yeb=[0]
   for xid in range(len(x)):
      if x[xid]-binlx < binsize:                      ## If the difference between the
                                                       #  current x and bin start x is
                                                       #  less than bin width:
         binct+=1
         yb[-1]=yb[-1]+y[xid]                          #  Add y to current bin
         yeb[-1]=yeb[-1]+(ye[xid]**2)                  #  Add y error in quadrature to
                                                       #  current bin
      else:                                           ## Otherwise:
         binlx+=binsize                                #  Create new bin with minimum x
                                                       #  equal to current x
         xb.append(binlx)                              #  Append next x value into new
                                                       #  array element
         yb[-1]=yb[-1]/binct                           #  Divide y in previous bin by
                                                       #  bincount to get the average
         yeb[-1]=(np.sqrt(yeb[-1]))/binct              #  Sqrt error and divide by bin
                                                       #  count
         yb.append(y[xid])                             #  Append current y value into
                                                       #  new array element
         yeb.append((ye[xid])**2)
         binct=1                                       #  Reset bin count to 1
   yb[-1]=yb[-1]/binct                                ## Clean up final bin
   yeb[-1]=(np.sqrt(yeb[-1]))/binct
   return np.array(xb),np.array(yb),np.array(yeb)

#-----BoolVal--------------------------------------------------------------------------

def boolval(data,reverse=True):
   '''Boolean Evaluator

   Description:
    Given a list of Boolean values, interprets them as binary and returns the 
    corresponding integer.  By default reads lists as having the highest-value digit
    first.
   Inputs:
    data    - LIST: a list of lists Boolean values.
    reverse - BOOL: If set to False, then the Boolean strings will be interpreted as
                    binaries with the lowest value (1) first.
   Outputs:
    data    - LIST: the integer values represented by 'data' if its values are
                    interpreted as binary.

   -J.M.Court, 2015'''
   keyr=range(len(data[0]))                           # Set up the 'key range' to
                                                      # convert Bool list into int
   if reverse:
      keyr=keyr[::-1]                                 # Reverse the key range
   mult=[1<<i for i in keyr]                          # Mult is a list of all the
                                                      # powers of 2 from 2^0 to 2^(len
                                                      # of data)
   data=np.array(data)*mult
   data=np.sum(data,axis=1)                           # Multiply Boolean list by mult,
                                                      # sum per row
   return data

#-----CalcLoop-------------------------------------------------------------------------

def calcloop(flux,color,fluxe,colore):
   y=flux
   ye=fluxe
   x=color
   xe=colore
   maxyp=y.tolist().index(max(y))
   minyp=y.tolist().index(min(y))
   if maxyp>minyp:
      interv=(minyp,maxyp)
   else:
      interv=(maxyp,minyp)
   newx={}
   newxe={}
   newy={}
   newye={}
   i1=1
   i2=2
   newx[False]=x[interv[0]:interv[1]+1]
   newxe[False]=xe[interv[0]:interv[1]+1]
   newy[False]=y[interv[0]:interv[1]+1]
   newye[False]=ye[interv[0]:interv[1]+1]
   newx[True]=np.append(x[interv[1]:],x[:interv[0]+1])
   newxe[True]=np.append(xe[interv[1]:],xe[:interv[0]+1])
   newy[True]=np.append(y[interv[1]:],y[:interv[0]+1])
   newye[True]=np.append(ye[interv[1]:],ye[:interv[0]+1])
   if len(newy[True])>=len(newy[False]):
      master=True
   else:
      master=False
   slave = not master
   newmaster=[list(x) for x in zip(*sorted(zip(newy[master], newx[master],
              newxe[master]), key=ope.itemgetter(0)))]
   sp=intp.UnivariateSpline(newmaster[0],newmaster[1],1/(np.array(newmaster[2])**2),
                            ext=0,k=1)
   se=intp.UnivariateSpline(newmaster[0],newmaster[2],ext=0,k=3)
   aves=0.0
   for q in range(len(newy[slave])-2):
      i=q+1
      aves+=( (sp(newy[slave][i])-newx[slave][i]) / np.sqrt(newxe[slave][i]**2+
             se(newy[slave][i])**2) )**2   
   loopc=stt.chisqprob(aves,len(newy[slave])+len(newy[master])-2)
   return loopc

#-----CCor-----------------------------------------------------------------------------

# Cross-Correlate

def ccor(data1,data2):
   assert len(data1)==len(data2)
   data1=np.array(data1)
   data2=np.array(data2)
   crosscor=[]
   crosscore=[]
   for i in range(len(data1)):
      r1=data1[(-i-1):]
      r2=data2[:i+1]
      mr1=np.mean(r1)
      mr2=np.mean(r2)
      sr1=np.std(r1)
      sr2=np.std(r2)
      ccorarray=(r1-mr1)*(r2-mr2)/(sr1*sr2)
      crosscor.append(np.mean(ccorarray))
      crosscore.append(np.std(ccorarray))
   for i in range(len(data1)-1):
      r1=data1[:(-i-1)]
      r2=data2[i+1:]
      mr1=np.mean(r1)
      mr2=np.mean(r2)
      sr1=np.std(r1)
      sr2=np.std(r2)
      ccorarray=(r1-mr1)*(r2-mr2)/(sr1*sr2)
      crosscor.append(np.mean(ccorarray))
      crosscore.append(np.std(ccorarray))
   times=np.array(range(len(crosscor)))
   times=times+1-len(data1)
   return times,crosscor,crosscore

#-----Eddington------------------------------------------------------------------------

def eddington(M):
   '''Eddington

   Description:
    Returns the Eddington Limit for a given black hole mass, assuming Hydrogen accreta.
   Inputs:
    M - FLOAT: The mass of the black hole in solar masses
   Outputs:
    L - FLOAT: The Eddington Luminosity in ergs/s

   -J.M.Court, 2016'''
   return 1.26E38*M

#-----EqRange--------------------------------------------------------------------------

# Equal Length Range

def eqrange(array):
   '''Equal Length Array

   Description:
    Creates a range with an equal length to that of the array or list given as an
    argument
   Inputs:
    array - ARRAYLIKE: The array to be used as comparison.
   Outputs:
    A range with equal length to the input

   -J.M.Court, 2015'''
   return range(len(array))

#-----Eval_Burst-----------------------------------------------------------------------

# Evaluate Burst

@mjit()
def eval_burst(t,y):
   '''Evaluate Burst

   Description:
    Takes a piece of data and its associated time array, and treats it as a single
    'burst'-like pattern.  Returns the peak flux, peak time, rise time, fall time of
    this burst.  Works best with Get_Bursts.
   Inputs:
    t         - ARRAY: The t (time) co-ordinates of the data.
    y         - ARRAY: The y co-ordinates of the data.
   Outputs:
    peak      - FLOAT: The highest y-value in the burst
    peak_time - FLOAT: The time at which the peak occurs
    rise_time - FLOAT: The time between the start of the burst and its peak
    fall_time - FLOAT: The time between the peak of the burst and its end
   '''
   peak=max(y)
   trough=min(y)
   p_ind=np.array(y).tolist().index(peak)
   rise_time=t[p_ind]-t[0]
   fall_time=t[-1]-t[p_ind]
   peak_time=t[p_ind]
   return peak,trough,peak_time,rise_time,fall_time

#-----FlnCheck-------------------------------------------------------------------------

def filenamecheck(filename,validext,continu=False):
   '''Filename Checker
   
   Description:
    Takes a filename and a string representing the expected file extension.  If the
    extension of the file does not match expectations, either kill the script or return
    'False'.
   Inputs:
    filename - STRING: The filename to be checked.
    validext - STRING: The extenstion expected for the file (WITHOUT the leading '.')
    continu  -   BOOL: [Optional: Default=False] If True, returns a value of False for
                       an incorrect file extension.  If False (default), kills the
                       script upon finding an incorrect file extension.
   Outputs:
    iscorr   -   BOOL: True if the file has the correct extension, False otherwise

   -J.M.Court, 2015'''
   flext=(filename.split('.')[-1])
   if flext != validext:
      if continu:
         return False
      else:
         print 'Invalid input file!  Must use .'+str(validext)+' file!'
         signoff()
         exit()
   else:
      return True

#-----Foldify--------------------------------------------------------------------------

def foldify(t,y,ye,period,binsize,phres=None,name='',compr=False,verb=True):
   '''Foldify

   Description:
    Folds a two-dimensional set of data over a period in the first dimension using the
    folding script which is provided in PyAstronomy.  Also calculates how much the peak
    trough difference of the data has been compressed by the fold, and tells the user.
   Inputs:
    t       -   LIST: The t- or x-values of the two-dimensional data.  The period to be
                      folded over is a period in this dimension.
    y       -   LIST: The y-values of the two-dimensional data, must be the same length
                      as x.
    ye      -   LIST: The errors associated with the y-values of the two-dimensional
                      data, must be the same length as x and y.
    period  -  FLOAT: The period of the data, in the same units as the data's x-values.
    binsize -  FLOAT: The size of the new x-axis bins in which to re-bin the data.
    phres   -  FLOAT: [Optional: Default=None] The resolution, in frequency space, at
                      which data will be shown.
    name    - STRING: [Optional: Default=''] the name of the file to be folded.  This
                      name will be used in text outputs printed to screen.
    compr   -   BOOL: [Optional: Default=False] if set True, gives a fourth output
                      which is the amount by which the min-max difference of the
                      data-set was compressed by folding.
    verb    -   BOOL: [Optional: Default=True] if set false, suppresses all non-error
                      text output.
   Outputs:
    newt    -  LIST: A clipped version of the input t which now only corresponds to one
                     period.
    newy    -  LIST: The folded y-values of the two-dimensional data for one period.
    newye   -  LIST: The errors associated with the folded y-values of the
                     two-dimensional data for one period.
    compr   - FLOAT: see 'compr' input option

   -J.M.Court, 2015'''
   try:
      from PyAstronomy.pyasl import foldAt
   except:
      print 'PyAstronomy module not found; aborting!'
      return t,y,ye
   tn=tnorm(t,binsize)
   if verb:
      print 'Folding File '+name+'...'
   ptdiff=max(y)-min(y)                               # Flux range before folding
   phases=foldAt(tn,period)
   if phres==None:
      try:
         phres=float(raw_input('Input Phase Resolution (0-1): '))
         assert phres<=1.0
      except:
         print 'Invalid phase resolution!  Aborting!'
         return t,y,ye
   npbins=int(1.0/phres)
   phasx =np.arange(0,1,phres)
   phasy =np.zeros(npbins)
   phasye=np.zeros(npbins)
   ny=np.zeros(npbins)
   for i in range(len(y)):
      k=int(phases[i]*npbins)
      phasy[k]+=y[i]
      phasye[k]+=(ye[i]**2)
      ny[k]+=1
   phasy=phasy/ny
   phasye=np.sqrt(phasye)/ny
   afdiff=max(phasy)-min(phasy)                       # Flux range after folding
   if verb:
      print 'Flattened by '+str(100-afdiff/ptdiff*100)+'%'
   if compr:
      return phasx,phasy,phasye,(afdiff/ptdiff)
   else:
      return phasx,phasy,phasye

#-----Fold_Bursts----------------------------------------------------------------------

@mjit()
def fold_bursts(times,data,q_lo=50,q_hi=90,do_smooth=False,alg='cubic spline',
                savgol=5):
   '''Return Phases Using Bursts as Reference Points

   Description:
    Takes a lightcurve and identifies 'bursts' in the data.  The peak of each burst is
    considered to be at zero phase, and all other points are assigned phases by
    linearly interpolating between them.
   Inputs:
    data       - ARRAY: The data in which bursts are sought.
    q_low      - FLOAT: [Optional: Default=30] The percentile value of the data which
                        will be used as the low-pass threshold.  This threshold
                        determines the edges of already-located bursts, and thus
                        changing it will change the quality of bursts but not the
                        quantity.
    q_mid      - FLOAT: [Optional: Default=50] The percentile value of the data which
                        will be used as the med-pass threshold.  This threshold
                        determines how deep an trough must be before the regions either
                        side of it are considered separate bursts-candidate regions.
                        Must be larger than q_low.
    q_hi       - FLOAT: [Optional: Default=70] The percentile value of the data which
                        will be used as the hi-pass threshold.  This threshold
                        determines how much peak flux a previously defined
                        burst-candidate region must be before it is considered a true
                        burst.  larger than q_med.
    smooth     -  BOOL: [Optional: Default=False] Apply a Savitsky-Golay Filter to
                        smooth the lightcurve before fetching peaks.
    alg        -STRING: [Optional: Default='cubic spline'] The algorithm to use to
                        obtain peaks.
   Outputs:
    burst_locs -  LIST: A list of tuples containing the start and end indices of each
                        burst.

   -J.M.Court, 2015'''
   assert len(times)==len(data)
   peaks=get_bursts(data,q_lo,q_hi,just_peaks=True,smooth=do_smooth,alg=alg,
                    times=times,savgol=savgol)
   peaks.sort()
   p0=peaks[0]
   pe=peaks[-1]
   #data=data[p0:pe+1]
   #times=times[p0:pe+1]
   phases=np.zeros(len(times))
   peaks=np.array(peaks)
   npeaks=[0]   
   for i in range(len(peaks)-1):
      if peaks[i+1]-peaks[i]>0.1*(peaks[-1]/float(len(peaks))):
                                                      # Remove any peaks separated by 
                                                      # less than 10% of average
                                                      # separation
         npeaks.append(peaks[i+1])
   peaks=npeaks
   numpeaks=len(peaks)
   phases=get_phases_intp(data,windows=1,q_lo=20,q_hi=90,peaks=peaks,givespline=False)
   return np.array(phases),numpeaks,(peaks[0],peaks[-1])  

#-----Gauss----------------------------------------------------------------------------

@mjit()
def gauss(mean,standev,x):
   '''Gauss.  Returns a Gaussian'''
   return (1.0/(standev*(2*np.pi)**0.5))*np.exp(-(x-mean)**2/(2*(standev**2)))

#-----Get_Bursts-----------------------------------------------------------------------

def get_bursts(data,q_lo=50,q_hi=90,just_peaks=False,smooth=False,savgol=5,
               alg='cubic spline',times=None):
   '''Get Bursts

   Description:
    Takes a lightcurve and identifies 'bursts' in the data; short, discrete regions of
    increased flux.  Returns the locations of all peaks identified as a list of tuples,
    each of which consist of two integers which correspond to the indices of the start
    and end of a peak in the original data.
   Inputs:
    data       - ARRAY: The data in which bursts are sought.
    q_low      - FLOAT: [Optional: Default=50] The percentile value of the data which
                        will be used as the low-pass threshold.  This threshold
                        determines the edges of already-located bursts, and thus
                        changing it will change the quality of bursts but not the
                        quantity.
    q_hi       - FLOAT: [Optional: Default=90] The percentile value of the data which
                        will be used as the hi-pass threshold.  This threshold
                        determines how much peak flux a previously defined burst-
                        candidate region must be before it is considered a true burst.
                        larger than q_med.
    just_peaks -  BOOL: [Optional: Default=False] If set to true, the function will
                        return a list of peak indices instead of a list of peak
                        datasets.
    smooth     -  BOOL: [Optional: Default=False] If set to true, applies a univariate
                        spline to the data to smooth it.
    savgol     -   INT: [Optional: Default=5] The window size for the Savitsky-Golay
                        filter
    alg        -STRING: [Optional: Default='cubic spline'] The algorithm to use to
                        obtain peaks.
    times      - ARRAY: [Optional: Default=None] If 'load' selected for algorithm, user
                        must also give the time array associated with the data.
   Outputs:
    burst_locs -  LIST: A list of tuples containing the start and end indices of each
                        burst.

   -J.M.Court, 2015'''
   if smooth:                                         # If user has requested
                                                      # smoothing...
      savgol=int(savgol)
      if savgol%2==0:
         savgol+=1
      data=sgnl.savgol_filter(data,savgol,3)          #  Smooth it!
   high_thresh=np.percentile(data,q_hi)
   low_thresh=np.percentile(data,q_lo)
   over_thresh=data>low_thresh                        # Create a Boolean array by
                                                      # testing whether the input array
                                                      # is above the mid threshold.
                                                      # Each region of consecutive
                                                      # 'True' objects is considered
                                                      # a burst-candidate region.
   peak_locs=[]
   burst_locs=[]
   if alg=='cubic spline':
      while True:                                                                                                               
         masked=np.array(data)*over_thresh            # Reduce all data outside of
                                                      # burst-candidate regions to zero

         if max(masked)<high_thresh:                  # If highest peak in all
                                                      # remaining burst-candidate
                                                      # regions is below the high
                                                      # threshold, assume there are no
                                                      # more bursts to be found.
            break
         peak_loc=masked.tolist().index(max(masked))  # Find peak in remaining data
         peak_locs.append(peak_loc)                   # Construct list of peak location
         i=peak_loc
         while i<len(data) and over_thresh[i]:        # Scrub the True objects in the 
                                                      # Boolean array corresponding to 
                                                      # that peak's candidate region,
                                                      # thus removing it
            over_thresh[i]=False
            i+=1
         i=peak_loc-1
         while i>=0 and over_thresh[i]:
            over_thresh[i]=False
            i-=1
   elif alg=='load':
      if times==None:
         raise Exception('Must provide data times when using "load" mode!')
      times=np.array(times)
      loadfilename=raw_input('Burst File Name: ')
      loadfile=open(loadfilename,'r')
      with open(loadfilename,'r') as f:
         first_line=f.readline()
      if len(first_line.split(','))>6:
         dataex=np.array(first_line.split(','))    
         for l in dataex:
            peak_locs.append(float(l))  
      else:
         for line in loadfile:
            l=line.split(',')
            peak_locs.append(float(l[0]))
      loadfile.close()
      peak_locs=(np.array(peak_locs)-times[0])/(times[1]-times[0])
      peak_locs=peak_locs.astype(int)
   if just_peaks: return peak_locs
   peak_locs.sort()                                   # Sort the list so peaks can be 
                                                      # returned in chronological order
   start_col=get_dip(data,0,peak_locs[0])
   for i in range(len(peak_locs)):
      if i==len(peak_locs)-1:
         n_peak=len(data)-1
      else:
         n_peak=peak_locs[i+1]
      end_col=get_dip(data,peak_locs[i],n_peak)
      if end_col>start_col:                           # Force any null-length 'bursts'
                                                      # to be removed
         burst_locs.append((start_col,end_col))
         start_col=end_col
   return burst_locs

#-----Get_Bursts_Windowed--------------------------------------------------------------

def get_bursts_windowed(data,windows,q_lo=50,q_hi=90,smooth=False):
   '''Get Bursts

   Description:
    Takes a lightcurve and identifies 'bursts' in the data; short, discrete regions of
    increased flux.  Returns the locations of all peaks identified as a list of tuples,
    each of which consist of two integers which correspond to the indices of the start
    and end of a peak in the original data.  Currently can only return the location of
    all burst peaks.
   Inputs:
    data       - ARRAY: The data in which bursts are sought.
    windows    -   INT: The number of windows into which to divide the data
    q_low      - FLOAT: [Optional: Default=50] The percentile value of the data which
                        will be used as the low-pass threshold.  This threshold
                        determines the edges of already-located bursts, and thus
                        changing it will change the quality of bursts but not the
                        quantity.
    q_hi       - FLOAT: [Optional: Default=90] The percentile value of the data which
                        will be used as the hi-pass threshold.  This threshold
                        determines how much peak flux a previously defined
                        burst-candidate region must be before it is considered a true
                        burst.  larger than q_med.
    smooth     -  BOOL: [Optional: Default=False] If set to true, applies a univariate
                        spline to the data to smooth it.
    savgol     -   INT: [Optional: Default=1] The window size for the Savitsky-Golay
                        filter
   Outputs:
    burst_locs -  LIST: A list of tuples containing the peak of each burst.

   -J.M.C.Court, 2016'''
   windows=int(windows)
   windowlength=len(data)/windows
   win_starts=[windowlength*i for i in range(windows)]
   win_ends=[windowlength*(i+1) for i in range(windows-1)]
   win_ends.append(len(data)+1)
   peak_locs=[]
   for i in range(windows):
      new_bursts=np.array(get_bursts(data[win_starts[i]:win_ends[i]],q_lo=q_lo,
                          q_hi=q_hi,just_peaks=True,smooth=smooth,
                          savgol=(win_ends[i]-win_starts[i])/4.0))
      new_bursts+=win_starts[i]
      peak_locs+=new_bursts.tolist()
   return peak_locs

#-----Get Dip--------------------------------------------------------------------------

def get_dip(data,start,finish,smooth=False,savgol=1):
   '''Get Dips

   Description:
    Returns the index of the lowest value between two given points in a dataset
   Inputs:
    data    - LIST:  The dataset in which a trough is to be found
    start   -  INT:  The index of the startpoint of the user-defined sub-range
    finish  -  INT:  The index of the endpoint of the user-defined sub-range
    smooth  -  BOOL: [Optional: Default=False] If set to true, applies a univariate
                     spline to the data to smooth it.
   Outputs:
    key_col -  INT: The index of the lowest value in the user-defined sub-range

   -J.M.Court, 2015'''
   data=np.array(data)
   data_l=np.arange(len(data))
   if smooth:
      savgol=int(savgol)
      if savgol%2==0:
         savgol+=1
      data=sgnl.savgol_filter(data,savgol,3)
   data=data*(data_l>=start)*(data_l<finish)
   data[data==0]=max(data)      
   keycol_loc=data.tolist().index(min(data))
   return keycol_loc 

#-----Get Phase------------------------------------------------------------------------

@mjit()
def get_phases(data,windows=1,q_lo=20,q_hi=90):
   '''Get Phases

   Description:
    Given a set of data points evenly spaced in time, returns the phase coordinate of
    each point.
   Inputs:
    data   -  ARRAY: The data in which bursts are sought.
    windows -   INT: [Optional: Default=1] The number of windows into which to divide
                     the data for analysis.
    q_low   - FLOAT: [Optional: Default=20] The percentile value of the data which will
                     be used as the low-pass threshold.  This threshold determines the
                     edges of already-located bursts, and thus changing it will change
                     the quality of bursts but not the quantity.
    q_hi    - FLOAT: [Optional: Default=90] The percentile value of the data which will
                     be used as the hi-pass threshold.  This threshold determines how
                     much peak flux a previously defined burst-candidate region must be
                     before it is considered a true burst.  larger than q_med.
   Outputs:
    phases - ARRAY: The phase coordinates of the input data

   -J.M.C.Court, 2016'''
   data_keys=range(len(data))                         # Generate the data indices
   data=np.array(data)                                # Format the input data
   data_phas=np.zeros(len(data),dtype=float)          # Create the phase array
   peak_keys=get_bursts_windowed(data,windows,q_lo=50,q_hi=90,smooth=False)
   peak_keys.sort()                                   # Fetch and sort the indices
                                                      # corresponding to peaks
   dip_keys=[]                                        # Create empty array for indices
                                                      # of dips
   if peak_keys[0]!=0:
      dip_keys.append(get_dip(data,0,peak_keys[0]))   # If there is no peak in the
                                                      # first slot, prepend a false dip
                                                      # at element 0
   for i in range(len(peak_keys)-1):
      dip_keys.append(get_dip(data,peak_keys[i],peak_keys[i+1]))
                                                      # For every pair of peaks, fetch
                                                      # the index of the dip between
                                                      # them
   if peak_keys[-1]!=data_keys[-1]:
      dip_keys.append(get_dip(data,peak_keys[-1],data_keys[-1]))
                                                      # If there is no peak in the last
                                                      # slot, append a false dip at
                                                      # element -1
   data_keys=np.array(data_keys)
   peak_keys=np.array(peak_keys)
   dip_keys=np.array(dip_keys)
   if peak_keys[0]==0:                                # Prepend false dips/peaks at 0 
                                                      # or at a negative index to force
                                                      # all comparisons to be valid.
                                                      # This has the effect of making
                                                      # the fit messy at the beginning
                                                      # and end of the time series,
                                                      # but this is not aviodable.
      dip_keys=np.hstack((-1,dip_keys))
   elif dip_keys[0]==0:
      peak_keys=np.hstack((-1,peak_keys))                             
                                                      #         ^
   elif peak_keys[0]<dip_keys[0]:                     #         |
      dip_keys=np.hstack((0,dip_keys))                #         |
      peak_keys=np.hstack((-1,peak_keys))             #         |
   else:                                              #         |
      peak_keys=np.hstack((0,peak_keys))              #         |
      dip_keys=np.hstack((-1,dip_keys))               #         |
                                                      #         |
   if peak_keys[-1]==data_keys[-1]:                   # Appending false dips/peaks, see
                                                      # above
      dip_keys=np.hstack((dip_keys,data_keys[-1]+1))
   elif dip_keys[-1]==data_keys[-1]:
      peak_keys=np.hstack((peak_keys,data_keys[-1]+1))
   elif peak_keys[-1]>dip_keys[-1]:
      dip_keys =np.hstack((dip_keys ,data_keys[-1]))
      peak_keys=np.hstack((peak_keys,data_keys[-1]+1)) 
   else:
      peak_keys=np.hstack((peak_keys,data_keys[-1]))
      dip_keys =np.hstack((dip_keys ,data_keys[-1]+1))
   for key in data_keys:                              # For each key, extract phase
      if key in peak_keys:                            # If key is a peak, set phase to
                                                      # 0.5
         data_phas[key]=0.5
         continue
      elif key in dip_keys:                           # If key is a dip, set phase to 
                                                      # 0.0
         continue
      prev_peak=peak_keys[peak_keys<key][-1]          # Otherwise, collect the keys of
                                                      # the previous/next peaks and
                                                      # dips
      prev_dip =dip_keys[ dip_keys<key][-1]
      next_peak=peak_keys[peak_keys>key][0]
      next_dip =dip_keys[ dip_keys>key][0]
      is_in_fall=(prev_peak>prev_dip)                 # If the considered element's
                                                      # most recent peak was more 
                                                      # recent than the most recent dip
                                                      # then it is in the 'fall' regime
                                                      # (phase>0.5)
      prev_marker=float(max(prev_peak,prev_dip))      # Collect the indices of the
                                                      # previous/next significant
                                                      # points of either type
      next_marker=float(min(next_peak,next_dip))
      phase=0.5*(float(key)-prev_marker)/(next_marker-prev_marker)
                                                      # Define phase as half the
                                                      # considered point's fractional
                                                      # distance between them
      if is_in_fall:
         phase+=0.5                                   # Add 0.5 in the fall regime
      data_phas[key]=phase
   return data_phas  

#-----Get Phase INTP-------------------------------------------------------------------

def get_phases_intp(data,windows=1,q_lo=20,q_hi=90,peaks=None,givespline=False):
   if peaks==None:
      peak_keys=get_bursts(data,q_lo=q_lo,q_hi=q_hi,just_peaks=True,smooth=False,
                           savgol=5)
   else:
      peak_keys=peaks
   peak_keys.sort()
   peak_keys=np.array(peak_keys)
   data=np.array(data)
   p_phases=np.arange(0,len(peak_keys),1.0)
   spline=intp.PchipInterpolator(peak_keys, p_phases, extrapolate=True)
   spline.firstpeak=peak_keys[0]                      # Store the keys of the first and
                                                      # last peaks in the spline object
   spline.lastpeak=peak_keys[-1]
   if givespline:
       return spline
   phases=spline(range(len(data)))
   phases=np.remainder(phases,1)
   return phases

#-----GTIMask--------------------------------------------------------------------------

@mjit()
def gtimask(times,gtis):

   '''GTI Mask

   Description:
    Takes a time series and a GTI file taken from FITS and creates a mask to place over
    the time series with 'True' values over timestamps within the GTI and 'False'
    values elsewhere.
   Inputs:
    times -       LIST: A list of times, the x-axis of the data to be considered.  Must
                        be in same physical units as GTI.
    gtis  - FITS TABLE: A FITS table object containing a list of 2-element lists, each
                        of which is the start and endpoint respectively of a good time
                        index.
   Outputs:
    mask  -       LIST: A mask to put over data to hide values outside of the GTI.

   -J.M.Court, 2015'''
   times=np.array(times)
   mask=np.zeros(len(times),dtype=bool)               # Set up initial blank list of
                                                      # False
   for gti in gtis:                                   # For every GTI index:
      smask=(times>gti[0]) & (times<gti[1])           # Create a submask which is the 
                                                      # 'and'ed product of
                                                      # times>gti_start and
                                                      # times<gti_end
      mask=mask|smask                                 # 'or' the submask with the main
                                                      # mask
   return mask

#-----LBinify--------------------------------------------------------------------------

@mjit()
def lbinify(x,y,ye,logres):

   '''Logarithmic Binify

   Decription:
    Takes a 2-dimenstional set of data which has already been evenly binned on the
    x-axis, and re-bins it into larger bins on the x-axis which are evenly spaced in
    log-space.  The original linear binning is retained in regions of the data where
    the logarithmic binning would be smaller than the data resolution.
   Inputs:
    x      -  LIST: The x-values of the two-dimensional data.
    y      -  LIST: The y-values of the two-dimensional data, must be the same length
                    as x.
    ye     -  LIST: The errors associated with the y-values of the two-dimensional
                    data, must be the same length as x and y.
    logres - FLOAT: the size of the new x-axis bins, in log10-space, in which to re-bin
                    the data.
   Outputs:
    xb     -  LIST: The re-binned x-values of the two-dimensional data, i.e. an array
                     of the left-hand edges of the new bins.
    yb     -  LIST: The re-binned y-values of the two-dimensional data.
    yeb    -  LIST: The errors associated with the rebinned y-values of the
                    two-dimensional data.
   Outputs:

   -J.M.Court, 2015'''
   hinge=((x[1]-x[0])*10**logres)/((10**logres)-1)    # Find the 'hinge' point at which
                                                      # to switch between linear and 
                                                      # logarithmic binning
   lbin=np.log10(x[0])
   xb =10**(np.arange(lbin,np.log10(x[-1]),logres))   # Setting up arrays to append
                                                      # binned values into
   yb =np.zeros(len(xb))
   yeb=np.zeros(len(xb))
   hingel=sum((xb)<=hinge)                            # Getting the ID of the hinge
                                                      # point in the log
   xbl=len(xb)
   for i in range(hingel,xbl):
      lowid=int(((10**((i*logres)+lbin))-x[0])/(x[1]-x[0]))
                                                      # Calculate the ID of the lowest
                                                      # linear bin that corresponds to
                                                      # this log bin
      uppid=int(((10**(((i+1)*logres)+lbin))-x[0])/(x[1]-x[0]))
                                                      # Calculate the ID of the highest
                                                      # linear bin that corresponds to
                                                      # this log bin
      if uppid>lowid:
         yb[i]=np.mean(y[lowid:uppid])
         yeb[i]=(np.sqrt(sum(np.array(ye[lowid:uppid])**2)))/int(uppid-lowid)
      else:
         yb[i]=0                                      # If no data found, error=power=0
         yeb[i]=0
   mask=x<hinge
   lmask=xb>hinge
   xf=np.append(x[mask],xb[lmask])
   yf=np.append(y[mask],yb[lmask])
   yef=np.append(ye[mask],yeb[lmask])
   return xf,yf,yef

#-----LeahyN---------------------------------------------------------------------------

@mjit()
def leahyn(data,counts,datres):
   '''Leahy Normaliser

   Description:
    Takes a raw FFT-algorithm output spectrum and normalises it by the Leahy
    convention.
   Inputs:
    data   - LIST: the spectrum to be normalised.
    counts -  INT: the number of photon events in the data sample for which the
                   spectrum was created.
    datres -  INT: the number of time bins in the data sample for which the spectrum
                   was created.
   Outputs:
    leahy  - LIST: the Leahy-normalised spectrum.

   -J.M.Court, 2015'''
   leahy=2*(abs(data[0:int(datres/2.0)])**2)/counts
   return leahy

#-----Lh2RMS---------------------------------------------------------------------------

@mjit()
def lh2rms(leahy,rate,bg,const):

   '''Leahy 2 RMS Converter
   Description:
    Takes a Leahy-normalised spectrum and re-normalises it by the (RMS/Mean)^2
    convention.
   Inputs:
    leahy -  LIST: the leahy-normalised spectrum to be re-normalised.
    rate  - FLOAT: the source + background count rate (per second) of the data sample
                   from which the spectrum was created.
    bg    - FLOAT: the background count rate (per second) of the data sample from which
                   the spectrum was created.
    const - FLOAT: the average power given in Leahy normalisation for pure white noise.
                   Theoretically const=2, but in practice is slightly lower and varies
                   between telescopes.
   Outputs:
    rms   -  LIST: the (RMS/Mean)^2-normalised spectrum.

   -J.M.Court, 2015'''
   denom=(rate-bg)**2
   if denom==0:
      mult=0.0
   else:
      mult=1.0/denom
   rms= (leahy-const)*(rate)*mult
   return rms

#-----LhConst--------------------------------------------------------------------------

def lhconst(data):
   '''LHS Const

   Decription:
    Finds the normalisation of white noise in a given power Leahy-normalised power
    spectrum.  Assumes no spectral features in the last 20% of the spectrum.
   Inputs:
    data  -  LIST: the data to be converted.
   Outputs:
    const - FLOAT: the normalisation of white noise.

   -J.M.Court, 2015'''
   def leahynoise(x,a):
      return a
   olen=len(data)
   olen=int((4/5.0)*olen)
   datav=data[olen:]
   const=optm.curve_fit(leahynoise,range(len(datav)),datav)
   const=const[0][0]
   if const>2.5 or const<1.5:
      print "WARNING: Leahy constant of "+str(const)+" outside of accepted range!"
   return const

#-----Lomb_Scargle---------------------------------------------------------------------

@mjit()
def lomb_scargle(x,y,ye,freqs):
   '''Lomb Scargle

   Decription:
    Returns the Lomb-Scargle periodogram of data provided by the user, scanning over a
    set of frequencies also provided by the user.
   Inputs:
    x     -  LIST: the time-array for the data to be converted.
    y     -  LIST: the data associated with time array x.
    ye    -  LIST: the errors associated with each point in y.
    freqs -  LIST: the list of discrete frequencies over which the user wants the L-S
                   periodogram to scan.
   Outputs:
    pgram - ARRAY: the Lomb-Scargle power associated with each frequency provided in
            freqs.

   -J.M.Court, 2015'''
   assert len(x)==len(y)
   x=np.array(x)
   y=np.array(y)
   ye=np.array(ye)
   x=x[y>0]                                           # Weed out any negative values
                                                      # here before they break things

   ye=ye[y>0]
   y=y[y>0]
   w=safe_div(np.ones(len(ye)),ye**2)
   y=y-(sum(y*w)/sum(w))
   freqs=np.array(freqs)*2*pi
   wt=np.multiply.outer(x,freqs)
   sin2wt=np.sin(2*wt)
   cos2wt=cosfromsin(2*wt,sin2wt)
   tau=(np.arctan(np.sum(sin2wt,axis=0)/np.sum(cos2wt,axis=0)))/(2*freqs)
   wttau=wt-(freqs*tau)
   sinw=np.sin(wttau)
   cosw=cosfromsin(wttau,sinw)
   yT=np.vstack(y)
   ysin=yT*sinw
   ycos=yT*cosw
   norm=2*np.sum(w*(y**2)/(len(y)-2))
   w=np.vstack(w)
   pgram=((np.sum(w*ycos,axis=0)**2)/np.sum((w*cosw)**2,axis=0)+
          (np.sum(w*ysin,axis=0)**2)/np.sum((w*sinw)**2,axis=0)) / norm
   return pgram

#-----MCErrorCalc----------------------------------------------------------------------

@mjit()
def mcerrorcalc(func,data,data_e,ntrials,verbose=False):
   '''Monte Carlo Error Calculation

   Description:
    For a function which takes a set of 1-dimensional values, and for values with
    associated errors, simulates new datasets from the given datavalues and errors and
    uses these simulations to estimate the error on the output value.
   Inputs:
    func    - FUNCTION: the function to be used.  MUST take arguments of the form
                        (data,data_error).
    data    -    ARRAY: the measured data from which new data is to be simulated.
    data_e  -    ARRAY: the error on the measured data.
    ntrials -      INT: the number of simulations to run.

   -J.M.Court, 2017'''
   vals=[]
   for i in range(ntrials):
      new_data=rn.normal(data,data_e)
      val=(func(new_data,data_e))
      if not np.isnan(val):
         vals.append(val)
      if verbose:
         print ' Trial: '+str(i)+'/'+str(ntrials)
   serr=np.std(vals)
   return serr

#-----MXRebin--------------------------------------------------------------------------

@mjit()
def mxrebin(spcdata,spcerrs,xaxis,good,bfac):
   '''Matrix X-Rebin

   Description:
    Takes 2-Dimensional arrays of data and errors which are linearly binned on the
    x-axis and rebins them by a factor of the user's choosing, returning new data
    array, error array, x-axis and Boolean 'good' array.
   Inputs:
    spcdata   - ARRAY: some 2 dimensional array of values.
    spcerrs   - ARRAY: a 2 dimensional array containing the errors associated with the
                       values in spcdata.  Must have the same dimensions as spcdata.
    xaxis     - ARRAY: an array of values corresponding to the x-values of columns in
                       spcdata.  Must be the same length as a row of spcerrs and must
                       be linearly spaced.
    good      - ARRAY: A Boolean array with as many elements as spcdata has columns.
                       Its entries are True unless the corresponding row in spcdata
                       does not correspond to a valid time within the GTIs of the
                       photon count data.
    bfac      -   INT: the binning factor, i.e. the ratio between new bin size and old
                       bin size.
   Outputs:
    b_spcdata - ARRAY: the rebinned 2-dimensional data array
    b_spcerrs - ARRAY: the rebinned 2-dimensional error array
    b_xaxis   - ARRAY: the rebinned x-axis
    b_good    - ARRAY: the rebinned 'good' array

   -J.M.Court, 2015'''
   spx=len(spcdata[:,0])                              # x-dimension of new matrix
                                                      # matches old matrix
   spy=int(len(spcdata[0,:])/bfac)                    # y-dimension of new matrix
                                                      # equals the y dimension of the
                                                      # old matrix divided by the
                                                      # binning factor
   b_good=np.zeros(spy,dtype=bool)                    # array to label good columns
   b_spcdata=np.zeros([spx,spy])                      # Create the new matrix
   b_spcerrs=np.zeros([spx,spy])                      # Create the new error matrix
   for i in range(spx):                               # For each freq row of fourgr:
      for j in range(spy):                            # For each time col of fourgr:
         celltot=0                                    # Create a running total to put
                                                      # into the matrix cell
         errtot=0                                     # Create a running total to put
                                                      # into the error matrix cell
         for k in range(bfac):                        # Calculate extent of current bin
            b_good[j]=(True)                          # Assume column is good
            if not good[j*bfac+k]:                    # If any of the columns being
                                                      # summed were flagged as bad,
                                                      # flag new column as bad
               celltot=0
               b_good[j]=(False)
               break                                  # Don't attempt to find spectrum
                                                      # in column if any constituent
                                                      # columns are bad
            celltot+=spcdata[i,j*bfac+k]              # Sum all values that fall within
                                                      # the given bin
            errtot+=((spcerrs[i,j*bfac+k])**2)
         b_spcdata[i,j]=celltot/bfac                  # Divide the cell total by the
                                                      # bin multiplier to convert to a
                                                      # mean
         b_spcerrs[i,j]=np.sqrt(errtot)/bfac
   b_xaxis=[]                                         # Calculate new x-axis
   for i in range(int(len(xaxis)/bfac)):
      b_xaxis.append(xaxis[i*bfac])
   return b_spcdata,b_spcerrs,b_xaxis,b_good

#-----Nones----------------------------------------------------------------------------

def nones(shape):
    '''Function to basically do what np.zeros and np.ones do, but creating an array of Nones.
    
    -J.Coxon, 2015'''
    makeNone = np.vectorize(lambda x: None)
    return makeNone(np.zeros(shape))

#-----PDColEx--------------------------------------------------------------------------

@mjit()
def pdcolex2(y1,y2,ye1,ye2,gmask):
   '''Plot Demon Colour Extract (2D)

   Description:
    Takes two equally spaced flux series in the same set of time co-ordinates and
    constructs an array representing the file2 / file1 colour over the same time 
    period.  Also returns the sum of the flux of the two series.
   Inputs:
    y1,y2   - ARRAYS: The first and second flux series respectively
    ye1,ye2 - ARRAYS: The errors of the first and second flux series respectively
    gmask   -  ARRAY: A mask of Boolean values with False at every point outside of a
                      GTI in this time series
   Outputs:
    flux    -  ARRAY: The sum total flux from the two bands
    fluxe   -  ARRAY: The errors of 'flux'
    col21   -  ARRAY: The file2/file1 colour
    col21e  -  ARRAY: The error on "col21"

   -J.M.Court, 2015'''
   warnings.filterwarnings("ignore")                  # Div 0 errors are a real
                                                      # possibility.  This is me
                                                      # ignoring them...
   y=   {}                                            # Prepare libraries for data
   ye=  {}
   col= {}
   cole={}
   y[1]=y1[gmask]                                     # Mask data, store in library
   y[2]=y2[gmask]
   ye[1]=ye1[gmask]
   ye[2]=ye2[gmask]
   flux=y[1]+y[2]                                     # Get total flux
   fluxe=np.sqrt(ye[1]**2+ye[2]**2)                   # Get flux error
   for i in range(1,3):                               # For the ith possible numerator
                                                      # band
      for j in range(1,3):                            # For the jth possible
                                                      # denominator band
         if j!=i:                                     # Prevents taking x/x colour
            ld=int(str(i)+str(j))
            col[ld]=(y[i]/y[j])                       # Fetch colour
            cole[ld]=col[ld]*np.sqrt(((ye[i]/y[i])**2)+((ye[j]/y[j])**2))
                                                      # Fetch colour error
   return flux,fluxe,y,ye,col,cole

@mjit()
def pdcolex3(y1,y2,y3,ye1,ye2,ye3,gmask):
   '''Plot Demon Colour Extract (3D)

   See help for Plot Demon Colour Extract (2D)

   -J.M.Court, 2015'''
   warnings.filterwarnings("ignore")                  # Div 0 errors are a real
                                                      # possibility.  This is me
                                                      # ignoring them...
   y=   {}                                            # Prepare libraries for data
   ye=  {}
   col= {}
   cole={}
   y[1]=y1[gmask]                                     # Mask data, store in library
   y[2]=y2[gmask]
   y[3]=y3[gmask]
   ye[1]=ye1[gmask]
   ye[2]=ye2[gmask]
   ye[3]=ye3[gmask]
   flux=y[1]+y[2]+y[3]                                # Get total flux
   fluxe=np.sqrt(ye[1]**2+ye[2]**2+ye[3]**2)          # Get flux error
   for i in range(1,4):                               # For the ith possible numerator
                                                      # band
      for j in range(1,4):                            # For the jth possible
                                                      # denominator band
         if j!=i:                                     # Prevents taking x/x colour 
            ld=int(str(i)+str(j))
            col[ld]=(y[i]/y[j])                       # Fetch colour
            cole[ld]=col[ld]*np.sqrt(((ye[i]/y[i])**2)+((ye[j]/y[j])**2)) 
                                                      # Fetch colour error
   return flux,fluxe,y,ye,col,cole

#-----PDload---------------------------------------------------------------------------

def pdload(filename,isplotd):
   '''PlotDemon loader

   Description:
    Calls plotdld or csvload depending on the type of file to be opened.
   Inputs:
    filename - STRING: The absolute or relative path to the location of the file that
                       will be opened.
    isplotd  - BOOL  : Whether the file to be opened has been identified as a .plotd
                       file
   Outputs:
    times    -      ARRAY: An array, the elements of which are the left-hand edges of
                           the time bins into which counts have been binned.  Units of
                           seconds.
    counts   -      ARRAY: The number of counts in each bin defined in 'times'.
    errors   -      ARRAY: The 1-sigma errors associated with each value in 'counts'
    binsize  -      FLOAT: The size of each bin in 'times'.  Saved for speed upon
                           loading.
    gti      - FITS TABLE: The table of GTI values from the event data .fits file.
    mxpcus   -        INT: The maximum number of PCUs active at any one time during the
                           observation.
    bgpcu    -      FLOAT: An estimate of the count rate of the background flux during
                           the full observation, in counts per second per PCU,
                           multiplied by the number of PCUs.
    flavour  -     STRING: A useful bit of text to put on plots to help identify them
                           later on.
    chanstr  -     STRING: A string containing the high and low channel numbers
                           separated by a dash.
    mission  -     STRING: The name of the satellite
    obsdata  -      TUPLE: The first element is the name of the object, the second is
                           the observation ID.
    version  -     STRING: The Version of FITSGenie in which the file was created
    
   -J.M.Court, 2015'''
   if isplotd:
       return plotdld(filename)
   else:
       return csvload(filename)

#-----PlotdLd--------------------------------------------------------------------------

def plotdld(filename):
   '''.Plotd Load

   Description:
    Opens a .plotd file and retrieves the useful data from it.
   Inputs:
    filename - STRING: The absolute or relative path to the location of the file that
                       will be opened.
   Outputs:
    times    -      ARRAY: An array, the elements of which are the left-hand edges of
                           the time bins into which counts have been binned.  Units of
                           seconds.
    counts   -      ARRAY: The number of counts in each bin defined in 'times'.
    errors   -      ARRAY: The 1-sigma errors associated with each value in 'counts'
    binsize  -      FLOAT: The size of each bin in 'times'.  Saved for speed upon
                           loading.
    gti      - FITS TABLE: The table of GTI values from the event data .fits file.
    mxpcus   -        INT: The maximum number of PCUs active at any one time during the
                           observation.
    bgpcu    -      FLOAT: An estimate of the count rate of the background flux during
                           the full observation, in counts per second per PCU,
                           multiplied by the number of PCUs.
    flavour  -     STRING: A useful bit of text to put on plots to help identify them
                           later on.
    chanstr  -     STRING: A string containing the high and low channel numbers
                           separated by a dash.
    mission  -     STRING: The name of the satellite
    obsdata  -      TUPLE: The first element is the name of the object, the second is
                           the observation ID.
    version  -     STRING: The Version of FITSGenie in which the file was created

   -J.M.Court, 2015'''
   try:
      readfile=open(filename,'rb')                    # Open the .plotd file
   except:
      print ''
      print 'File '+filename+' not found!  Aborting!'
      signoff()
      exit()
   data=pickle.load(readfile)                         # Unpickle the .plotd file
   times=np.array(data['time'])                       # Unleash the beast! [extract the
                                                      # file]
   rates=np.array(data['rate'])
   errors=np.array(data['errs'])
   tstart=data['tstr']
   binsize=data['bsiz']
   gti=data['gtis']
   mxpcus=data['pcus']
   bgest=data['bkgr']
   bgsub=data['bsub']
   bgdata=data['bdat']
   flavour=data['flav']
   chanstr=data['chan']
   mission=data['miss']
   obsdata=data['obsd']
   version=data['vers']
   bgpcu=bgest*mxpcus                                 # Collect background * PCUs
   readfile.close()
   return times,rates,errors,tstart,binsize,gti,mxpcus,bgpcu,bgsub,bgdata,flavour,
                chanstr,mission,obsdata,version

#-----csvLoad--------------------------------------------------------------------------

def linesplit(lx,iscomma):
   if iscomma:
      return lx.split(',')
   return lx.split()

def csvload(filename):
   '''.csv Loader
   
   Description:
    Extracts information from a csv file.  Column delimiters automatically set using
    python string object's .split() function.  Assumes the file is of format
    'time,rate' if two columns, 'time,rate,rate_error' if three columns of 'time,
    time_error,rate,rate_error; if four or more columns.
   Inputs:
    filename - STRING: The absolute or relative path to the location of the file that
                       will be opened.
   Outputs:
    times    -      ARRAY: An array, the elements of which are the left-hand edges of
                           the time bins into which counts have been binned.  Units of
                           seconds.
    counts   -      ARRAY: The number of counts in each bin defined in 'times'.
    errors   -      ARRAY: The 1-sigma errors associated with each value in 'counts'
    binsize  -      FLOAT: The size of each bin in 'times'.  Saved for speed upon
                           loading.
    gti      - FITS TABLE: The table of GTI values from the event data .fits file.
    mxpcus   -        INT: The maximum number of PCUs active at any one time during the
                           observation.
    bgpcu    -      FLOAT: An estimate of the count rate of the background flux during
                           the full observation, in counts per second per PCU,
                           multiplied by the number of PCUs.
    flavour  -     STRING: A useful bit of text to put on plots to help identify them
                           later on.
    chanstr  -     STRING: A string containing the high and low channel numbers
                           separated by a dash.
    mission  -     STRING: The name of the satellite
    obsdata  -      TUPLE: The first element is the name of the object, the second is
                           the observation ID.
    version  -     STRING: The Version of FITSGenie in which the file was created

   -J.M.Court, 2015'''
   f=open(filename,'r')
   times=[]
   rates=[]
   errors=[]
   got_firstline=False
   haserrs=False
   xind=0
   for line in f:
       if not got_firstline:
          if ((len(line.split())<2) and (len(line.split(','))<2)):
                                                      # Assume lines with <2 columns,
                                                      # or a non-number in col zero are
                                                      # part of the header, skip 'em
             continue
          if ',' in line:
             iscomma=True
          else:
             iscomma=False
          l=linesplit(line,iscomma)
          try:
             float(l[0].strip('"'))
          except:
             continue
          llen=len(l)
          if llen<3:                                  # Interpret csv data depending on
                                                      # how many columns
             yind=1
             haserrs=False
          elif llen<4:
             yind=1
             haserrs=True
             eind=2
          elif l[0][0]=='"':
             yind=1
             haserrs=True
             eind=2
          else:
             yind=2
             haserrs=True
             eind=3
          got_firstline=True
       l=linesplit(line,iscomma)
       times.append(float(l[xind].strip('\n').strip('"')))
       rates.append(float(l[yind].strip('\n').strip('"')))
       if haserrs:
          errors.append(float(l[eind].strip('\n').strip('"')))
   if not haserrs:
       errors=np.sqrt(np.array(rates))                # Assume root flux errors if none
   times=np.array(times)
   rates=np.array(rates)
   errors=np.array(errors)
   tstart=times[0]
   times=times-times[0]
   binsize=times[1]-times[0]
   gti=None
   mxpcus=1
   bgpcu=1
   bgsub=0
   bgdata=None
   flavour='csv'
   chanstr='Unknown'
   mission='Unknown'
   obsdata=['Unknown','Unknown']
   version='From csv'
   return times,rates,errors,tstart,binsize,gti,mxpcus,bgpcu,bgsub,bgdata,flavour,
                chanstr,mission,obsdata,version

#-----PlotdSv--------------------------------------------------------------------------

@mjit()
def plotdsv(filename,times,rates,errors,tstart,binsize,gti,mxpcus,bgest,bgsub,bgdata,
            flavour,chanstr,mission,obsdata,version):
   '''.Plotd Save

   Description:
    Takes the input of the data products required to create a .plotd file (to read with
    plotdemon) and creates a .plotd file at a location given as the first input.
   Inputs:
    filename -     STRING: The absolute or relative path to the location of the file
                           that will be created.
    times    -      ARRAY: An array, the elements of which are the left-hand edges of
                           the time bins into which counts have been binned.  Units of
                           seconds.
    counts   -      ARRAY: The number of counts in each bin defined in 'times'.
    binsize  -      FLOAT: The size of each bin in 'times'.  Saved for speed upon
                           loading.
    gti      - FITS TABLE: The table of GTI values from the event data .fits file.
    mxpcus   -        INT: The maximum number of PCUs active at any one time during the
                           observation.
    bgest    -      FLOAT: An estimate of the count rate of the background flux during
                           the full observation, in counts per second per PCU.
    flavour  -     STRING: A useful bit of text to put on plots to help identify them
                           later on.
    chanstr  -     STRING: A string containing the high and low channel numbers
                           separated by a dash.
    mission  -     STRING: The name of the satellite
    obsdata  -      TUPLE: The first element is the name of the object, the second is
                           the observation ID.
    version  -     STRING: The Version of FITSGenie in which the file was created
   Outputs:
    [none]

   -J.M.Court, 2015'''
   savedata={}                                        # Open library object to save in
                                                      # file
   savedata['time']=times                             # Dump each piece of data into an
                                                      # appropriate library element
   savedata['rate']=np.array(rates)
   savedata['errs']=np.array(errors)
   savedata['tstr']=tstart
   savedata['bsiz']=binsize
   savedata['gtis']=gti
   savedata['pcus']=mxpcus
   savedata['bkgr']=bgest
   savedata['flav']=flavour
   savedata['chan']=chanstr
   savedata['miss']=mission
   savedata['bsub']=bgsub
   savedata['bdat']=bgdata
   savedata['obsd']=obsdata
   savedata['vers']=version
   filename=uniqfname(filename,'plotd')               # Get the next available name of
                                                      # form filename(x).plotd
   wfile = open(filename, 'wb')                       # Open file to write to
   pickle.dump(savedata,wfile)                        # Pickle the data (convert into 
                                                      # bitstream) and dump to file
   wfile.close()                                      # Close file
   return filename

#-----RMS_N----------------------------------------------------------------------------

@mjit()
def rms_n(data,counts,datres,rate,bg,const):
   '''RMS Normaliser

   Description:
    Takes a raw FFT-algorithm output spectrum and normalises it by the (RMS/Mean)^2
    convention.
   Inputs:
    data   -  LIST: the spectrum to be normalised.
    counts -   INT: the number of photon events in the data sample for which the
                    spectrum was created.
    datres -   INT: the number of time bins in the data sample for which the spectrum
                    was created.
    rate   - FLOAT: the source + background count rate (per second) of the data sample
                    from which the
                    spectrum was created.
    bg     - FLOAT: the background count rate (per second) of the data sample from
                    which the spectrum was created.
    const  - FLOAT: the average power given in Leahy normalisation for pure white
                    noise.  Theoretically const=2, but in practice is slightly lower
                    and varies between telescopes.
   Outputs:
    rms    -  LIST: the Leahy-normalised spectrum.

   -J.M.Court, 2015'''
   leahy=leahyn(data,counts,datres)                   # Leahy normalise the data
   rms=lh2rms(leahy,rate,bg,const)                    # Convert to RMS
   return rms

#-----RMS------------------------------------------------------------------------------

#@mjit()
def rms(data,data_err=None,with_err=False):
   '''RMS

   Description:
    Returns the fractional RMS of a 1-dimensional data set
   Inputs:
    data     - LIST: The data to find the RMS of.
    data_err - LIST: The errors on those data points
   Outputs:
    rms  - FLOAT: The rms of the data

   -J.M.Court, 2015'''
   if type(data_err)==type(None):
      data_err=np.array([0]*len(data))
   if np.mean(data)==0:
      return 'div0'
   nr=1.0/len(data)
   mn=np.mean(data)
   vd=np.sum((data-mn)**2)-np.sum((data_err)**2)
   orms=((vd*nr)**0.5)/abs(mn)

   if not with_err:
      return orms
   else:
      rmserr=mcerrorcalc(rms,data,data_err,100,verbose=False)
      return orms,rmserr

#-----Safe_Div-------------------------------------------------------------------------

@mjit()
def safe_div(x,y):
   '''Safe Div

   Description:
    Divides the first inputs by the second inputs if the latter is nonzero.  If an
    element of the second input is zero, the corresponding element in the output is
    zero.
   Inputs:
    x - ARRAY: The numerator
    y - ARRAY: The denominator
   Outputs:
    r - ARRAY: The result of division, or zero

   -J.M.Court, 2015'''
   r=np.zeros(len(y))
   r[y!=0]=x[y!=0]/y[y!=0]
   return r

#-----SignOff--------------------------------------------------------------------------

def signoff():
   '''Sign Off

   Description:
    Prints an underline with some spaces.  That's all.

   -J.M.Court, 2015'''
   print ''
   print '------------------------------------------------'
   print ''

#-----sinfromcos-----------------------------------------------------------------------

@mjit()
def sinfromcos(x,cosx):
   '''Sine from Cosine

   Description:
    Returns the sine of an array of values when also given their cosines.  Using this
    function is faster than using sine if the cosine values are already stored.
   Inputs:
    x    - ARRAY: the array of values to calculate the sine of.
    cosx - ARRAY: the cosines array of values to calculate the sines of.
   Outputs:
    sinx - ARRAY: the sines of x.

   -J.M.Court, 2015'''
   sinx=np.absolute((1-cosx**2)**0.5)
   signx=np.sign(((x+pi)%(2*pi))-pi)
   return sinx*signx

@mjit()
def cosfromsin(x,sinx):
   '''Cosine from Sine

   Description:
    Returns the cosine of an array of values when also given their sines.  Using this
    function is faster than using cosine if the sine values are already stored.
   Inputs:
    x    - ARRAY: the array of values to calculate the cosine of.
    sinx - ARRAY: the sines array of values to calculate the cosines of.
   Outputs:
    cosx - ARRAY: the sines of x.

   -J.M.Court, 2015'''
   cosx=np.absolute((1-sinx**2)**0.5)
   signx=np.sign(((x-pi/2)%(2*pi))-pi)
   return cosx*signx

#-----SLPlot---------------------------------------------------------------------------

def slplot(x,y,ye,xlabel,ylabel,title,figid="",typ='both',errors=True):
   '''Standard/Log Plotter

   Description:
    Takes a two-dimensional array of data and plots it twice on the same figure; once
    on standard linear axes, and once on logarithmic axes.
   Inputs:
    x      -   LIST: The x-values of the two-dimensional data.
    y      -   LIST: The y-values of the two-dimensional data, must be the same length
                     as x.
    xlabel - STRING: The title of the x-axis on the graphs.
    ylabel - STRING: The title of the y-axis on the graphs.
    title  - STRING: The title of the figure.
    figid  - STRING: Gives the Figure a name such that it can be easily manipulated or
                     closed outside of this function.
    typ    - STRING: User can input 'log' or 'lin' to only display plot of that type.
    errors -   BOOL: True or False: whether to display errorbars on plots
   Outputs:
    filename - STRING: The filename actually used when saving

   -J.M.Court, 2015'''
   pl.close(figid)                                    # Close any previous plot of this
                                                      # type
   pl.figure(figid)                                   # Create spectrum plot
   if typ in ('lin','both'):
      if typ=='lin':
         pl.subplot(111)                              # If 'lin' passed as typ word,
                                                      # only create one subplot
      else:
         pl.subplot(211)                              # If 'both' passed as typ word, 
                                                      # make 1st of 2 subplots
      pl.grid(True,which="both")
      pl.xlabel(xlabel)
      pl.ylabel(ylabel)
      pl.title(title)
      if errors:
         pl.errorbar(x,y,ye)                          # Plot data
      else:
         pl.plot(x,y)
      pl.plot([x[0],x[-1]],[0,0])
   if typ in ('log','both'):
      if typ=='log':
         ax=pl.subplot(111)                           # If 'log' passed as typ word,
                                                      # only create one subplot
      else:
         ax=pl.subplot(212)                           # If 'both' passed as typ word, 
                                                      # make 2nd of 2 subplots
      pl.xlabel(xlabel)
      pl.ylabel(ylabel)
      pl.title(title)
      if errors:
         pl.errorbar(x,abs(y),ye,fmt='k')             # Plot data
      else:
         pl.plot(x,abs(y),'k')                        # Plot log-log data
      ax.set_xscale('log')
      ax.set_yscale('log')
      pl.grid(True,which="both")
   #if typ in ('lin','log','both'):
   #   pl.show(block=False)                           # Show both plots together
   else:
      print 'Invalid typ!  No plot shown.'            # Complain if none of 'lin',
                                                      # 'log' or 'both are given as typ
                                                      # word

#-----Spliner--------------------------------------------------------------------------

@mjit()
def spliner(data,errors=None):
   '''Spliner

   Description:
    Smooths evenly-sampled data using a first-order Univariate spline algorithm
   Inputs:
    data        - ARRAY: The data to be smoothed
    errors      - ARRAY: [Optional] The errors associated with the errors
   Outputs:
    smooth_data - ARRAY: The smoothed data values
    
   - J.M.C.Court,2016'''
   if type(errors)==type(None):
      errors=np.ones(len(data))                       # If no errors given, do not
                                                      # weight points
   else:
      assert len(errors)==len(data)
   spline=intp.UnivariatrSpline(range(len(data)),data,w=errors**-1,k=2)   
                                                      # Construct a univariate spline
                                                      # function from the data
   #pl.figure()                                       # Uncomment these 4 lines to
                                                      # allow for display of smoothed
                                                      # and pre-smoothed data
   #pl.plot(data_keys,data,'0.5')
   smooth_data=spline(range(len(data)))               # Reconstruct the data using the
                                                      # spline
   #pl.plot(data_keys,data,'k')
   #pl.show(block=True)
   return smooth_data

#-----SpecaLd--------------------------------------------------------------------------

def specald(filename):
   '''.Speca Load

   Description:
    Opens a .speca file and retrieves the useful data from it.
   Inputs:
    filename - STRING: The absolute or relative path to the location of the file that
                       will be opened.
   Outputs
    spcdata  -  ARRAY: An array, the elements of which are the non-normalised power
                       spectra taken over different equal-length time intervals of the
                       photon count data being considered.
    good     -  ARRAY: A Boolean array with as many elements as spcdata has columns.
                       Its entries are True unless the corresponding row in spcdata 
                       does not correspond to a valid time within the GTIs of the
                       photon count data.
    rates    -  ARRAY: The count rate per second of photons in the time interval
                       represented by the corresponding row of spcdata.
    prates   -  ARRAY: An array of floats with as many elements as spcdata has columns.
                       Denotes the highest number of total counts in each timing window
                       when the counts are binned on a time of binsize * binfac
    trates   -  ARRAY: An array of floats with as many elements as spcdata has columns.
                       Denotes the lowest number of total counts in each timing window
                       when the counts are binned on a time of binsize * binfac
    phcts    -    INT: The total number of photons detected, overall.
    bg       -  ARRAY: An estimate of the count rate of the background flux during the
                       full observation, in counts per second per PCU, multiplied by
                       the number of PCUs active in the corresponding row of specdata.
    binsize  -  FLOAT: The size in seconds of the time bins used when converting event
                       data into time binned photon count data.
    foures   -  FLOAT: The length of time corresponding to a single row of spcdata, in
                       seconds.
    bgest    -  FLOAT: An estimate of the count rate of the background flux during the
                       full observation, in counts per second per PCU.
    flavour  - STRING: A useful bit of text to put on plots to help identify them later
                       on.
    cs       - STRING: A string containing the high and low channel numbers separated
                       by a dash.
    mission  - STRING: The name of the satellite
    obsdata  -  TUPLE: The first element is the name of the object, the second is the
                       observation ID.
    wtype    - STRING: A string denoting the functional geometry of the windows used
                       for Fourier analysis.
    slide    -  FLOAT: The time separation of the start times of windows represented
                       by two consecutive rows of spcdata, in seconds
    binfac   -    INT: See trates or prates
    version  - STRING: The Version of FITSGenie in which the file was created

   -J.M.Court, 2015'''
   try:
      readfile=open(filename,'rb')                    # Open the .speca file
   except:
      print ''
      print 'File not found!  Aborting!'
      signoff()
      exit()
   data=pickle.load(readfile)                         # Unpickle the .speca file
   spcdata=data['data']                               # Unleash the beast! [extract the
                                                      # file]
   good=np.array(data['good'])
   rates=np.array(data['rate'])
   prates=np.array(data['prts'])
   trates=np.array(data['trts'])
   phcts=data['phct']
   n_pcus=np.array(data['npcu'])
   binsize=data['bsiz']
   bgest=data['bkgr']
   foures=data['fres']
   flavour=data['flav']
   cs=data['chan']
   mission=data['miss']
   obsdata=data['obsd']
   wtype=data['wndw']
   slide=data['slid']
   binfac=data['sbin']
   version=data['vers']
   readfile.close()
   print ''
   print 'Power spectra taken over '+str(foures)+'s of data each'
   print str(sum(good))+'/'+str(len(spcdata))+' power spectra are good'
   if flavour!='':
      print "Flavour is '"+str(flavour)+"'"           # Only print flavour if there is
                                                      # a flavour to print
   bg=n_pcus*bgest
   return spcdata,good,rates,prates,trates,phcts,bg,binsize,foures,bgest,flavour,cs,
                  mission,obsdata,wtype,slide,binfac,version

#-----SpecaSv--------------------------------------------------------------------------

@mjit()
def specasv(filename,spcdata,good,rates,prates,trates,phcts,npcus,binsize,bgest,foures,
            flavour,cs,mission,obsdata,wtype,slide,spcbinfac,version):
   '''.Speca Save

   "Should've gone to SpecaSaver..."

   Description:
    Takes the input of the data products required to create a .speca file (to read with
    specangel) and creates a .speca file at a location given as the first input.
   Inputs:
    filename  - STRING: The absolute or relative path to the location of the file that
                        will be created.
    spcdata   -  ARRAY: An array, the elements of which are the non-normalised power
                        spectra taken over different equal-length time intervals of the
                        photon count data being considered.
    good      -  ARRAY: A Boolean array with as many elements as spcdata has columns.
                        Its entries are True unless the corresponding row in spcdata
                        does not correspond to a valid time within the GTIs of the 
                        photon count data.
    rates     -  ARRAY: An array of floats with as many elements as spcdata has
                        columns.  Denotes the total number of photon counts in the time
                        interval represented by the corresponding row in spcdata,
                        divided by the width of a column in seconds.
    prates    -  ARRAY: An array of floats with as many elements as spcdata has
                        columns.  Denotes the highest number of total counts in each
                        timing window when the counts are binned on a time of
                        binsize * spcbinfac
    trates    -  ARRAY: An array of floats with as many elements as spcdata has
                        columns.  Denotes the lowest number of total counts in each
                        timing window when the counts are binned on a time of
                        binsize * spcbinfac
    phcts     -    INT: The total number of photons detected, overall.
    npcus     -  ARRAY: An array of ints with as many elements as spcdata has columns.
                        States the number of detectors that were active when the data
                        represented by the corresponding row of spcdata was recorded.
    binsize   -  FLOAT: The size in seconds of the time bins used when converting event
                        data into time binned photon count data.
    bgest     -  FLOAT: An estimate of the count rate of the background flux during the
                        full observation, in counts per second per PCU.
    foures    -  FLOAT: The length of time corresponding to a single row of spcdata, in
                        seconds.
    flavour   - STRING: A useful bit of text to put on plots to help identify them
                        later on.
    cs        - STRING: A string containing the high and low channel numbers separated
                        by a dash.
    mission   - STRING: The name of the satellite
    obsdata   -  TUPLE: The first element is the name of the object, the second is the
                        observation ID.
    wtype     - STRING: A string denoting the functional geometry of the windows used
                        for Fourier analysis.
    slide     -  FLOAT: The time separation of the start times of windows represented
                        by two consecutive rows of spcdata, in seconds
    spcbinfac -    INT: See trates or prates
    version   - STRING: The Version of FITSGenie in which the file was created
   Outputs:
    filename - STRING: The filename actually used when saving

   -J.M.Court, 2015'''
   savedata={}                                        # Open library object to save in
                                                      # file
   savedata['data']=spcdata                           # Dump each piece of data into an
                                                      # appropriate library element
   savedata['good']=good
   savedata['rate']=rates
   savedata['prts']=prates
   savedata['trts']=trates
   savedata['phct']=phcts
   savedata['npcu']=npcus
   savedata['bsiz']=binsize
   savedata['bkgr']=bgest
   savedata['fres']=foures
   savedata['flav']=flavour
   savedata['chan']=cs
   savedata['miss']=mission
   savedata['obsd']=obsdata
   savedata['wndw']=wtype
   savedata['slid']=slide
   savedata['sbin']=spcbinfac
   savedata['vers']=version
   filename=uniqfname(filename,'speca')               # Get the next available name of
                                                      # form filename(x).speca
   wfile = open(filename, 'wb')                       # Open file to write to
   pickle.dump(savedata,wfile)                        # Pickle the data (convert into 
                                                      # bitstream) and dump to file
   wfile.close()                                      # Close file
   return filename

#-----SRinR----------------------------------------------------------------------------

def srinr(t,binning,domain,minv=None,maxv=None):
   '''Subrange in Range: Subrange Creator

   Description:
    Takes an array of ordered values and asks the user to select a subrange.  Converts
    the users entries (in the same units as the data) into data IDs and checks that
    they are valid and within the array.
   Inputs:
    t       -   LIST: An evenly spaced list of values.
    binning -   LIST: The spacing of the list of values.
    domain  - STRING: The name of the physical quantity represented by the data in t.
   Outputs:
    new_mn  -    INT: The array index of the subrange minimum.
    new_mx  -    INT: The array index of the subrange maximum.
    boolv   -   BOOL: A flag to denote whether range was clipped.

   -J.M.Court, 2015'''
   old_mn=0
   old_mx=len(t)
   try:
      if minv==None:
         new_mn=float(raw_input('Minimum '+domain+': '))
                                                      # Fetch new min value from user
      else:
         new_mn=minv
      new_mn=len(t[t<new_mn]) 
   except:
      new_mn=old_mn                                   # Treat garbage input as 'no
                                                      # change'
   try:
      if maxv==None:
         new_mx=float(raw_input('Maximum '+domain+': '))
                                                      # Fetch new max value from user
      else:
         new_mx=maxv
      new_mx=len(t[t<=new_mx])-1      
   except:
      new_mx=old_mx                                   # Treat garbage input as 'no
                                                      # change'
   if new_mn>=new_mx:
      print 'Invalid clipping!  Resetting to full.'
      boolv=False                                     # Set the flag to propagate the 
                                                      # fact no clipping took place
      new_mn=old_mn
      new_mx=old_mx
   else:
      boolv=True
   return new_mn,new_mx,boolv

#-----TNorm----------------------------------------------------------------------------

def tnorm(t,res):
   '''Time-Normer

   Description:
    Takes an array of evenly-spaced time values and normalises them by subtracting the
    lowest value from each.  Then forces each time to equal an integer multiple of the
    pre-normalising resolution to deal with computational inaccuracies in the
    subtraction process.
   Inputs:
    t   -  LIST: The list of evenly spaced time-values.  They need not be ordered, but
                 the lowest value must be first.
    res - FLOAT: The resolution of the list t; entered by hand again to negate
                 computational error.
   Outputs:
    nt  -  LIST: The normalised time values.

   -J.M.Court, 2014'''
   t=np.array(t)
   sttime=t[0]
   for i in range(len(t)):
      t[i]=res*np.floor((t[i]-sttime)/float(res))     # Rescaling time to start at 0, 
                                                      # rounding to deal with errors
                                                      # incurred by subtraction of
                                                      # large numbers
   return t

#-----TokenLoc-------------------------------------------------------------------------

def tokenloc(enigma,token):
   envals=[]
   entoks=[]
   for i in range(len(enigma)):
      if enigma[i][0]==',':
         et=enigma[i][1:]                             # Extract the tokens and how many
                                                      # bytesare allocated to each in
                                                      # the datawords
      else:
         et=enigma[i]
      entoks.append(et[0])
      if et[0]=='C':
         tk=et.split('[')[1].split(']')[0].split(',')
         envals.append(int(enigma[i].split('{')[1]))
   if not (token in entoks):
      return None,None
   cloc=entoks.index(token)                           # Fetch the 'channels' token
   r1=sum(envals[:cloc])
   r2=sum(envals[cloc+1:])
   return tk,(r1,r2)

#-----UniqFName------------------------------------------------------------------------

def uniqfname(filename,extension):
   '''Unique Filename Generator

   Description:
    When given the path to a file, ascertains whether a file already exists in that
    location.  If it does, this function finds the lowest value of n such that
    'filename(n).extension' is unique and returns this new name.  If the file does not
    exist, returns 'filename.extension'.
   Inputs:
    filename  - STRING: The full path to the proposed file location, excluding the
                        extension.
    extension - STRING: The extension of the proposed file location, excluding the
                        leading '.'.
   Outputs:
    uniqname  - STRING: A string containing the best available unique filename.

   -J.M.Court, 2015'''
   filenamex=filename
   n=1
   while os.path.isfile(filenamex+'.'+extension):
      # print filename+'.'+extension+' already exists!# Can uncomment this for more
                                                      # verbosity.
      filenamex=filename+'('+str(n)+')'
      n+=1
   uniqname=filenamex+'.'+extension
   return uniqname

#-----VCRebin--------------------------------------------------------------------------

@mjit()
def vcrebin(vecdata,bfac):
   '''Vector X-Rebin

   Description:
    Takes 1-Dimensional array of data and which is linearly binned and rebins it by a
    factor of the user's choosing, returning new data array.
   Inputs:
    vecdata   - ARRAY: some 2 dimensional array of values.
   Outputs:
    b_vecdata - ARRAY: the rebinned 2-dimensional data array

   -J.M.Court, 2015'''
   bfac=int(bfac)
   spx=len(vecdata)                                   # x-dimension of new vector
   b_vecdata=np.zeros(spx)                            # Create the new vector
   for i in range(spx):                               # For each freq row of fourgr:
      celltot=0                                       # Create a running total to
                                                      # deposit into the vector cell
      for k in range(bfac):                           # Calculate extent of current bin
         celltot+=vecdata[i*bfac+k]                   # Sum all values that fall within
                                                      # the given bin
      b_vecdata[i]=celltot/bfac                       # Divide the cell total by the
                                                      # bin multiplier to convert to a
                                                      # mean
   return b_vecdata

#-----XtrFilLoc------------------------------------------------------------------------

@mjit()
def xtrfilloc(filepath):

   '''Extract File Location
   Description:
    Given the relative path to a file, extracts the file name and it's location.
   Inputs:
    filepath - STRING: The path to a file
   Outputs:
    filename - STRING: The name of the file
    fileloca - STRING: The location of the file

   -J.M.Court, 2015'''
   filename=(filepath.split('/')[-1])                 # Identify filename without
                                                      # directory
   if filename!=filepath:
      fileloca=filepath[:-len(filename)]              # Fetch directory
   else:
      fileloca=os.getcwd()

   if fileloca[0]!='/':
      fileloca=os.getcwd()+fileloca[1:]               # Dump current directory onto the
                                                      # front to make this absolute
   return filename,fileloca
\end{minted}

\section{XTE-PAN Lib}

\begin{minted}[fontsize=\scriptsize]{python}
#! /usr/bin/env python

# |----------------------------------------------------------------------|
# |-----------------------------XTEPAN_LIB-------------------------------|
# |----------------------------------------------------------------------|

# 
# A selection of useful XTE-specific functions which are placed here to reduce clutter
# in the other files of PANTHEON.  Supports GoodXenon_2s and E_125us_64M_0_1s
#  DATAMODEs.

# 
#
# Contents:
#
#  CHRANGE   - clips a set of event data given to it to screen out photons outside of 
#              some energy range given by the user.
#
#  DISCNEV   - Discards non-photon events from a table of data
# 
#  BIHCHAN   - converts an RXTE channel ID into a channel range ID.
#
#  GETBIN    - gets the binning time of a FITS data table
#
#  GETDAT    - gets the data of a FITS data table
#
#  GETGTI    - gets the GTIs associated with a FITS data table 
#
#  GETINI    - gets the start time of the observation associated with a FITS data
#              table
#
#  GETPCU    - gets the number of PCUs that contributed events to a FITS data table
#
#  MAXEN     - returns the highest energy or channel valid for the instrument.

# Modes supported:
# 'E'
# 'B''
# 'SB'
# 'GoodXenon' 

#-----Importing Modules----------------------------------------------------------------

import pan_lib as pan
from numpy import array, zeros, arange
from numpy import sum as npsum

#-----ChRange--------------------------------------------------------------------------

def chrange(data,low,high,header):
   '''Channel Ranger
   
   Description:
    Takes raw event or GoodXenon data from PCA on RXTE and removes all photons outside
    of an energy channel range given by the user.
   Inputs:
    data     - FITS DATA: the event data; raw photon arrival times and data words.
    low      -       INT: the lowest channel the user wants to consider.
    high     -       INT: the highest channel the user wants to consider.
    datamode -    STRING: the datamode in which the data was taken.
   Outputs:
    ch_data  - FITS DATA: the same as data, but with all photons outside of the given
                          channel range removed.

   -J.M.Court, 2015'''
   datamode=header['DATAMODE']
   if datamode[0]=='B':
      chconv=header['TDDES2'].split('&')[2].split('[')[1].split(']')[0].split(',')
      low=bihchan(datamode,low,chconv)
      high=bihchan(datamode,high,chconv)+1
      data=data[:,low:high,:]
      data=npsum(data,axis=1)
      data=data.reshape([len(data)*len(data[0])])
      return data
   elif datamode[:2]=='SB':
      print 'No energy information in this datamode!' # No energy information in SB
                                                      # datamodes, so don't filter
      return data
   else:
      words=data.field(1)
      if low<=0 and high>=255:
         return data                                  # Don't bother searching through
                                                      # if the user wants full range
      if datamode[:10]=='GoodXenon_':
         r=(17,25)                                    # GoodXenon data contains the
                                                      # channels as written, no need to
                                                      # convert
      elif datamode[:2]=='E_':
         enigma=header['TEVTB2'].split('^')[0][1:-1].split('}')[:-1]
                                                      # Fetch (and decode) the
                                                      # enigmatic TEVTB2 word
         tk,r=pan.tokenloc(enigma,'C')
         if tk==None:
            print 'No energy information in this datamode!'   # If no channels token is
                                                      # present, can't filter on energy
            return data
         low=bihchan(datamode,low,tk)
         high=bihchan(datamode,high,tk)+1
      else:
         print datamode,'not yet supported, using full range!'
         return data
      words=array(pan.boolval((words[:,r[0]:r[1]]).tolist()))
      mask1=(words>=low)
      mask2=(words<=high)
      ch_data=data[mask1&mask2]
      return ch_data

#-----DiscNEv--------------------------------------------------------------------------

def discnev(datas,datamode):
   '''Discard Non-Events
   
   Decription:
    Given a FITS data table, discards all non-photon events from the table.
   -J.M.Court, 2014'''
   if datamode[:2] in ['B_','SB']:
      return discnevb(datamode,datas.field(1))
   mask=datas['Event'][:,0]==True                     # Creating a mask to obscure any
                                                      # data not labelled as photons
   datas=datas[mask]                                  # Applying the mask
   return datas

#-----DiscNEvB-------------------------------------------------------------------------

def discnevb(datamode,datas):

   '''Discard Non-Events: Binned Data Version
   Decription:
    Given a FITS binned data table, discards all non-photon events from the table.

   -J.M.Court, 2014'''
   if datamode[:2]=='SB':
      datas=datas.reshape([len(datas)*len(datas[0])])
      return datas
   return datas

#-----BiHChan--------------------------------------------------------------------------

def bihchan(datamode,chan,chanconv):
   '''Channel-Get
   
   Description:
    Converts a channel number into the ID of the range which contains that channel in
    B_2ms_4B_0_35_H or B_8ms_16A_0_35_H data from PCA on RXTE.
   Inputs:
    chan   - INT: the real channel number for PCA data from RXTE.
   Outputs:
    n_chan - INT: the ID of the range containing the relevant channel.
    
   -J.M.Court, 2015'''
   n_nchan=len(chanconv)
   chan=int(chan)
   t_chan=[0]*n_nchan
   for i in range(len(chanconv)):
      t_chan[i]=map(int,chanconv[i].split('~'))
   if chan>t_chan[-1][-1]:                            # Simple sanity check to prevent
                                                      # messy accidents
      print 'This data type does not store photons above Channel '+str(t_chan[-1][-1])+'!'
      pan.signoff()
      exit()
   elif chan<t_chan[0][0]:
      print 'This data type does not store photons below Channel '+str(t_chan[0][0])+'!'
      pan.signoff()
      exit()
   for i in range(len(t_chan)):
      if chan<=t_chan[i][-1]:                         # This is just a list of ifs.
                                                      # It checks if the value falls
                                                      # into each and, if not, carries
                                                      # on.
         return i

#-----GetBG----------------------------------------------------------------------------

@pan.mjit()
def getbg(data,low_channel,high_channel):
   '''Get BG
   
   Description:
    Returns the counts from a standard XTE Background file created with pcabackest
   Inputs:
    event        - FITS OBJECT: The data array of FITS file that has been opened.
    low_channel  -         INT: The highest channel data is to be collected from.
    high_channel -         INT: The lowest channel data is to be collected from.
   Outputs:
    times  -              LIST: The time array associated with the fluxes
    fluxes -              LIST: The fluxes at each time
    
   -J.M.Court,2015'''
   times=data.field(0)
   PCU0=data.field(1)                                 # Collect data from PCU0
   PCU1=data.field(2)                                 # Collect data from PCU1
   PCU2=data.field(3)                                 # Collect data from PCU2
   PCU3=data.field(4)                                 # Collect data from PCU3
   PCU4=data.field(5)                                 # Collect data from PCU4
   PCU=array([PCU0,PCU1,PCU2,PCU3,PCU4])
   PCU[:,:,:low_channel]=0                            # Remove data from outside of
                                                      # channel arrays
   PCU[:,:,(high_channel+1):]=0
   PCU=npsum(PCU,axis=2)
   fluxes=npsum(PCU,axis=0)
   flux_ers=fluxes**0.5
   fluxes=fluxes/16.0                                 # Change from counts to counts/s
   flux_ers=flux_ers/16.0                             # Change from counts to counts/s
   return times,fluxes,flux_ers

#-----GetBin---------------------------------------------------------------------------

def getbin(event,datamode):
   '''Get Bin
   
   Description:
    Returns the time binning of a given set of FITS data.
   Inputs:
    event - FITS OBJECT: The FITS file that has been opened.
   Outputs:
    bsz   -       FLOAT: The bin size of the observation (s).
    
   -J.M.Court, 2014'''
   if datamode[:2] in ['B_','SB']:
      bsz=event[1].header['1CDLT2']
   else:
      bsz=event[1].header['TIMEDEL']
   return bsz

#-----GetDat---------------------------------------------------------------------------

def getdat(event):
   '''Get data
   
   Description:
    Returns the data table of a given set of FITS data.
   Inputs:
    event - FITS OBJECT: The FITS file that has been opened.
   Outputs:
    data  -  FITS TABLE: The data table from the FITS file.
    
   -J.M.Court, 2014'''
   data=event[1].data                                 # Extract GTI indices
   return data

#-----GetGTI---------------------------------------------------------------------------

def getgti(event):
   '''Get GTI
   
   Description:
    Returns the GTI of a given set of FITS data.
   Inputs:
    event - FITS OBJECT: The FITS file that has been opened.
   Outputs:
    gti   -     2D LIST: A list, each element of which contains the start and end point
                of a GTI.
                
   -J.M.Court, 2014'''
   gti=event[2].data                                  # Extract GTI indices
   return gti

#-----GetIni---------------------------------------------------------------------------

def getini(event):
   '''Get Ini
   
   Description:
    Returns the initial (lowest) time of a given set of FITS data.
   Inputs:
    event - FITS OBJECT: The FITS file that has been opened.
   Outputs:
    ini   -       FLOAT: The starting time of the observation (s).
    
   -J.M.Court, 2014'''
   ini=event[1].header['TSTART']
   return ini

#-----Get Obs--------------------------------------------------------------------------

def getobs(event,datamode,filepath):
   '''Get Obs
   
   Description:
    Fetches a tuple consisting of the object and obs_id of the observation.'''
   if datamode[:10]=='GoodXenon_':
      try:
         obsid=(filepath.split('/')[-4])              # GoodXenon for XTE doesnt store
                                                      #  obs_id for some reason

      except:
         obsid=''
   else:
      try:
         obsid=event[1].header['OBS_ID']
      except:
         obsid=''
   obsdata=(event[1].header['OBJECT'],obsid)
   return obsdata

#-----Get PCU--------------------------------------------------------------------------

def getpcu(words,header,t_pcus=None,pculist=False):
   '''Get PCUs
   
   Description:
    If given a list of RXTE event words, and the data-mode in which their associated data is stored,
    returns the number of PCUs that contributed photons to the dataset.  Currently works for two
    datamodes: GoodXenon_2s and E_125us_64M_0_1s with the intention to eventually generalise it to
    any event data.
   Inputs:
    words    - 2D ARRAY: The array of event words, each of which is an array of True/False statements.
    datamode -   STRING: The DATAMODE in which the data to be analysed is stored.
   Outputs:
    pcus     -      INT: The number of PCUs active when the data was taken.  If a non-recognised event
                         word is given, 1 is returned instead along with an error message.
   -J.M.Court, 2015'''
   datamode=header['DATAMODE']
   if datamode[0]=='E':
      enigma=header['TEVTB2'].split('^')[0][1:-1].split('}')[:-1]
                                                      # Again fetch (and decode) the
                                                      # enigmatic TEVTB2 word
      tk,r=pan.tokenloc(enigma,'D')
   elif datamode=='GoodXenon_2s':
      r=7,10
   else:
      r=None
   if r==None and t_pcus==None:
      goodpcus=False
      while not goodpcus:
         try:
            pcus=int(raw_input('Number of Active PCUS: '))
                                                      # Ask the user how many there are
            assert pcus<6                             # Check they give a number between 
                                                      # 1 and 5 (inclusive)
            assert pcus>0
            goodpcus=True
         except:
            'Invalid number of PCUs!'
      return pcus                                     # Use the number they give as the
                                                      # number of PCUs
   elif t_pcus!=None:
      return t_pcus
   words=(words[:,r[0]:r[1]]).tolist()
   pcus=[0,0,0,0,0]
   if [False,False,False] in words: pcus[0]=1
   if [False,False,True] in words: pcus[1]=1
   if [False,True,False] in words: pcus[2]=1
   if [False,True,True] in words: pcus[3]=1
   if [True,False,False] in words: pcus[4]=1
   if sum(pcus)==0:
      print 'Error!  0 PCUs present in data!'
      return 1
   elif not pculist:
      return sum(pcus)
   else:
      return (sum(pcus), pcus)

#-----Get Tim--------------------------------------------------------------------------

def gettim(data,event,tstart,res,datamode):
   '''Get Times
   
   Description: Returns the DATA Word or table column containing data on arrival times.
   
   -J.M.Court, 2015'''
   if datamode[0]=='B':
      times=[]
      for i in range(len(data)):
         times+=[(i*res)+tstart]
      return array(times),data
   elif datamode[:2]=='SB':
      data=data.tolist()
      timeseeds=event.field(0)-event.field(0)[0]+tstart
      times=[]
      indx=0
      prevtimeseed=timeseeds[0]
      for i in timeseeds:                                                
         nx=prevtimeseed+1
         while i-nx>0.9:                              # When a 1s chunk is missing,
                                                      # zero pad the data here

            times+=(arange(0,1.0,res)+nx).tolist()
            data=data[:indx]+([(data[indx]+data[indx-1])/2.0]*int(1.0/res))+data[indx:]
            nx+=1
            indx+=int(1.0/res)
         for j in arange(0,1.0,res):
            times+=[i+j]
         indx+=int(1.0/res)
         prevtimeseed=i
      return array(times),array(data)
      
   else:
      return data.field(0),data 

#-----Get Wrd--------------------------------------------------------------------------

def getwrd(data,datamode):
   '''Get Words
   
   Description: Returns the DATA Word or table column containing data on PCUs.
   
   -J.M.Court, 2015'''
   if datamode[:2] in ['B_','SB']:
      return None
   else:
      return data.field(1)

#-----Get Wrd Row----------------------------------------------------------------------

def getwrdrow(words,mask,datamode):
   '''Get Word Rows
   
   Description: Returns Data Words filtered by a mask, for data that has datawords.
   
   -J.M.Court, 2015'''
   if datamode[:2] in ['B_','SB']:
      return None
   else:
      return words[mask]

#-----MaxEn----------------------------------------------------------------------------

def maxen(datamode):
   '''Max Energy
   
   Description:
    Returns the highest energy or channel valid for the instrument.
    
   -J.M.Court, 2015'''
   if datamode[:2] in ['B_','SB']:
      mcha=datamode.split('_')[-2]
      return int(mcha)
   else:
      return 255

\end{minted}

\section{Mode Get}

\begin{minted}[fontsize=\scriptsize]{python}
#! /usr/bin/env python

# |----------------------------------------------------------------------|
# |-----------------------------MODE_GET---------------------------------|
# |----------------------------------------------------------------------|

# A script to extract the datamode, start time and end time of all .FITS files in a
# folder and save this data to a .txt file.

#-----Importing Modules----------------------------------------------------------------

try:
   import sys
   import os
   import pan_lib as pan
   from astropy.io import fits
   from math import log
   from numpy import array
except:
   print 'Modules missing!  Aborting!'
   print ''
   print '------------------------------------------------'
   print ''
   exit()

#-----Welcoming Header-----------------------------------------------------------------

print ''
print '-------Running Mode Get: J.M.Court, 2015------'
print ''

#-----Opening Files--------------------------------------------------------------------

read=0
outname=pan.uniqfname('modes','txt')
tmpname=pan.uniqfname('temp','txt')
info = open(outname, 'w')
files=sorted(os.listdir(os.getcwd()))

#-----Excluding Readings of .plotd and .speca Files------------------------------------

for fileid in range(len(files)):
   filename=files[fileid]
   if '.plotd' in filename or '.speca' in filename:
      files[fileid]=tmpname

#-----Creating Header------------------------------------------------------------------

info.write('{0:26} {1:22} {2:12} {3:12} {4:6} \n \n'.format('File','Datamode','Start',
                                                            'End','Length'))


#-----Extracting Data------------------------------------------------------------------

files=array(files)
for filename in files[files!=tmpname]:
   try:
      cfile=fits.open(filename)[1].header
      read+=1
      try:
         dmode=cfile['DATAMODE']
      except:
         dmode=None
      try:
         stime=float(cfile['TSTART'])
         etime=float(cfile['TSTOP'])
         dtime=etime-stime
      except:
         stime=None
         etime=None
         dtime=None
      info.write('{0:26} {1:22} {2:12} {3:12} {4:6} \n'.format(filename,dmode,
                                                               str(stime),str(etime),
                                                               str(dtime)))
   except:
      None
info.close()
if read==0:
   print 'No FITS files found!'
   os.remove(outname)
else:
   print 'Information saved as '+outname+'!'
pan.signoff()
exit()

\end{minted}